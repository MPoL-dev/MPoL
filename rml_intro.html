

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction to Regularized Maximum Likelihood Imaging &mdash; MPoL 0.1.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
        <script src="https://buttons.github.io/buttons.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/bullets.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/faculty.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans|Roboto:400,700|Roboto+Mono:400,700&display=swap" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="MPoL Installation" href="installation.html" />
    <link rel="prev" title="Million Points of Light (MPoL)" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

  
    <a class="heading heading-extra-margin" href="index.html">
      <div class="logo-box logo-box-large">
        <img class="logo" src="_static/logo.png"/>
      </div>
      
        <span class="icon icon-home"> MPoL</span>
      
    </a>
  

  
    
    
      <div class="version">0.1.1</div>
    
  

  
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction to Regularized Maximum Likelihood Imaging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction-to-likelihood-functions">Introduction to Likelihood functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-in-the-fourier-domain">Data in the Fourier domain</a></li>
<li class="toctree-l2"><a class="reference internal" href="#likelihood-functions-for-fourier-data">Likelihood functions for Fourier data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rml-images-as-non-parametric-models">RML images as non-parametric models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-mpol-package-for-regularized-maximum-likelihood-imaging">The MPoL package for Regularized Maximum Likelihood imaging</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">MPoL Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="units-and-conventions.html">Units and Conventions</a></li>
<li class="toctree-l1"><a class="reference internal" href="developer-documentation.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="ci-tutorials/PyTorch.html">Introduction to PyTorch: Tensors and Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="ci-tutorials/gridder.html">Gridding and diagnostic images</a></li>
<li class="toctree-l1"><a class="reference internal" href="ci-tutorials/optimization.html">Optimization Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="ci-tutorials/crossvalidation.html">Cross validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ci-tutorials/gpu_setup.html">GPU Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="ci-tutorials/initializedirtyimage.html">Initializing with the Dirty Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="large-tutorials/HD143006_part_1.html">HD143006 Tutorial Part 1</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MPoL</a>
        
      </nav>


      <div class="wy-nav-content">

  

  
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
  <li class="breadcrumb"><a href="index.html">MPoL</a> &raquo;</li>
    
  <li class="breadcrumb">Introduction to Regularized Maximum Likelihood Imaging</li>

    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/rml_intro.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="introduction-to-regularized-maximum-likelihood-imaging">
<span id="rml-intro-label"></span><h1>Introduction to Regularized Maximum Likelihood Imaging<a class="headerlink" href="#introduction-to-regularized-maximum-likelihood-imaging" title="Permalink to this headline">¶</a></h1>
<p>This document is an attempt to provide a whirlwind introduction to what Regularized Maximum Likelihood (RML) imaging is, and why you might want to use this MPoL package to perform it with your interferometric dataset. Of course, the field is rich, varied, and this short introduction couldn’t possibly do justice to cover the topic in depth. We recommend that you check out many of the links and suggestions in this document for further reading and understanding.</p>
<section id="introduction-to-likelihood-functions">
<h2>Introduction to Likelihood functions<a class="headerlink" href="#introduction-to-likelihood-functions" title="Permalink to this headline">¶</a></h2>
<p>Typically, when astronomers fit a model to some dataset, such as a line <span class="math notranslate nohighlight">\(y = m x + b\)</span> to a collection of <span class="math notranslate nohighlight">\(\boldsymbol{X} = \{x_1, x_2, \ldots\, x_N\}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{Y} = \{y_1, y_2, \ldots\, y_N\}\)</span> points, we require a likelihood function. Simply put, the likelihood function specifies the probability of the data, given a model, and encapsulates our assumptions about the data and noise generating processes.</p>
<p>For most real-world datasets, we don’t measure the “true” <span class="math notranslate nohighlight">\(y\)</span> value of the line (i.e., <span class="math notranslate nohighlight">\(mx + b\)</span>), but rather make a measurement which has been partially corrupted by some “noise.” In that case, we say that each <span class="math notranslate nohighlight">\(y_i\)</span> data point is actually generated by</p>
<div class="math notranslate nohighlight">
\[y_i = m x_i + b + \epsilon\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a noise realization from a standard <a class="reference external" href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> with standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[\epsilon \sim \mathcal{N}(0, \sigma).\]</div>
<p>This information about the data and noise generating process means that we can write down a likelihood function to calculate the probability of the data, given a set of model parameters. The likelihood function is <span class="math notranslate nohighlight">\(p(\boldsymbol{Y} |\,\boldsymbol{\theta})\)</span>. Sometimes it is written as <span class="math notranslate nohighlight">\(\mathcal{L}(\boldsymbol{Y} |\,\boldsymbol{\theta})\)</span>, and frequently, when employed in computation, we’ll use the logarithm of the likelihood function, or “log-likelihood,” <span class="math notranslate nohighlight">\(\ln \mathcal{L}\)</span> to avoid numerical under/overflow issues. Let’s call <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = \{m, b\}\)</span> and <span class="math notranslate nohighlight">\(M(x_i |\, \boldsymbol{\theta}) = m x_i + b\)</span>. The likelihood function for this line problem is</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\boldsymbol{Y} |\,\boldsymbol{\theta}) = \prod_i^N \frac{1}{\sqrt{2 \pi} \sigma} \exp \left [ - \frac{(y_i - M(x_i |\,\boldsymbol{\theta}))^2}{2 \sigma^2}\right ]\]</div>
<p>The logarithm of the likelihood function is</p>
<div class="math notranslate nohighlight">
\[\ln \mathcal{L}(\boldsymbol{Y} |\,\boldsymbol{\theta}) = -N \ln(\sqrt{2 \pi} \sigma) - \frac{1}{2} \sum_i^N \frac{(y_i - M(x_i |\,\boldsymbol{\theta}))^2}{\sigma^2}\]</div>
<p>You may recognize the right hand term looks similar to the <span class="math notranslate nohighlight">\(\chi^2\)</span> metric,</p>
<div class="math notranslate nohighlight">
\[\chi^2(\boldsymbol{Y} |\,\boldsymbol{\theta}) = \sum_i^N \frac{(y_i - M(x_i |\,\boldsymbol{\theta}))^2}{\sigma^2}\]</div>
<p>Assuming that the uncertainty (<span class="math notranslate nohighlight">\(\sigma\)</span>) on each data point is known (and remains constant), the first term in the log likelihood expression remains constant, and we have</p>
<div class="math notranslate nohighlight">
\[\ln \mathcal{L}(\boldsymbol{Y} |\,\boldsymbol{\theta}) = - \frac{1}{2} \chi^2 (\boldsymbol{Y} |\,\boldsymbol{\theta}) + C\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> is a constant with respect to the model parameters. It is common to use shorthand to say that “the likelihood function is <span class="math notranslate nohighlight">\(\chi^2\)</span>” to indicate situations where the data uncertainties are Gaussian. Very often, we (or others) are interested in the parameter values <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_\mathrm{MLE}\)</span> which maximize the likelihood function. Unsurprisingly, these parameters are called the <em>maximum likelihood estimate</em> (or MLE), and usually they represent something like a “best-fit” model. <a class="footnote-reference brackets" href="#mle-solution" id="id1">1</a></p>
<p>When it comes time to do parameter inference, however, it’s important to keep in mind</p>
<ol class="arabic simple">
<li><p>the simplifying assumptions we made about the noise uncertainties being constant with respect to the model parameters. If we were to “fit for the noise” in a hierarchical model, for example, we would need to use the full form of the log-likelihood function, including the <span class="math notranslate nohighlight">\(-N \ln \left (\sqrt{2 \pi} \sigma \right)\)</span> term.</p></li>
<li><p>that in order to maximize the likelihood function we want to <em>minimize</em> the <span class="math notranslate nohighlight">\(\chi^2\)</span> function.</p></li>
<li><p>that constants of proportionality (e.g., the <span class="math notranslate nohighlight">\(1/2\)</span> in front of the <span class="math notranslate nohighlight">\(\chi^2\)</span>) can matter when combining likelihood functions with prior distributions for Bayesian parameter inference. We’ll have more to say on this in a second when we talk about regularizers and their strengths.</p></li>
</ol>
<p>To be specific, <span class="math notranslate nohighlight">\(\chi^2\)</span> is not the end of the story when we’d like to perform Bayesian parameter inference. To do so, we need the posterior probability distribution of the model parameters given the dataset, <span class="math notranslate nohighlight">\(p(\boldsymbol{\theta}|\,\boldsymbol{Y})\)</span>. We can calculate this quantity using Bayes rule</p>
<div class="math notranslate nohighlight">
\[p(\boldsymbol{\theta}|\,\boldsymbol{Y}) = \frac{p(\boldsymbol{Y}|\,\boldsymbol{\theta})\, p(\boldsymbol{\theta})}{p(\boldsymbol{Y})}\]</div>
<p>The denominator is a constant so long as the model specification remains the same, leaving</p>
<div class="math notranslate nohighlight">
\[p(\boldsymbol{\theta}|\,\boldsymbol{Y}) \propto p(\boldsymbol{Y}|\,\boldsymbol{\theta})\, p(\boldsymbol{\theta}).\]</div>
<p>So we need a prior probability distribution <span class="math notranslate nohighlight">\(p(\boldsymbol{\theta})\)</span> in addition to the likelihood function to calculate the posterior probability distribution of the model parameters. Analogous to the maximum likelihood estimate, there is also the <em>maximum a posteriori</em> estimate (or MAP), which includes the effect of the prior probability distribution.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Useful resources on Bayesian inference include</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.amazon.com/Data-Analysis-Bayesian-Devinderjit-Sivia/dp/0198568320">Data Analysis: A Bayesian Tutorial</a> by Sivia and Skilling</p></li>
<li><p><a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2010arXiv1008.4686H/abstract">Data analysis recipes: Fitting a model to data</a> by Hogg, Bovy, and Lang</p></li>
<li><p><a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2012arXiv1205.4446H/abstract">Data analysis recipes: Probability calculus for inference</a> by Hogg. Both this and the previous Hogg et al. document contain useful descriptions of what a forward or “generative” model is.</p></li>
</ul>
</div>
</section>
<section id="data-in-the-fourier-domain">
<h2>Data in the Fourier domain<a class="headerlink" href="#data-in-the-fourier-domain" title="Permalink to this headline">¶</a></h2>
<p>MPoL is a package to make images from interferometric data. Currently, we are most focused on modeling datasets from radio interferometers like the <a class="reference external" href="https://almascience.nrao.edu/">Atacama Large Millimeter Array</a> (ALMA), so the following introduction will have a radio astronomy flavor to it. But the concept of forward modeling interferometric data is quite general, and with a few additions the MPoL package could be applied to imaging problems involving Fourier data from optical and infrared telescopes (if this describes your dataset, please get in touch).</p>
<p>As astronomers, we are most often interested in characterizing what an astrophysical source looks like. In other words, how its surface brightness <span class="math notranslate nohighlight">\(I\)</span> changes as a function of sky position. However, intereferometers acquire samples of data in the Fourier domain, also called the visibility domain. The visibility domain is the Fourier transform of the image sky brightness</p>
<div class="math notranslate nohighlight">
\[{\cal V}(u,v) = \iint I(l,m) \exp \left \{- 2 \pi i (ul + vm) \right \} \, \mathrm{d}l\,\mathrm{d}m,\]</div>
<p>where <span class="math notranslate nohighlight">\(l\)</span> and <span class="math notranslate nohighlight">\(m\)</span> are direction cosines (roughly equivalent to R.A. and Dec) which parameterize the surface brightness distribution of the image <span class="math notranslate nohighlight">\(I(l,m)\)</span>, and <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> are spatial frequencies which parameterize the visibility function <span class="math notranslate nohighlight">\(\cal{V}(u,v)\)</span>. For more information on the meaning of these units, see <a class="reference internal" href="units-and-conventions.html#units-conventions-label"><span class="std std-ref">Units and Conventions</span></a>.</p>
<p>The visibility function is complex-valued, and each measurement of it (denoted by <span class="math notranslate nohighlight">\(V_i\)</span>) is made in the presence of noise</p>
<div class="math notranslate nohighlight">
\[V_i = \mathcal{V}(u_i, v_i) + \epsilon.\]</div>
<p>Here <span class="math notranslate nohighlight">\(\epsilon\)</span> represents a noise realization from a <a class="reference external" href="https://en.wikipedia.org/wiki/Complex_normal_distribution">complex normal</a> (Gaussian) distribution. Thankfully, most interferometric datasets do not exhibit significant covariance between the real and imaginary noise components, so we could equivalently say that the real and imaginary components of the noise are separately generated by draws from normal distributions characterized by standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\epsilon_\Re \sim \mathcal{N}(0, \sigma) \\
\epsilon_\Im \sim \mathcal{N}(0, \sigma)\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\epsilon = \epsilon_\Re + i \epsilon_\Im\]</div>
<p>Radio interferometers will commonly represent the uncertainty on each visibility measurement by a “weight” <span class="math notranslate nohighlight">\(w_i\)</span>, where</p>
<div class="math notranslate nohighlight">
\[w_i = \frac{1}{\sigma_i^2}\]</div>
<p>A full interferometric dataset is a collection of visibility measurements, which we represent by</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{V} = \{V_1, V_2, \ldots \}_{i=1}^N\]</div>
<p>For reference, a typical ALMA dataset might contain a half-million individual visibility samples, acquired over a range of spatial frequencies.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>A full introduction to Fourier transforms, radio astronomy, and interferometry is beyond the scope of this introduction. However, here are some additional resources that we recommend checking out.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.cv.nrao.edu/~sransom/web/xxx.html">Essential radio astronomy</a> textbook by James Condon and Scott Ransom, and in particular, Chapter 3.7 on Radio Interferometry.</p></li>
<li><p>NRAO’s <a class="reference external" href="http://www.cvent.com/events/virtual-17th-synthesis-imaging-workshop/agenda-0d59eb6cd1474978bce811194b2ff961.aspx">17th Synthesis Imaging Workshop</a> recorded lectures and slides available</p></li>
<li><p><a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2017isra.book.....T/abstract">Interferometry and Synthesis in Radio Astronomy</a> by Thompson, Moran, and Swenson. An excellent and comprehensive reference on all things interferometry.</p></li>
<li><p>NJIT’s online course materials for <a class="reference external" href="https://web.njit.edu/~gary/728/">Radio Astronomy</a></p></li>
</ul>
</div>
</section>
<section id="likelihood-functions-for-fourier-data">
<h2>Likelihood functions for Fourier data<a class="headerlink" href="#likelihood-functions-for-fourier-data" title="Permalink to this headline">¶</a></h2>
<p>Now that we’ve introduced likelihood functions in general and the specifics of Fourier data, let’s talk about likelihood functions for inference with Fourier data. As before, our statement about the data generating process</p>
<div class="math notranslate nohighlight">
\[V_i = \mathcal{V}(u_i, v_i) + \epsilon\]</div>
<p>leads us to the formulation of the likelihood function.</p>
<p>First, let’s assume we have some model that we’d like to fit to our dataset. To be a forward model, it should be able to predict the value of the visibility function for any spatial frequency, i.e., we need to be able to calculate <span class="math notranslate nohighlight">\(\mathcal{V}(u, v) = M_\mathcal{V}(u, v |, \boldsymbol{\theta})\)</span>.</p>
<p>It’s difficult to reason about all but the simplest models directly in the Fourier plane, so usually models are constructed in the image plane <span class="math notranslate nohighlight">\(M_I(l,m |,\boldsymbol{\theta})\)</span> and then Fourier transformed (either analytically, or via the FFT) to construct visibility models <span class="math notranslate nohighlight">\(M_\mathcal{V}(u, v |, \boldsymbol{\theta}) \leftrightharpoons M_I(l,m |,\boldsymbol{\theta})\)</span>. For example, these models could be channel maps of carbon monoxide emission from a rotating protoplanetary disk (as in <a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2015ApJ...806..154C/abstract">Czekala et al. 2015</a>, where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> contains parameters setting the structure of the disk), or rings of continuum emission from a protoplanetary disk (as in <a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2018ApJ...869L..48G/abstract">Guzmán et al. 2018</a>, where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> contains parameters setting the sizes and locations of the rings).</p>
<p>Following the discussion about how the complex noise realization <span class="math notranslate nohighlight">\(\epsilon\)</span> is generated, this leads to a log likelihood function</p>
<div class="math notranslate nohighlight">
\[\ln \mathcal{L}(\boldsymbol{V}|\,\boldsymbol{\theta}) = - \frac{1}{2} \chi^2(\boldsymbol{V}|\,\boldsymbol{\theta}) + C\]</div>
<p>Because the data and model are complex-valued, <span class="math notranslate nohighlight">\(\chi^2\)</span> is evaluated as</p>
<div class="math notranslate nohighlight">
\[\chi^2(\boldsymbol{V}|\,\boldsymbol{\theta}) = \sum_i^N \frac{|V_i - M_\mathcal{V}(u_i, v_i |\,\boldsymbol{\theta})|^2}{\sigma_i^2}\]</div>
<p>where <span class="math notranslate nohighlight">\(| |\)</span> denotes the modulus squared. Equivalently, the calculation can be broken up into sums over the real (<span class="math notranslate nohighlight">\(\Re\)</span>) and imaginary (<span class="math notranslate nohighlight">\(\Im\)</span>) components of the visibility data and model</p>
<div class="math notranslate nohighlight">
\[\chi^2(\boldsymbol{V}|\,\boldsymbol{\theta}) = \sum_i^N \frac{(V_{\Re,i} - M_\mathcal{V,\Re}(u_i, v_i |\,\boldsymbol{\theta}))^2}{\sigma_i^2} + \sum_i^N \frac{(V_{\Im,i} - M_\mathcal{V,\Im}(u_i, v_i |\,\boldsymbol{\theta}))^2}{\sigma_i^2}\]</div>
<p>Now with the likelihood function specified, we can add prior probability distributions <span class="math notranslate nohighlight">\(p(\boldsymbol{\theta})\)</span>, and calculate and explore the posterior probability distribution of the model parameters using algorithms like Markov Chain Monte Carlo. In this type of Bayesian inference, we’re usually using forward models constructed with a small to medium number of parameters (e.g., 10 - 30), like in the protoplanetary disk examples of <a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2015ApJ...806..154C/abstract">Czekala et al. 2015</a> or <a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2018ApJ...869L..48G/abstract">Guzmán et al. 2018</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Even though we would say that “traditional” Bayesian parameter inference is not the main focus of RML algorithms, it is entirely <a class="reference external" href="https://github.com/MPoL-dev/MPoL/issues/33">possible with the MPoL package</a>. In fact, the gradient-based nature of the MPoL package (discussed in a moment) can make posterior exploration very fast using efficient Hamiltonian Monte Carlo samplers.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <span class="math notranslate nohighlight">\(\chi^2\)</span> likelihood function as formulated above is appropriate for visibilities with minimal spectral covariance. When modeling spectral line datasets, in particular those that have not been channel-averaged and retain the spectral response function from their Hann windowing, this covariance must be taken into account in the likelihood function. More information on how to derive these covariance matrices is provided in the appendices of <a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2018AJ....155..182L/abstract">Loomis et al. 2018</a> and will be detailed in forthcoming tutorials.</p>
</div>
</section>
<section id="rml-images-as-non-parametric-models">
<h2>RML images as non-parametric models<a class="headerlink" href="#rml-images-as-non-parametric-models" title="Permalink to this headline">¶</a></h2>
<p>Now that we’ve introduced what it means to forward-model a dataset and how to calculate a likelihood function, let’s talk about non-parametric models.</p>
<p>Say that our <span class="math notranslate nohighlight">\(\boldsymbol{X} = \{x_1, x_2, \ldots\, x_N\}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{Y} = \{y_1, y_2, \ldots\, y_N\}\)</span> dataset looked a bit more structured than a simple <span class="math notranslate nohighlight">\(y = mx + b\)</span> relationship. We could expand the model by adding more parameters, for example, by adding quadratic and cubic terms, e.g., <span class="math notranslate nohighlight">\(y = a_0 + a_1 x + a_2 x^2 + a_3 x^3\)</span>. This would be a reasonable approach, especially if the parameters <span class="math notranslate nohighlight">\(a_2\)</span>, <span class="math notranslate nohighlight">\(a_3\)</span>, etc… had physical meaning. But if all that we’re interested in is modeling the relationship between <span class="math notranslate nohighlight">\(y = f(x)\)</span> in order to make predictions, we could just as easily use a <a class="reference external" href="https://www.section.io/engineering-education/parametric-vs-nonparametric/">non-parametric model</a>, like a <a class="reference external" href="https://en.wikipedia.org/wiki/Spline_(mathematics)">spline</a> or a <a class="reference external" href="https://distill.pub/2019/visual-exploration-gaussian-processes/">Gaussian process</a>.</p>
<p>With RML imaging, we’re trying to come up with a model that will fit the dataset. But rather than using a parametric model like a protoplanetary disk structure model or a series of Gaussian rings, we’re using a non-parametric model of <em>the image itself</em>. This could be as simple as parameterizing the image using the intensity values of the pixels themselves, i.e.,</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\theta} = \{I_1, I_2, \ldots, I_{N^2} \}\]</div>
<p>assuming we have an <span class="math notranslate nohighlight">\(N \times N\)</span> image.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>RML imaging is different from CLEAN imaging, which operates as a deconvolution procedure in the image plane. At least at sub-mm and radio wavelengths, CLEAN is by far the dominant algorithm used to synthesize images from interferometric data. Therefore, if you’re interested in RML imaging, it’s worth first understanding the basics of the CLEAN algorithm.</p>
<p>Here are some useful resources on the CLEAN algorithm.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2017isra.book.....T/abstract">Interferometry and Synthesis in Radio Astronomy</a> Chapter 11.1</p></li>
<li><p><a class="reference external" href="https://casa.nrao.edu/casadocs-devel/stable/imaging/synthesis-imaging">CASA documentation on tclean</a></p></li>
<li><p>David Wilner’s lecture on <a class="reference external" href="https://www.youtube.com/watch?v=mRUZ9eckHZg">Imaging and Deconvolution in Radio Astronomy</a></p></li>
<li><p>For a discussion on using both CLEAN and RML techniques to robustly interpret kinematic data of protoplanetary disks, see Section 3 of <a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2020arXiv200904345D/abstract">Visualizing the Kinematics of Planet Formation</a> by The Disk Dynamics Collaboration</p></li>
</ul>
</div>
<p>A flexible image model for RML imaging is mostly analogous to using a spline or Gaussian process to fit a series of <span class="math notranslate nohighlight">\(\boldsymbol{X} = \{x_1, x_2, \ldots\, x_N\}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{Y} = \{y_1, y_2, \ldots\, y_N\}\)</span> points—the model will nearly always have enough flexibility to capture the structure that exists in the dataset. The most straightforward formulation of a non-parametric image model is the pixel basis set, but we could also use more sophisticated basis sets like a set of wavelet coefficients, or even more exotic basis sets constructed from trained neural networks. These may have some serious advantages when it comes to the “regularizing” part of “regularized maximum likelihood” imaging. But first, let’s talk about the “maximum likelihood” part.</p>
<p>Given some image parameterization (e.g., a pixel basis set of <span class="math notranslate nohighlight">\(N \times N\)</span> pixels, with each pixel <code class="docutils literal notranslate"><span class="pre">cell_size</span></code> in width), we would like to find the maximum likelihood image <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_\mathrm{MLE}\)</span>. Fortunately, because the Fourier transform is a linear operation, we can analytically calculate the maximum solution (the same way we might find the best-fit slope and intercept for the line example). This maximum likelihood solution is called (in the radio astronomy world) the dirty image, and its associated point spread function is called the dirty beam.</p>
<p>In the construction of the dirty image, all unsampled spatial frequencies are set to zero power. This means that the dirty image will only contain spatial frequencies about which we have at least some data. This assumption, however, rarely translates into good image fidelity, especially if there are many unsampled spatial frequencies which carry significant power. It’s also important to recognize that dirty image is only <em>one</em> out of a set of <em>many</em> images that could maximize the likelihood function. From the perspective of the likelihood calculation, we could modify the unsampled spatial frequencies of the dirty image to whatever power we might like, and, because they are <em>unsampled</em>, the value of the likelihood calculation won’t change, i.e., it will still remain maximal.</p>
<p>When synthesis imaging is described as an “ill-posed inverse problem,” this is what is meant. There is a (potentially infinite) range of images that could <em>exactly</em> fit the dataset, and without additional information we have no way of discriminating which is best. As you might suspect, this is now where the “regularization” part of “regularized maximum likelihood” imaging comes in.</p>
<p>There are a number of different ways to talk about regularization. If one wants to be Bayesian about it, one would talk about specifying <em>priors</em>, i.e., we introduce terms like <span class="math notranslate nohighlight">\(p(\boldsymbol{\theta})\)</span> such that we might calculate the maximum a posteriori (MAP) image <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_\mathrm{MAP}\)</span> using the posterior probability distribution</p>
<div class="math notranslate nohighlight">
\[p(\boldsymbol{\theta} |\, \boldsymbol{V}) \propto \mathcal{L}(\boldsymbol{V} |\, \boldsymbol{\theta}) \, p(\boldsymbol{\theta}).\]</div>
<p>For computational reasons related to numerical over/underflow, we would most likely use the logarithm of the posterior probability distribution</p>
<div class="math notranslate nohighlight">
\[\ln p(\boldsymbol{\theta} |\, \boldsymbol{V}) \propto \ln \mathcal{L}(\boldsymbol{V} |\, \boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}).\]</div>
<p>One could accomplish the same goal without necessarily invoking the Bayesian language by simply talking about which parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> optimize some objective function.</p>
<p>We’ll adopt the perspective that we have some objective “cost” function that we’d like to <em>minimize</em> to obtain the optimal parameters <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span>. The machine learning community calls this a “loss” function <span class="math notranslate nohighlight">\(L(\boldsymbol{\theta})\)</span>, and so we’ll borrow that terminology here. For an unregularized fit, an acceptable loss function is just the negative log likelihood (“nll”) term,</p>
<div class="math notranslate nohighlight">
\[L(\boldsymbol{\theta}) = L_\mathrm{nll}(\boldsymbol{\theta}) = - \ln \mathcal{L}(\boldsymbol{V}|\,\boldsymbol{\theta}) = \frac{1}{2} \chi^2(\boldsymbol{V}|\,\boldsymbol{\theta})\]</div>
<p>If we’re only interested in <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span>, it doesn’t matter whether we include the <span class="math notranslate nohighlight">\(1/2\)</span> prefactor in front of <span class="math notranslate nohighlight">\(\chi^2\)</span>, the loss function will still have the same optimum. However, when it comes time to add additional terms to the loss function, these prefactors matter in controlling the relative strength of each term.</p>
<p>When phrased in the terminology of function optimization, additional terms can be described as regularization penalties. To be specific, let’s add a term that regularizes the sparsity of an image.</p>
<div class="math notranslate nohighlight">
\[L_\mathrm{sparsity}(\boldsymbol{\theta}) = \sum_i |I_i|\]</div>
<p>This prior is described in more detail in the <a class="reference external" href="api.html#mpol.losses.sparsity">API documentation</a>. In short, the L1 norm promotes sparse solutions (solutions where many pixel values are zero). The combination of these two terms leads to a new loss function</p>
<div class="math notranslate nohighlight">
\[L(\boldsymbol{\theta}) = L_\mathrm{nll}(\boldsymbol{\theta}) + \lambda_\mathrm{sparsity} L_\mathrm{sparsity}(\boldsymbol{\theta})\]</div>
<p>Where we control the relative “strength” of the regularization via the scalar prefactor <span class="math notranslate nohighlight">\(\lambda_\mathrm{sparsity}\)</span>. If <span class="math notranslate nohighlight">\(\lambda_\mathrm{sparsity} = 0\)</span>, no sparsity regularization is applied. Non-zero values of <span class="math notranslate nohighlight">\(\lambda_\mathrm{sparsity}\)</span> will add in regularization that penalizes non-sparse <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> values. How strong this penalization is depends on the strength relative to the other terms in the loss calculation. <a class="footnote-reference brackets" href="#relative-strength" id="id2">2</a></p>
<p>We can equivalently specify this using Bayesian terminology, such that</p>
<div class="math notranslate nohighlight">
\[p(\boldsymbol{\theta} |\,\boldsymbol{V}) = \mathcal{L}(\boldsymbol{V}|\,\boldsymbol{\theta}) \, p(\boldsymbol{\theta})\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[p(\boldsymbol{\theta}) = C \exp \left (-\lambda_\mathrm{sparsity} \sum_i | I_i| \right)\]</div>
<p>and <span class="math notranslate nohighlight">\(C\)</span> is a normalization factor. When working with the logarithm of the posterior, this constant term is irrelevant.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>That’s RML imaging in a nutshell, but we’ve barely scratched the surface. We highly recommend checking out the following excellent resources.</p>
<ul class="simple">
<li><p>The fourth paper in the 2019 <a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2019ApJ...875L...4E/abstract">Event Horizon Telescope Collaboration series</a> describing the imaging principles</p></li>
<li><p><a class="reference external" href="https://ui.adsabs.harvard.edu/abs/1986ARA%26A..24..127N/abstract">Maximum entropy image restoration in astronomy</a> AR&amp;A by Narayan and Nityananda 1986</p></li>
<li><p><a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2018A%26C....22...16C/abstract">Multi-GPU maximum entropy image synthesis for radio astronomy</a> by Cárcamo et al. 2018</p></li>
</ul>
</div>
</section>
<section id="the-mpol-package-for-regularized-maximum-likelihood-imaging">
<h2>The MPoL package for Regularized Maximum Likelihood imaging<a class="headerlink" href="#the-mpol-package-for-regularized-maximum-likelihood-imaging" title="Permalink to this headline">¶</a></h2>
<p><em>Million Points of Light</em> or “MPoL” is a Python package that is used to perform regularized maximum likelihood imaging. By that we mean that the package provides the building blocks to create flexible image models and optimize them to fit interferometric datasets. The package is developed completely in the open on <a class="reference external" href="https://github.com/MPoL-dev/MPoL">Github</a>.</p>
<p>We strive to</p>
<ul class="simple">
<li><p>create an open, welcoming, and supportive community for new users and contributors (see our <a class="reference external" href="https://github.com/MPoL-dev/MPoL/blob/main/CODE_OF_CONDUCT.md">code of conduct</a> and <a class="reference external" href="developer-documentation.html">developer documentation</a>)</p></li>
<li><p>support well-tested (<a class="reference external" href="https://github.com/MPoL-dev/MPoL/actions/workflows/tests.yml"><img alt="Tests badge" src="https://github.com/MPoL-dev/MPoL/actions/workflows/tests.yml/badge.svg" /></a>) and stable releases (i.e., <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">mpol</span></code>) that run on all currently-supported Python versions, on Linux, MacOS, and Windows</p></li>
<li><p>maintain up-to-date <a class="reference external" href="api.html">API documentation</a></p></li>
<li><p>cultivate tutorials covering real-world applications</p></li>
</ul>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>We also recommend checking out several other excellent packages for RML imaging:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/astrosmili/smili">SMILI</a></p></li>
<li><p><a class="reference external" href="https://github.com/achael/eht-imaging">eht-imaging</a></p></li>
<li><p><a class="reference external" href="https://github.com/miguelcarcamov/gpuvmem">GPUVMEM</a></p></li>
</ul>
</div>
<p>There are a few things about  MPoL that we believe make it an appealing platform for RML modeling.</p>
<p><strong>Built on PyTorch</strong>: Many of MPoL’s exciting features stem from the fact that it is built on top of a rich computational library that supports autodifferentiation and construction of complex neural networks. Autodifferentiation libraries like <a class="reference external" href="https://github.com/aesara-devs/aesara">Theano/Aesara</a>, <a class="reference external" href="https://www.tensorflow.org/">Tensorflow</a>, <a class="reference external" href="https://pytorch.org/">PyTorch</a>, and <a class="reference external" href="https://jax.readthedocs.io/">JAX</a> have revolutionized the way we compute and optimize functions. For now, PyTorch is the library that best satisfies our needs, but we’re keeping a close eye on the Python autodifferentiation ecosystem should a more suitable framework arrive. If you are familiar with scientific computing with Python but haven’t yet tried any of these frameworks, don’t worry, the syntax is easy to pick up and quite similar to working with numpy arrays. For example, check out our tutorial <a class="reference external" href="ci-tutorials/PyTorch.html">introduction to PyTorch</a>.</p>
<p><strong>Autodifferentiation</strong>: PyTorch gives MPoL the capacity to autodifferentiate through a model. The <em>gradient</em> of the objective function is exceptionally useful for finding the “downhill” direction in a large parameter space (such as the set of image pixels). Traditionally, these gradients would have needed to been calculated analytically (by hand) or via finite-difference methods which can be noisy in high dimensions. By leveraging the autodifferentiation capabilities, this allows us to rapidly formulate and implement complex prior distributions which would otherwise be difficult to differentiate by hand.</p>
<p><strong>Optimization</strong>: PyTorch provides a full-featured suite of research-grade <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">optimizers</a> designed to train deep neural networks. These same optimizers can be employed to quickly find the optimum RML image.</p>
<p><strong>GPU acceleration</strong>: PyTorch wraps CUDA libraries, making it seamless to take advantage of (multi-)GPU acceleration to optimize images. No need to use a single line of CUDA.</p>
<p><strong>Model composability</strong>: Rather than being a monolithic program for single-click RML imaging, MPoL strives to be a flexible, composable, RML imaging <em>library</em> that provides primitives that can be used to easily solve your particular imaging challenge. One way we do this is by mimicking the PyTorch ecosystem and writing the RML imaging workflow using <a class="reference external" href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">PyTorch modules</a>. This makes it easy to mix and match modules to construct arbitrarily complex imaging workflows. We’re working on tutorials that describe these ideas in depth, but one example would be the ability to use a single latent space image model to simultaneously fit single dish and interferometric data.</p>
<p><strong>A bridge to the machine learning/neural network community</strong>: MPoL will happily calculate RML images for you using “traditional” image priors, lest you are the kind of person that turns your nose up at the words “machine learning” or “neural network.” However, if you are the kind of person that sees opportunity in these tools, because MPoL is built on PyTorch, it is straightforward to take advantage of them for RML imaging. For example, if one were to train a variational autoencoder on protoplanetary disk emission morphologies, the latent space + decoder architecture could be easily plugged in to MPoL and serve as an imaging basis set.</p>
<p>To get started with MPoL, we recommend <a class="reference external" href="installation.html">installing the package</a> and reading through the tutorial series. If you have any questions about the package, we invite you to join us on our <a class="reference external" href="https://github.com/MPoL-dev/MPoL/discussions">Github discussions page</a>.</p>
<p class="rubric">Footnotes</p>
<dl class="footnote brackets">
<dt class="label" id="mle-solution"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>There’s actually a lot to unpack here. When your model has many parameters (i.e., the posterior distribution is high dimensional), the MLE (or MAP) solution is unlikely to represent a <em>typical</em> realization of your model parameters. This is a quirk of the geometry of high dimensional spaces. For more information, we recommend checking out Chapter 1 of <a class="reference external" href="https://arxiv.org/abs/1701.02434">Betancourt 2017</a>. Still, the MLE solution is often a useful quantity to communicate, summarizing the mode of the probability distribution.</p>
</dd>
<dt class="label" id="relative-strength"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>This is where the factor of <span class="math notranslate nohighlight">\(1/2\)</span> in front of <span class="math notranslate nohighlight">\(\chi^2\)</span> becomes important. You could use something like <span class="math notranslate nohighlight">\(L_\mathrm{nll}(\boldsymbol{\theta}) = \chi^2(\boldsymbol{\theta})\)</span>, but then you’d need to change the value of <span class="math notranslate nohighlight">\(\lambda_\mathrm{sparsity}\)</span> to achieve the same relative regularization.</p>
</dd>
</dl>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="installation.html" class="btn btn-neutral float-right" title="MPoL Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Million Points of Light (MPoL)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019-21, Ian Czekala

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>


      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-5472810-8', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>