.. _rml-intro-label:

======================================================
Introduction to Regularized Maximum Likelihood Imaging
======================================================

This document is an attempt to provide a whirlwind introduction to what Regularized Maximum Likelihood (RML) imaging is, and why you might want to use this MPoL package to perform it with your interferometric dataset. Of course, the field is rich, varied, and this short introduction couldn't possibly do justice to cover the topic in depth. We recommend that you check out many of the links and suggestions in this document for further reading and understanding.

Data in the Fourier domain
--------------------------

MPoL is a package to make images from interferometric data. Currently, we are most focused on modeling datasets from radio interferometers like the `Atacama Large Millimeter Array <https://almascience.nrao.edu/>`__ (ALMA), so the following introduction will have a radio astronomy flavor to it. But the concept of forward modeling interferometric data is quite general, and with a few additions the MPoL package could be applied to imaging problems involving Fourier data from optical and infrared telescopes (if this describes your dataset, please get in touch).

Intereferometers acquire samples of data in the Fourier domain, also called the visibility domain. The visibility domain is the Fourier transform of the image sky brightness

.. math::

    {\cal V}(u,v) = \iint I(l,m) \exp \left \{- 2 \pi i (ul + vm) \right \} \, \mathrm{d}l\,\mathrm{d}m.

Here :math:`l` and :math:`m` are direction cosines (roughly equivalent to R.A. and Dec) which parameterize the surface brightness distribution of the image :math:`I(l,m)`, and :math:`u` and :math:`v` are the spatial frequencies which parameterize the visibility function :math:`\cal{V}(u,v)`. For more information on the meaning of these units, see :ref:`units-conventions-label`.

The visibility function is complex-valued, and each measurement of it (denoted by :math:`V_i`) is made in the presence of noise

.. math::

    V_i = \mathcal{V}(u_i, v_i) + \epsilon.

Here :math:`\epsilon` represents a noise realization from a `complex normal <https://en.wikipedia.org/wiki/Complex_normal_distribution>`__ (Gaussian) distribution. Thankfully, most interferometric datasets do not exhibit significant covariance between the real and imaginary noise components, so we could equivalently say that the real and imaginary components of the noise are separately generated by draws from normal distributions characterized by standard deviation :math:`\sigma`

.. math::

    \epsilon_\Re \sim \mathcal{N}(0, \sigma) \\
    \epsilon_\Im \sim \mathcal{N}(0, \sigma)

and

.. math::

    \epsilon = \epsilon_\Re + i \epsilon_\Im

Radio interferometers will commonly represent the uncertainty on each visibility measurement by a "weight" :math:`w_i`, where

.. math::

    w_i = \frac{1}{\sigma_i^2}

A full interferometric dataset is a collection of visibility measurements, which we represent by

.. math::

    \boldsymbol{V} = \{V_1, V_2, \ldots \}_{i=1}^N

For reference, a typical ALMA dataset might contain a half-million individual visibility samples, acquired over a range of spatial frequencies.

.. seealso::

    A full introduction to Fourier transforms, radio astronomy, and interferometry is beyond the scope of this introduction. However, here are some additional resources that we recommend checking out.

    * `Essential radio astronomy <https://www.cv.nrao.edu/~sransom/web/xxx.html>`__ textbook by James Condon and Scott Ransom, and in particular, Chapter 3.7 on Radio Interferometry.
    * NRAO's `17th Synthesis Imaging Workshop <http://www.cvent.com/events/virtual-17th-synthesis-imaging-workshop/agenda-0d59eb6cd1474978bce811194b2ff961.aspx>`__ recorded lectures and slides available
    * `Interferometry and Synthesis in Radio Astronomy <https://ui.adsabs.harvard.edu/abs/2017isra.book.....T/abstract>`__ by Thompson, Moran, and Swenson. An excellent and comprehensive reference on all things interferometry.


Likelihood functions and parameter inference
--------------------------------------------

A simple likelihood function to fit a line
++++++++++++++++++++++++++++++++++++++++++

Typically, when astronomers fit a model to some dataset, such as a line :math:`y = m x + b` to a collection of :math:`\boldsymbol{X} = \{x_1, x_2, \ldots\, x_N\}` and :math:`\boldsymbol{Y} = \{y_1, y_2, \ldots\, y_N\}` points, we require a likelihood function. Simply put, the likelihood function specifies the probability of the data, given a model, and encapsulates our assumptions about the data and noise generating processes.

For the line example, let's say each :math:`y_i` data point is generated by

.. math::

    y_i = m x_i + b + \epsilon

where :math:`\epsilon` is a noise realization from a standard normal distribution with standard deviation :math:`\sigma`, i.e.,

.. math::

    \epsilon \sim \mathcal{N}(0, \sigma).

This information about the data and noise generating process means that we can write down a likelihood function to calculate the probability of the data, given a set of model parameters. The likelihood function is :math:`p(\boldsymbol{Y} |\,\boldsymbol{\theta})`. Sometimes it is written as :math:`\mathcal{L}(\boldsymbol{Y} |\,\boldsymbol{\theta})`, and frequently, when employed in computation, we'll use the logarithm of the likelihood function, or "log-likelihood," :math:`\ln \mathcal{L}`. Let's call :math:`\boldsymbol{\theta} = \{m, b\}` and :math:`M(x_i |\, \boldsymbol{\theta}) = m x_i + b`. The likelihood function for this line problem is

.. math::

    \mathcal{L}(\boldsymbol{Y} |\,\boldsymbol{\theta}) = \prod_i^N \frac{1}{\sqrt{2 \pi} \sigma} \exp \left [ - \frac{(y_i - M(x_i |\,\boldsymbol{\theta}))^2}{2 \sigma^2}\right ]

The logarithm of the likelihood function is

.. math::

    \ln \mathcal{L}(\boldsymbol{Y} |\,\boldsymbol{\theta}) = -N \ln(\sqrt{2 \pi} \sigma) - \frac{1}{2} \sum_i^N \frac{(y_i - M(x_i |\,\boldsymbol{\theta}))^2}{\sigma^2}

You may recognize the right hand term looks similar to the :math:`\chi^2` metric,

.. math::

    \chi^2(\boldsymbol{Y} |\,\boldsymbol{\theta}) = \sum_i^N \frac{(y_i - M(x_i |\,\boldsymbol{\theta}))^2}{\sigma^2}


Assuming that the uncertainty (:math:`\sigma`) on each data point is known (and remains constant), the first term in the log likelihood expression remains constant, and we have

.. math::

    \ln \mathcal{L}(\boldsymbol{Y} |\,\boldsymbol{\theta}) = - \frac{1}{2} \chi^2 (\boldsymbol{Y} |\,\boldsymbol{\theta}) + C

where :math:`C` is a constant with respect to the model parameters. It is common to use shorthand to say that "the likelihood function is :math:`\chi^2`" to indicate situations where the data uncertainties are Gaussian. Very often, we (or others) are interested in the parameter values :math:`\boldsymbol{\theta}_\mathrm{MLE}` which maximize the likelihood function. Unsurprisingly, these parameters are called the *maximum likelihood estimate* (or MLE), and usually they represent something like a "best-fit" model. [#mle_solution]_

When it comes time to do parameter inference, however, it's important to keep in mind

1) the simplifying assumptions we made about the noise uncertainties being constant with respect to the model parameters. If we were to "fit for the noise" in a hierarchical model, for example, we would need to use the full form of the log-likelihood function, including the :math:`-N \ln \left (\sqrt{2 \pi} \sigma \right)` term.
2) that in order to maximize the likelihood function we want to *minimize* the :math:`\chi^2` function.
3) that constants of proportionality (e.g., the :math:`1/2` in front of the :math:`\chi^2`) can matter when combining likelihood functions with prior distributions for Bayesian parameter inference. We'll have more to say on this in a second when we talk about regularizers and their strengths.

To be specific, :math:`\chi^2` is not the end of the story when we'd like to perform Bayesian parameter inference. To do so, we need the posterior probability distribution of the model parameters given the dataset, :math:`p(\boldsymbol{\theta}|\,\boldsymbol{Y})`. We can calculate this quantity using Bayes rule

.. math::

    p(\boldsymbol{\theta}|\,\boldsymbol{Y}) = \frac{p(\boldsymbol{Y}|\,\boldsymbol{\theta})\, p(\boldsymbol{\theta})}{p(\boldsymbol{Y})}

The denominator is a constant so long as the model specification remains the same, leaving

.. math::

    p(\boldsymbol{\theta}|\,\boldsymbol{Y}) \propto p(\boldsymbol{Y}|\,\boldsymbol{\theta})\, p(\boldsymbol{\theta}).

So we need a prior probability distribution :math:`p(\boldsymbol{\theta})` in addition to the likelihood function to calculate the posterior probability distribution of the model parameters. Analogous to the maximum likelihood estimate, there is also the *maximum a posteriori* estimate (or MAP), which includes the effect of the prior probability distribution.

.. seealso::

    Useful resources on Bayesian inference include

    * `Data Analysis: A Bayesian Tutorial <https://www.amazon.com/Data-Analysis-Bayesian-Devinderjit-Sivia/dp/0198568320>`__ by Sivia and Skilling
    * `Data analysis recipes: Fitting a model to data <https://ui.adsabs.harvard.edu/abs/2010arXiv1008.4686H/abstract>`__ by Hogg, Bovy, and Lang
    * `Data analysis recipes: Probability calculus for inference <https://ui.adsabs.harvard.edu/abs/2012arXiv1205.4446H/abstract>`__ by Hogg


Likelihood functions for Fourier data
+++++++++++++++++++++++++++++++++++++

Now that we've introduced likelihood functions in general, let's discuss some of the likelihood functions for inference with Fourier data. As before, our statement about the data generating process

.. math::

    V_i = \mathcal{V}(u_i, v_i) + \epsilon

leads us to the formulation of the likelihood function.

First, let's assume we have some model that we'd like to fit to our dataset. To be a full forward model, it should be able to predict the value of the visibility function for any spatial frequency, i.e., we need to be able to calculate :math:`\mathcal{V}(u, v) = M_\mathcal{V}(u, v |, \boldsymbol{\theta})`.

It's difficult to reason about all but the simplest models directly in the Fourier plane, so usually models are constructed in the image plane :math:`M_I(l,m |,\boldsymbol{\theta})` and then Fourier transformed (either analytically, or via the FFT) to construct visibility models :math:`M_\mathcal{V}(u, v |, \boldsymbol{\theta}) \leftrightharpoons M_I(l,m |,\boldsymbol{\theta})`. For example, these models could be channel maps of carbon monoxide emission from a rotating protoplanetary disk (as in `Czekala et al. 2015 <https://ui.adsabs.harvard.edu/abs/2015ApJ...806..154C/abstract>`__, where :math:`\boldsymbol{\theta}` contains parameters setting the structure of the disk), or rings of continuum emission from a protoplanetary disk (as in `Guzmán et al. 2018 <https://ui.adsabs.harvard.edu/abs/2018ApJ...869L..48G/abstract>`__, where :math:`\boldsymbol{\theta}` contains parameters setting the sizes and locations of the rings).

Following the discussion about how the complex noise realization :math:`\epsilon` is generated, this leads to a log likelihood function

.. math::

    \ln \mathcal{L}(\boldsymbol{V}|\,\boldsymbol{\theta}) = - \frac{1}{2} \chi^2(\boldsymbol{V}|\,\boldsymbol{\theta}) + C

Because the data and model are complex-valued, :math:`\chi^2` is evaluated as

.. math::

    \chi^2(\boldsymbol{V}|\,\boldsymbol{\theta}) = \sum_i^N \frac{|V_i - M_\mathcal{V}(u_i, v_i |\,\boldsymbol{\theta})|^2}{\sigma_i^2}


where :math:`| |` denotes the modulus squared. Equivalently, the calculation can be broken up into sums over the real (:math:`\Re`) and imaginary (:math:`\Im`) components of the visibility data and model

.. math::

    \chi^2(\boldsymbol{V}|\,\boldsymbol{\theta}) = \sum_i^N \frac{(V_{\Re,i} - M_\mathcal{V,\Re}(u_i, v_i |\,\boldsymbol{\theta}))^2}{\sigma_i^2} + \sum_i^N \frac{(V_{\Im,i} - M_\mathcal{V,\Im}(u_i, v_i |\,\boldsymbol{\theta}))^2}{\sigma_i^2}

Now with the likelihood function specified, we can add prior probability distributions :math:`p(\boldsymbol{\theta})`, and calculate and explore the posterior probability distribution of the model parameters using algorithms like Markov Chain Monte Carlo. In this type of Bayesian inference, we're usually using forward models constructed with a small to medium number of parameters (e.g., 10 - 30), like in the protoplanetary disk examples of `Czekala et al. 2015 <https://ui.adsabs.harvard.edu/abs/2015ApJ...806..154C/abstract>`__ or `Guzmán et al. 2018 <https://ui.adsabs.harvard.edu/abs/2018ApJ...869L..48G/abstract>`__.

.. note::

    Even though we would say that "traditional" Bayesian parameter inference is not the main focus of RML algorithms, it is entirely `possible with the MPoL package <https://github.com/MPoL-dev/MPoL/issues/33>`__. In fact, the gradient-based nature of the MPoL package (discussed in a moment) can make posterior exploration very fast using efficient Hamiltonian Monte Carlo samplers.

.. note::

    The :math:`\chi^2` likelihood function as formulated above is appropriate for visibilities with minimal spectral covariance. When modeling spectral line datasets, in particular those that have not been channel-averaged and retain the spectral response function from their Hann windowing, this covariance must be taken into account in the likelihood function. More information on how to derive these covariance matrices is provided in the appendices of `Loomis et al. 2018 <https://ui.adsabs.harvard.edu/abs/2018AJ....155..182L/abstract>`__ and will be detailed in forthcoming tutorials.

RML images as non-parametric models
-----------------------------------

Now that we've introduced what it means to forward-model a dataset and how to calculate a likelihood function, let's talk about non-parametric models.

Say that our :math:`\boldsymbol{X} = \{x_1, x_2, \ldots\, x_N\}` and :math:`\boldsymbol{Y} = \{y_1, y_2, \ldots\, y_N\}` dataset looked a bit more structured than a simple :math:`y = mx + b` relationship. We could expand the model by adding more parameters, for example, by adding quadratic and cubic terms, e.g., :math:`y = a_0 + a_1 x + a_2 x^2 + a_3 x^3`. This would be a reasonable approach, especially if the parameters :math:`a_2`, :math:`a_3`, etc... had physical meaning. But if all that we're interested in is modeling the relationship between :math:`y = f(x)` in order to make predictions, we could just as easily use a `non-parametric model <https://www.section.io/engineering-education/parametric-vs-nonparametric/>`__, like a `spline <https://en.wikipedia.org/wiki/Spline_(mathematics)>`__ or a `Gaussian process <https://distill.pub/2019/visual-exploration-gaussian-processes/>`__.

With RML imaging, we're trying to come up with a model that will fit the dataset. But rather than using a parametric model like a protoplanetary disk structure model or a series of Gaussian rings, we're using a non-parametric model of *the image itself*. This could be as simple as parameterizing the image using the intensity values of the pixels themselves, i.e.,

.. math::

    \boldsymbol{\theta} = \{I_1, I_2, \ldots, I_{N^2} \}

assuming we have an :math:`N \times N` image.

A flexible image model is mostly analogous to using a spline or Gaussian process to fit a series of :math:`\boldsymbol{X} = \{x_1, x_2, \ldots\, x_N\}` and :math:`\boldsymbol{Y} = \{y_1, y_2, \ldots\, y_N\}` points---the model will nearly always have enough flexibility to capture the structure that exists in the dataset. The most straightforward formulation of a non-parametric image model is the pixel basis set, but we could also use more sophisticated basis sets like a set of wavelet coefficients, or even more exotic basis sets constructed from trained neural networks. These may have some serious advantages when it comes to the "regularizing" part of "regularized maximum likelihood" imaging. But first, let's talk about the "maximum likelihood" part.

Given some image parameterization, we would like to find the maximum likelihood image :math:`\boldsymbol{\theta}_\mathrm{MLE}`. Fortunately, because the Fourier transform is a linear operation, we can analytically calculate the maximum solution (the same way we might find the best-fit slope and intercept for the line example). This maximum likelihood solution is called (in the radio astronomy world) the dirty image, and its associated point spread function is called the dirty beam.

In the construction of the dirty image, all unsampled spatial frequencies are set to zero power. This means that the image will only contain spatial frequencies about which we have at least some data. This assumption, however, rarely translates into good image fidelity, especially if there are many unsampled spatial frequencies which carry significant power. The dirty image is also not unique as an image that maximizes the likelihood function. From the perspective of the likelihood calculation, we could set those unsampled spatial frequencies to whatever power we might like, and, because they are *unsampled*, the value of the likelihood calculation won't change, i.e., it will still remain maximal.

When synthesis imaging is described as an "ill-posed inverse problem," this is what is meant. There is a (potentially infinite) range of images that could *exactly* fit the dataset, and without additional information we have no way of discriminating which is best. As you might suspect, this is now where the "regularization" part of "regularized maximum likelihood" imaging comes in.

There are a number of different ways to talk about regularization. If one wants to be Bayesian about it, one would talk about specifying *priors*, i.e., we introduce terms like :math:`p(\boldsymbol{\theta})` such that we might calculate the maximum a posteriori (MAP) image :math:`\boldsymbol{\theta}_\mathrm{MAP}` using the posterior probability distribution

.. math::

    p(\boldsymbol{\theta} |\, \boldsymbol{V}) \propto \mathcal{L}(\boldsymbol{V} |\, \boldsymbol{\theta}) \, p(\boldsymbol{\theta}).

For computational reasons related to numerical over/underflow, we would most likely use the logarithm of the posterior probability distribution

.. math::

    \ln p(\boldsymbol{\theta} |\, \boldsymbol{V}) \propto \ln \mathcal{L}(\boldsymbol{V} |\, \boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}).

One could accomplish the same goal without necessarily invoking the Bayesian language by simply talking about which parameters :math:`\boldsymbol{\theta}` optimize some objective function.

We'll adopt the perspective that we have some objective "cost" function that we'd like to *minimize* to obtain the optimal parameters :math:`\hat{\boldsymbol{\theta}}`. The machine learning community calls this a "loss" function :math:`L(\boldsymbol{\theta})`, and so we'll borrow that terminology here. For an unregularized fit, an acceptable loss function is just the negative log likelihood ("nll") term,

.. math::

    L(\boldsymbol{\theta}) = L_\mathrm{nll}(\boldsymbol{\theta}) = - \ln \mathcal{L}(\boldsymbol{V}|\,\boldsymbol{\theta}) = \frac{1}{2} \chi^2(\boldsymbol{V}|\,\boldsymbol{\theta})

If we're only interested in :math:`\hat{\boldsymbol{\theta}}`, it doesn't matter whether we include the :math:`1/2` prefactor in front of :math:`\chi^2`, the loss function will still have the same optimum. However, when it comes time to add additional terms to the loss function, these prefactors matter in controlling the relative strength of each term.

When phrased in the terminology of function optimization, additional terms can be described as regularization penalties. To be specific, let's add a term that regularizes the sparsity of an image.

.. math::

    L_\mathrm{sparsity}(\boldsymbol{\theta}) = \sum_i |I_i|

This prior is described in more detail in the `API documentation <api.html#mpol.losses.sparsity>`__. In short, the L1 norm promotes sparse solutions (solutions where many pixel values are zero). The combination of these two terms leads to a new loss function

.. math::

    L(\boldsymbol{\theta}) = L_\mathrm{nll}(\boldsymbol{\theta}) + \lambda_\mathrm{sparsity} L_\mathrm{sparsity}(\boldsymbol{\theta})

Where we control the relative "strength" of the regularization via the scalar prefactor :math:`\lambda_\mathrm{sparsity}`. If :math:`\lambda_\mathrm{sparsity} = 0`, no sparsity regularization is applied. Non-zero values of :math:`\lambda_\mathrm{sparsity}` will add in regularization that penalizes non-sparse :math:`\boldsymbol{\theta}` values. How strong this penalization is depends on the strength relative to the other terms in the loss calculation. [#relative_strength]_

We can equivalently specify this using Bayesian terminology, such that

.. math::

    p(\boldsymbol{\theta} |\,\boldsymbol{V}) = \mathcal{L}(\boldsymbol{V}|\,\boldsymbol{\theta}) \, p(\boldsymbol{\theta})

where

.. math::

    p(\boldsymbol{\theta}) = C \exp \left (-\lambda_\mathrm{sparsity} \sum_i | I_i| \right)

and :math:`C` is a normalization factor.

.. seealso::

    That's RML imaging in a nutshell, but we've barely scratched the surface. We highly recommend checking out the following excellent resources.

    * The fourth paper in the 2019 `Event Horizon Telescope Collaboration series <https://ui.adsabs.harvard.edu/abs/2019ApJ...875L...4E/abstract>`__ describing the imaging principles
    * `Maximum entropy image restoration in astronomy <https://ui.adsabs.harvard.edu/abs/1986ARA%26A..24..127N/abstract>`__ AR&A by Narayan and Nityananda 1986
    * `Multi-GPU maximum entropy image synthesis for radio astronomy   <https://ui.adsabs.harvard.edu/abs/2018A%26C....22...16C/abstract>`__ by Cárcamo et al. 2018

.. note::

    RML imaging is different from CLEAN imaging, which operates as a deconvolution procedure in the image plane. At least at sub-mm and radio wavelengths, CLEAN is by far the dominant algorithm used to synthesize images from interferometric data. Therefore, if you're interested in RML imaging, it's worth first understanding the basics of the CLEAN algorithm.

    Here are some useful resources on the CLEAN algorithm.

    * `Interferometry and Synthesis in Radio Astronomy <https://ui.adsabs.harvard.edu/abs/2017isra.book.....T/abstract>`__ Chapter 11.1
    * `CASA documentation on tclean <https://casa.nrao.edu/casadocs-devel/stable/imaging/synthesis-imaging>`__
    * David Wilner's lecture on `Imaging and Deconvolution in Radio Astronomy <https://www.youtube.com/watch?v=mRUZ9eckHZg>`__


The MPoL package for Regularized Maximum Likelihood imaging
-----------------------------------------------------------

Hopefully we've provided.

What's new here? Autodifferentiation. Opportunities for expansion. And the tight integration with PyTorch and neural networks. Easy to run on the GPU (link)

2) Getting started with imaging (links to CASA, other imaging software)
3) Getting started with PyTorch


Existing RML packages. Encourage you to check out.

This package is meant to be modular.


Some advantages to doing RML imaging. Provides an alternative to assessing image quality w/ tclean.

Essentially model fitting
Likelihood. Loss functions. (link). Different formulations between Bayesian probability and/or regularizer formulation. An excellent resource here is the EHT-IV paper.

All of this is in contrast to the CLEAN algorithm, which operates as an image-plane deconvolution algorithm.

Machine learning language as a "loss."

Writing things outside of Bayesian language, we can also state this as a likelihood function, or


### Introduction to Regularized Maximum Likelihood (RML) Imaging

Regularized Maximum Likelihood (RML) imaging is a forward modeling methodology. We predict an image then represent it as an array of pixels. This is brought to the visibility domain through a forward Fourier transform. This is then compared to the measured data to ensure it is a viable predicted image. Due to information loss of the true image when obtaining the measured data, several predicted images- including incorrect ones- will match. To get to our best predicted image, we make new predictions by choosing the most likely (Maximum Likelihood) configuration and favoring specified criteria (Regularized). These criteria or regularizers, are chosen by the user. Some examples of favored criteria are smoothness and sparsity. The likeliness and how well a predicted image meet a certain criterion is mathematically represented in a loss function that contains hyperparameters used to weight data and regularizers. We minimize this loss function by performing a gradient descent, in which we adjust the pixel value intensities. Within this optimization run, hyperparameters are usually held fixed, but can be tuned between runs to produce a better image. When the loss function is minimized, our predicted image is at its best version to fit the collected data and follow our specified criterion.



- Package for synthesis imaging and model fitting from interferometric data.
- Built on PyTorch provides state of the art autodifferentiation capabilities
- Well tested, stable, on supported Python versions. Always a goal of core, usable routines in PyPi releases (i.e., `pip install mpol`). Maintainability.
- Scalability. By keeping modules modular, and *open* and emphasizing the building of imaging components rather than a single, monolithic function, the interested user can expand their applications.

Show the example of HD 143006 CLEAN vs. RML as example of why you might want to use this package… resolution, sensitivity, independent characterization of interesting features.


.. seealso::

    We also recommend checking out several of the other excellent packages for RML imaging.

    * SMILI
    * eht-imaging
    * GPUVMEM

These could be nice videos, but aspects of them probably need to be tutorials first.

 * Autodifferentiation
     * neural networks, deep learning, graident descent, JAX
 * Layers + Nodes w/in Neural landscape
    * Input and Output connections.
    * Relation of "loss" to Bayesian inference
 * RML Imaging as forward modeling
     * optimization as training

Following on from the layer discussion, and the relationship to Bayesian inference, the idea is that there is some set of parameters that maximize the posterior.


One approach would be to combine all of the data into a single container, and just train/optimize off of that.


But let's say you had a combination of multiple datasets, from different telescope and there was an unknown calibration factor for each telescope.


This approach would be to "batch" the data in the training loop, and train in each step. This training loop is commonly to other neural network architectures.



.. rubric:: Footnotes

.. [#mle_solution] There's actually a lot to unpack here. When your model has many parameters (i.e., the posterior distribution is high dimensional), the MLE (or MAP) solution is unlikely to represent a *typical* realization of your model parameters. This is a quirk of the geometry of high dimensional spaces. For more information, we recommend checking out Chapter 1 of `Betancourt 2017 <https://arxiv.org/abs/1701.02434>`__. Still, the MLE solution is often a useful quantity to communicate.

.. [#relative_strength] This is where the factor of :math:`1/2` in front of :math:`\chi^2` becomes important. You could use something like :math:`L_\mathrm{nll}(\boldsymbol{\theta}) = \chi^2(\boldsymbol{\theta})`, but then you'd need to change the value of :math:`\lambda_\mathrm{sparsity}` to achieve the same relative regularization.
