
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>mpol.losses &#8212; MPoL 0.1.13 documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="https://buttons.github.io/buttons.js"></script>
    <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">MPoL 0.1.13 documentation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  User Guide
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../rml_intro.html">
   Introduction to Regularized Maximum Likelihood Imaging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../installation.html">
   MPoL Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../units-and-conventions.html">
   Units and Conventions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../developer-documentation.html">
   Developer Documentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../api.html">
   API
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ci-tutorials/PyTorch.html">
   Introduction to PyTorch: Tensors and Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ci-tutorials/gridder.html">
   Gridding and diagnostic images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ci-tutorials/optimization.html">
   Intro to RML with MPoL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ci-tutorials/loose-visibilities.html">
   Likelihood functions and model visibilities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ci-tutorials/crossvalidation.html">
   Cross validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ci-tutorials/gpu_setup.html">
   GPU Acceleration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ci-tutorials/initializedirtyimage.html">
   Initializing with the Dirty Image
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../large-tutorials/HD143006_part_1.html">
   HD143006 Tutorial Part 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../large-tutorials/HD143006_part_2.html">
   HD143006 Tutorial Part 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ci-tutorials/fakedata.html">
   Making a Mock Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../large-tutorials/pyro.html">
   Parametric Inference with Pyro
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../changelog.html">
   Changelog
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/MPoL-dev/MPoL"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for mpol.losses</h1><div class="highlight"><pre>
<span></span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The following loss functions are available to use in imaging. Many of the definitions follow those in Appendix A of `EHT-IV 2019 &lt;https://ui.adsabs.harvard.edu/abs/2019ApJ...875L...4E/abstract&gt;`_, including the regularization strength, which aspires to be similar across all terms, providing at least a starting point for tuning multiple loss functions.</span>

<span class="sd">If you don&#39;t see a loss function you need, it&#39;s easy to write your own directly within your optimization script. If you like it, please consider opening a pull request!</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">.constants</span> <span class="kn">import</span> <span class="o">*</span>


<div class="viewcode-block" id="chi_squared"><a class="viewcode-back" href="../../api.html#mpol.losses.chi_squared">[docs]</a><span class="k">def</span> <span class="nf">chi_squared</span><span class="p">(</span><span class="n">model_vis</span><span class="p">,</span> <span class="n">data_vis</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the :math:`\chi^2` between the complex data :math:`\boldsymbol{V}` and model :math:`M` visibilities using</span>

<span class="sd">    .. math::</span>

<span class="sd">        \chi^2(\boldsymbol{V}|\,\boldsymbol{\theta}) = \sum_i^N \frac{|V_i - M(u_i, v_i |\,\boldsymbol{\theta})|^2}{\sigma_i^2}</span>

<span class="sd">    where :math:`\sigma_i^2 = 1/w_i`. The sum is over all of the provided visibilities. This function is agnostic as to whether the sum should include the Hermitian conjugate visibilities, but be aware that the answer returned will be different between the two cases. We recommend not including the Hermitian conjugates.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_vis (PyTorch complex): array tuple of the model representing :math:`\boldsymbol{V}`</span>
<span class="sd">        data_vis (PyTorch complex): array of the data values representing :math:`M`</span>
<span class="sd">        weight (PyTorch real): array of weight values representing :math:`w_i`</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: the :math:`\chi^2` likelihood</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># print(&quot;inside chi_squared&quot;)</span>
    <span class="c1"># print(&quot;model&quot;, model_vis.shape)</span>
    <span class="c1"># print(&quot;data&quot;, data_vis.shape)</span>
    <span class="c1"># print(&quot;weight&quot;, weight.shape)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">data_vis</span> <span class="o">-</span> <span class="n">model_vis</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span></div>


<div class="viewcode-block" id="log_likelihood"><a class="viewcode-back" href="../../api.html#mpol.losses.log_likelihood">[docs]</a><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">model_vis</span><span class="p">,</span> <span class="n">data_vis</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the log likelihood function :math:`\ln\mathcal{L}` between the complex data :math:`\boldsymbol{V}` and model :math:`M` visibilities using</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ln \mathcal{L}(\boldsymbol{V}|\,\boldsymbol{\theta}) = - \left ( N \ln 2 \pi +  \sum_i^N \sigma_i^2 + \frac{1}{2} \chi^2(\boldsymbol{V}|\,\boldsymbol{\theta}) \right )</span>

<span class="sd">    where :math:`\chi^2` is evaluated using :func:`mpol.losses.chi_squared`.</span>

<span class="sd">    This function is agnostic as to whether the sum should include the Hermitian conjugate visibilities, but be aware that the normalization of the answer returned will be different between the two cases. Inference of the parameter values should be unaffected. We recommend not including the Hermitian conjugates.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_vis (PyTorch complex): array tuple of the model representing :math:`\boldsymbol{V}`</span>
<span class="sd">        data_vis (PyTorch complex): array of the data values representing :math:`M`</span>
<span class="sd">        weight (PyTorch real): array of weight values representing :math:`w_i`</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: the :math:`\ln\mathcal{L}` log likelihood</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># If model and data are multidimensional, then flatten them to get full N</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">data_vis</span><span class="p">))</span>

    <span class="n">sigma_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">weight</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">N</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">sigma_term</span>
        <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">chi_squared</span><span class="p">(</span><span class="n">model_vis</span><span class="p">,</span> <span class="n">data_vis</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="nll"><a class="viewcode-back" href="../../api.html#mpol.losses.nll">[docs]</a><span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="n">model_vis</span><span class="p">,</span> <span class="n">data_vis</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate a normalized &quot;negative log likelihood&quot; loss between the complex data :math:`\boldsymbol{V}` and model :math:`M` visibilities using</span>

<span class="sd">    .. math::</span>

<span class="sd">        L_\mathrm{nll} = \frac{1}{2 N} \chi^2(\boldsymbol{V}|\,\boldsymbol{\theta})</span>

<span class="sd">    where :math:`\chi^2` is evaluated using :func:`mpol.losses.chi_squared`. Visibilities may be any shape as long as all quantities have the same shape. Following `EHT-IV 2019 &lt;https://ui.adsabs.harvard.edu/abs/2019ApJ...875L...4E/abstract&gt;`_, we apply</span>
<span class="sd">    a prefactor :math:`1/(2 N)`, where :math:`N` is the number of visibilities. The factor of 2 comes in because we must count real and imaginaries in the :math:`\chi^2` sum. This means that this normalized negative log likelihood loss function will have a minimum value of $L_\mathrm{nll}(\hat{\boldsymbol{\theta}}) \approx 1$ for a well-fit model (regardless of the number of data points), making it easier to set the prefactor strengths of other regularizers *relative* to this value.</span>

<span class="sd">    Note that this function should only be used in an optimization or point estimate situation. If it is used in any situation where uncertainties on parameter values are determined (such as Markov Chain Monte Carlo), it will return the wrong answer. This is because the relative scaling of :math:`L_\mathrm{nll}` with respect to parameter value is incorrect.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_vis (PyTorch complex): array tuple of the model representing :math:`\boldsymbol{V}`</span>
<span class="sd">        data_vis (PyTorch complex): array of the data values representing :math:`M`</span>
<span class="sd">        weight (PyTorch real): array of weight values representing :math:`w_i`</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: the normalized negative log likelihood likelihood loss</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># If model and data are multidimensional, then flatten them to get full N</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">data_vis</span><span class="p">))</span>

    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">chi_squared</span><span class="p">(</span><span class="n">model_vis</span><span class="p">,</span> <span class="n">data_vis</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="chi_squared_gridded"><a class="viewcode-back" href="../../api.html#mpol.losses.chi_squared_gridded">[docs]</a><span class="k">def</span> <span class="nf">chi_squared_gridded</span><span class="p">(</span><span class="n">modelVisibilityCube</span><span class="p">,</span> <span class="n">griddedDataset</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the :math:`\chi^2` (corresponding to :func:`~mpol.losses.chi_squared`) using gridded data and model visibilities.</span>

<span class="sd">    Args:</span>
<span class="sd">        modelVisibilityCube (torch complex tensor): torch tensor with shape ``(nchan, npix, npix)`` to be indexed by the ``mask`` from :class:`~mpol.datasets.GriddedDataset`. Assumes tensor is &quot;pre-packed,&quot; as in output from :meth:`mpol.fourier.FourierCube.forward()`.</span>
<span class="sd">        griddedDataset: instantiated :class:`~mpol.datasets.GriddedDataset` object</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: the :math:`\chi^2` value</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># get the model_visibilities from the dataset</span>
    <span class="c1"># 1D torch tensor collapsed across cube dimensions, like</span>
    <span class="c1"># griddedDataset.vis_indexed and griddedDataset.weight_indexed</span>
    
    <span class="n">model_vis</span> <span class="o">=</span> <span class="n">griddedDataset</span><span class="p">(</span><span class="n">modelVisibilityCube</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">chi_squared</span><span class="p">(</span>
        <span class="n">model_vis</span><span class="p">,</span> <span class="n">griddedDataset</span><span class="o">.</span><span class="n">vis_indexed</span><span class="p">,</span> <span class="n">griddedDataset</span><span class="o">.</span><span class="n">weight_indexed</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="log_likelihood_gridded"><a class="viewcode-back" href="../../api.html#mpol.losses.log_likelihood_gridded">[docs]</a><span class="k">def</span> <span class="nf">log_likelihood_gridded</span><span class="p">(</span><span class="n">modelVisibilityCube</span><span class="p">,</span> <span class="n">griddedDataset</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the log likelihood function :math:`\ln\mathcal{L}` (corresponding to :func:`~mpol.losses.log_likelihood`) using gridded data and model visibilities.</span>

<span class="sd">    Args:</span>
<span class="sd">        modelVisibilityCube (torch complex tensor): torch tensor with shape ``(nchan, npix, npix)`` to be indexed by the ``mask`` from :class:`~mpol.datasets.GriddedDataset`. Assumes tensor is &quot;pre-packed,&quot; as in output from :meth:`mpol.fourier.FourierCube.forward()`.</span>
<span class="sd">        griddedDataset: instantiated :class:`~mpol.datasets.GriddedDataset` object</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: the :math:`\ln\mathcal{L}` value</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># get the model_visibilities from the dataset</span>
    <span class="c1"># 1D torch tensor collapsed across cube dimensions, like</span>
    <span class="c1"># griddedDataset.vis_indexed and griddedDataset.weight_indexed</span>
    <span class="n">model_vis</span> <span class="o">=</span> <span class="n">griddedDataset</span><span class="p">(</span><span class="n">modelVisibilityCube</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">log_likelihood</span><span class="p">(</span>
        <span class="n">model_vis</span><span class="p">,</span> <span class="n">griddedDataset</span><span class="o">.</span><span class="n">vis_indexed</span><span class="p">,</span> <span class="n">griddedDataset</span><span class="o">.</span><span class="n">weight_indexed</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="nll_gridded"><a class="viewcode-back" href="../../api.html#mpol.losses.nll_gridded">[docs]</a><span class="k">def</span> <span class="nf">nll_gridded</span><span class="p">(</span><span class="n">modelVisibilityCube</span><span class="p">,</span> <span class="n">griddedDataset</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate a normalized &quot;negative log likelihood&quot; (corresponding to :func:`~mpol.losses.nll`) using gridded data and model visibilities. Function will return the same value regardless of whether Hermitian pairs are included.</span>

<span class="sd">    Args:</span>
<span class="sd">        vis (torch complex tensor): torch tensor with shape ``(nchan, npix, npix)`` to be indexed by the ``mask`` from :class:`~mpol.datasets.GriddedDataset`. Assumes tensor is &quot;pre-packed,&quot; as in output from :meth:`mpol.fourier.FourierCube.forward()`.</span>
<span class="sd">        griddedDataset: instantiated :class:`~mpol.datasets.GriddedDataset` object</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: the normalized negative log likelihood likelihood loss</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_vis</span> <span class="o">=</span> <span class="n">griddedDataset</span><span class="p">(</span><span class="n">modelVisibilityCube</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">nll</span><span class="p">(</span><span class="n">model_vis</span><span class="p">,</span> <span class="n">griddedDataset</span><span class="o">.</span><span class="n">vis_indexed</span><span class="p">,</span> <span class="n">griddedDataset</span><span class="o">.</span><span class="n">weight_indexed</span><span class="p">)</span></div>


<div class="viewcode-block" id="entropy"><a class="viewcode-back" href="../../api.html#mpol.losses.entropy">[docs]</a><span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">cube</span><span class="p">,</span> <span class="n">prior_intensity</span><span class="p">,</span> <span class="n">tot_flux</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the entropy loss of a set of pixels following the definition in `EHT-IV 2019 &lt;https://ui.adsabs.harvard.edu/abs/2019ApJ...875L...4E/abstract&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        cube (any tensor): pixel values must be positive :math:`I_i &gt; 0` for all :math:`i`</span>
<span class="sd">        prior_intensity (any tensor): the prior value :math:`p` to calculate entropy against. Could be a single constant or an array the same shape as image.</span>
<span class="sd">        tot_flux (float): a fixed normalization factor; the user-defined target total flux density</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: entropy loss</span>

<span class="sd">    The entropy loss is calculated as</span>

<span class="sd">    .. math::</span>

<span class="sd">        L = \frac{1}{\zeta} \sum_i I_i \; \ln \frac{I_i}{p_i}</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># check to make sure image is positive, otherwise raise an error</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">cube</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(),</span> <span class="s2">&quot;image cube contained negative pixel values&quot;</span>
    <span class="k">assert</span> <span class="n">prior_intensity</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;image prior intensity must be positive&quot;</span>
    <span class="k">assert</span> <span class="n">tot_flux</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;target total flux must be positive&quot;</span>

    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">tot_flux</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cube</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">cube</span> <span class="o">/</span> <span class="n">prior_intensity</span><span class="p">))</span></div>


<div class="viewcode-block" id="TV_image"><a class="viewcode-back" href="../../api.html#mpol.losses.TV_image">[docs]</a><span class="k">def</span> <span class="nf">TV_image</span><span class="p">(</span><span class="n">sky_cube</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the total variation (TV) loss in the image dimension (R.A. and DEC). Following the definition in `EHT-IV 2019 &lt;https://ui.adsabs.harvard.edu/abs/2019ApJ...875L...4E/abstract&gt;`_ Promotes the image to be piecewise smooth and the gradient of the image to be sparse.</span>

<span class="sd">    Args:</span>
<span class="sd">        sky_cube (any 3D tensor): the image cube array :math:`I_{lmv}`, where :math:`l` is R.A. in :math:`ndim=3`, :math:`m` is DEC in :math:`ndim=2`, and :math:`v` is the channel (velocity or frequency) dimension in :math:`ndim=1`. Should be in sky format representation.</span>
<span class="sd">        epsilon (float): a softening parameter in [:math:`\mathrm{Jy}/\mathrm{arcsec}^2`]. Any pixel-to-pixel variations within each image slice greater than this parameter will have a significant penalty.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: total variation loss</span>

<span class="sd">    .. math::</span>

<span class="sd">        L = \sum_{l,m,v} \sqrt{(I_{l + 1, m, v} - I_{l,m,v})^2 + (I_{l, m+1, v} - I_{l, m, v})^2 + \epsilon}</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># diff the cube in ll and remove the last row</span>
    <span class="n">diff_ll</span> <span class="o">=</span> <span class="n">sky_cube</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">sky_cube</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># diff the cube in mm and remove the last column</span>
    <span class="n">diff_mm</span> <span class="o">=</span> <span class="n">sky_cube</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">sky_cube</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">diff_ll</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">diff_mm</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="TV_channel"><a class="viewcode-back" href="../../api.html#mpol.losses.TV_channel">[docs]</a><span class="k">def</span> <span class="nf">TV_channel</span><span class="p">(</span><span class="n">cube</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the total variation (TV) loss in the channel dimension. Following the definition in `EHT-IV 2019 &lt;https://ui.adsabs.harvard.edu/abs/2019ApJ...875L...4E/abstract&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        cube (any 3D tensor): the image cube array :math:`I_{lmv}`</span>
<span class="sd">        epsilon (float): a softening parameter in [:math:`\mathrm{Jy}/\mathrm{arcsec}^2`]. Any channel-to-channel pixel variations greater than this parameter will have a significant penalty.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: total variation loss</span>

<span class="sd">    .. math::</span>

<span class="sd">        L = \sum_{l,m,v} \sqrt{(I_{l, m, v + 1} - I_{l,m,v})^2 + \epsilon}</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># calculate the difference between the n+1 cube and the n cube</span>
    <span class="n">diff_vel</span> <span class="o">=</span> <span class="n">cube</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">cube</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">diff_vel</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="edge_clamp"><a class="viewcode-back" href="../../api.html#mpol.losses.edge_clamp">[docs]</a><span class="k">def</span> <span class="nf">edge_clamp</span><span class="p">(</span><span class="n">cube</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Promote all pixels at the edge of the image to be zero using an :math:`L_2` norm.</span>

<span class="sd">    Args:</span>
<span class="sd">        cube (any 3D tensor): the array and pixel values</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: edge loss</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># find edge pixels</span>
    <span class="c1"># all channels</span>
    <span class="c1"># pixel edges</span>
    <span class="n">bt_edges</span> <span class="o">=</span> <span class="n">cube</span><span class="p">[:,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">lr_edges</span> <span class="o">=</span> <span class="n">cube</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">bt_edges</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lr_edges</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="sparsity"><a class="viewcode-back" href="../../api.html#mpol.losses.sparsity">[docs]</a><span class="k">def</span> <span class="nf">sparsity</span><span class="p">(</span><span class="n">cube</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enforce a sparsity prior on the image cube using the :math:`L_1` norm. Optionally provide a boolean mask to apply the prior to only the ``True`` locations. For example, you might want this mask to be ``True`` for background regions.</span>

<span class="sd">    Args:</span>
<span class="sd">        cube (nchan, npix, npix): tensor image cube</span>
<span class="sd">        mask (boolean): tensor array the same shape as ``cube``. The sparsity prior will be applied to those pixels where the mask is ``True``. Default is to apply prior to all pixels.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: sparsity loss calculated where ``mask == True``</span>

<span class="sd">    The sparsity loss calculated as</span>

<span class="sd">    .. math::</span>

<span class="sd">        L = \sum_i | I_i |</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">cube</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">mask</span><span class="p">)))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">cube</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="UV_sparsity"><a class="viewcode-back" href="../../api.html#mpol.losses.UV_sparsity">[docs]</a><span class="k">def</span> <span class="nf">UV_sparsity</span><span class="p">(</span><span class="n">vis</span><span class="p">,</span> <span class="n">qs</span><span class="p">,</span> <span class="n">q_max</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enforce a sparsity prior for all :math:`q = \sqrt{u^2 + v^2}` points larger than :math:`q_\mathrm{max}`.</span>

<span class="sd">    Args:</span>
<span class="sd">        vis (torch.double) : visibility cube of (nchan, npix, npix//2 +1, 2)</span>
<span class="sd">        qs: numpy array corresponding to visibility coordinates. Dimensionality of (npix, npix//2)</span>
<span class="sd">        q_max (float): maximum radial baseline</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: UV sparsity loss above :math:`q_\mathrm{max}`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># make a mask, then send it to the device (in case we&#39;re using a GPU)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((</span><span class="n">qs</span> <span class="o">&gt;</span> <span class="n">q_max</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">vis</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">vis_re</span> <span class="o">=</span> <span class="n">vis</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">vis_im</span> <span class="o">=</span> <span class="n">vis</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1"># broadcast mask to the same shape as vis</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">vis_re</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">mask</span><span class="p">)))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">vis_im</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="PSD"><a class="viewcode-back" href="../../api.html#mpol.losses.PSD">[docs]</a><span class="k">def</span> <span class="nf">PSD</span><span class="p">(</span><span class="n">qs</span><span class="p">,</span> <span class="n">psd</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply a loss function corresponding to the power spectral density using a Gaussian process kernel.</span>

<span class="sd">    Assumes an image plane kernel of</span>

<span class="sd">    .. math::</span>

<span class="sd">        k(r) = exp(-\frac{r^2}{2 \ell^2})</span>

<span class="sd">    The corresponding power spectral density is</span>

<span class="sd">    .. math::</span>

<span class="sd">        P(q) = (2 \pi \ell^2) exp(- 2 \pi^2 \ell^2 q^2)</span>


<span class="sd">    Args:</span>
<span class="sd">        qs (torch.double): the radial UV coordinate (in kilolambda)</span>
<span class="sd">        psd (torch.double): the power spectral density cube</span>
<span class="sd">        l (torch.double): the correlation length in the image plane (in arcsec)</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double : the loss calculated using the power spectral density</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># stack to the full 3D shape</span>
    <span class="n">qs</span> <span class="o">=</span> <span class="n">qs</span> <span class="o">*</span> <span class="mf">1e3</span>  <span class="c1"># lambda</span>

    <span class="n">l_rad</span> <span class="o">=</span> <span class="n">l</span> <span class="o">*</span> <span class="n">arcsec</span>  <span class="c1"># radians</span>

    <span class="c1"># calculate the expected power spectral density</span>
    <span class="n">expected_PSD</span> <span class="o">=</span> <span class="p">(</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">l_rad</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">l_rad</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">qs</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># evaluate the chi^2 for the PSD, making sure it broadcasts across all channels</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">psd</span> <span class="o">/</span> <span class="n">expected_PSD</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="TSV"><a class="viewcode-back" href="../../api.html#mpol.losses.TSV">[docs]</a><span class="k">def</span> <span class="nf">TSV</span><span class="p">(</span><span class="n">sky_cube</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the total square variation (TSV) loss in the image dimension (R.A. and DEC). Following the definition in `EHT-IV 2019 &lt;https://ui.adsabs.harvard.edu/abs/2019ApJ...875L...4E/abstract&gt;`_ Promotes the image to be edge smoothed which may be a better reoresentation of the truth image `K. Kuramochi et al 2018 &lt;https://ui.adsabs.harvard.edu/abs/2018ApJ...858...56K/abstract&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        sky_cube (any 3D tensor): the image cube array :math:`I_{lmv}`, where :math:`l` is R.A. in :math:`ndim=3`, :math:`m` is DEC in :math:`ndim=2`, and :math:`v` is the channel (velocity or frequency) dimension in :math:`ndim=1`. Should be in sky format representation.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.double: total square variation loss</span>

<span class="sd">    .. math::</span>

<span class="sd">        L = \sum_{l,m,v} (I_{l + 1, m, v} - I_{l,m,v})^2 + (I_{l, m+1, v} - I_{l, m, v})^2</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># diff the cube in ll and remove the last row</span>
    <span class="n">diff_ll</span> <span class="o">=</span> <span class="n">sky_cube</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">sky_cube</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># diff the cube in mm and remove the last column</span>
    <span class="n">diff_mm</span> <span class="o">=</span> <span class="n">sky_cube</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">sky_cube</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">diff_ll</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">diff_mm</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span></div>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ian Czekala<br/>
  
      &copy; Copyright 2019-22, Ian Czekala.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>