

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Cross validation &mdash; MPoL 0.1.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://buttons.github.io/buttons.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
  <link rel="stylesheet" href="../_static/bullets.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/faculty.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans|Roboto:400,700|Roboto+Mono:400,700&display=swap" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="GPU Acceleration" href="gpu_setup.html" />
    <link rel="prev" title="Optimization Loop" href="optimization.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

  
    <a class="heading heading-extra-margin" href="../index.html">
      <div class="logo-box logo-box-large">
        <img class="logo" src="../_static/logo.png"/>
      </div>
      
        <span class="icon icon-home"> MPoL</span>
      
    </a>
  

  
    
    
      <div class="version">0.1.1</div>
    
  

  
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rml_intro.html">Introduction to Regularized Maximum Likelihood Imaging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">MPoL Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../units-and-conventions.html">Units and Conventions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer-documentation.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="PyTorch.html">Introduction to PyTorch: Tensors and Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="gridder.html">Gridding and diagnostic images</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">Optimization Loop</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Cross validation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#k-fold-cross-validation">K-fold cross validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#choosing-the-k-folds">Choosing the K-folds</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-cross-validation-loop">The cross validation loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="#results">Results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gpu_setup.html">GPU Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializedirtyimage.html">Initializing with the Dirty Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../large-tutorials/HD143006_part_1.html">HD143006 Tutorial Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../large-tutorials/HD143006_part_2.html">HD143006 Tutorial Part 2</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MPoL</a>
        
      </nav>


      <div class="wy-nav-content">

  

  
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
  <li class="breadcrumb"><a href="../index.html">MPoL</a> &raquo;</li>
    
  <li class="breadcrumb">Cross validation</li>

    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/ci-tutorials/crossvalidation.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%matplotlib inline
%run notebook_setup
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/runner/work/MPoL/MPoL/docs/ci-tutorials/notebook_setup.py:7: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).
  get_ipython().magic(&#39;config InlineBackend.figure_format = &quot;retina&quot;&#39;)
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="cross-validation">
<h1>Cross validation<a class="headerlink" href="#cross-validation" title="Permalink to this heading">¶</a></h1>
<p>In this tutorial, we’ll design and optimize a more sophisticated imaging workflow. Cross validation will help us build confidence that we are setting the regularization hyperparameters appropriately.</p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">¶</a></h2>
<p>We’ll continue with the same central channel of the ALMA logo measurement set as before. If these commands don’t make sense, please consult the previous tutorials.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib
import matplotlib.pyplot as plt
import numpy as np
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import torch
from astropy.io import fits
from astropy.utils.data import download_file
from torch.utils.tensorboard import SummaryWriter

from mpol import (
    connectors,
    coordinates,
    datasets,
    gridding,
    images,
    losses,
    precomposed,
)

# load the mock dataset of the ALMA logo
fname = download_file(
    &quot;https://zenodo.org/record/4930016/files/logo_cube.noise.npz&quot;,
    cache=True,
    show_progress=True,
    pkgname=&quot;mpol&quot;,
)

# this is a multi-channel dataset... for demonstration purposes we&#39;ll use
# only the central, single channel
chan = 4
d = np.load(fname)
uu = d[&quot;uu&quot;][chan]
vv = d[&quot;vv&quot;][chan]
weight = d[&quot;weight&quot;][chan]
data = d[&quot;data&quot;][chan]
data_re = np.real(data)
data_im = np.imag(data)

# define the image dimensions, making sure they are big enough to fit all
# of the expected emission
coords = coordinates.GridCoords(cell_size=0.03, npix=180)
gridder = gridding.Gridder(
    coords=coords, uu=uu, vv=vv, weight=weight, data_re=data_re, data_im=data_im
)

# export to PyTorch dataset
dset = gridder.to_pytorch_dataset()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Show the dirty image
img, beam = gridder.get_dirty_image(weighting=&quot;briggs&quot;, robust=0.0)
kw = {&quot;origin&quot;: &quot;lower&quot;, &quot;extent&quot;: gridder.coords.img_ext}
fig, ax = plt.subplots(ncols=1)
ax.imshow(np.squeeze(img), **kw)
ax.set_title(&quot;image&quot;)
ax.set_xlabel(r&quot;$\Delta \alpha \cos \delta$ [${}^{\prime\prime}$]&quot;)
ax.set_ylabel(r&quot;$\Delta \delta$ [${}^{\prime\prime}$]&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;$\\Delta \\delta$ [${}^{\\prime\\prime}$]&#39;)
</pre></div>
</div>
<img alt="../_images/552d5bc684d55f070da401b121bd56487374a34040c39acb33dc70a582e73da1.png" src="../_images/552d5bc684d55f070da401b121bd56487374a34040c39acb33dc70a582e73da1.png" />
</div>
</div>
</section>
<section id="k-fold-cross-validation">
<h2>K-fold cross validation<a class="headerlink" href="#k-fold-cross-validation" title="Permalink to this heading">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">K-fold cross validation</a> is a technique used to assess model validity. In the context of RML imaging, we use “model” to describe a whole host of assumptions inherent to the imaging workflow. Model settings include the <code class="docutils literal notranslate"><span class="pre">cell_size</span></code>, the number of pixels, the mapping of the BaseCube to the ImageCube, as well as hyperparameter choices like the strength of the regularizer terms for each type of loss function. Usually we’re most interested in assessing whether we have adequately set hyperparameters (like in this tutorial), but sometimes we’d like to assess model settings too.</p>
<p>If you’re coming from astronomy or astrophysics, you might be most familiar with doing <a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2019arXiv190912313S/abstract">Bayesian parameter inference</a> with all of the data at once. In a typical workflow, you might implicitly assume that your model is valid and explore the shape of the unnormalized posterior distribution using a standard MCMC technique like Metropolis-Hastings. If you did want to compare the validity of models, then you would need to use a sampler which computes the Bayesian evidence, or posterior normalization.</p>
<p>But if you’re coming from the machine learning community, you’re most likely already familiar from the concept of optimizing your model using a “training” dataset and the assessing how well it does using a “test” or “validation” dataset. Astrophysical datasets are typically hard-won, however, so it’s not often that we have a sizeable chunk of data lying around to use as a test set <em>in addition to</em> what we want to incorporate into our training dataset.</p>
<p><span class="math notranslate nohighlight">\(K\)</span>-fold cross validation helps alleviate this concern somewhat by rotating testing/training chunks through the dataset. To implement <span class="math notranslate nohighlight">\(K\)</span>-fold cross validation, first split your dataset into <span class="math notranslate nohighlight">\(K\)</span> (approximately equal) chunks. Then, do the following <span class="math notranslate nohighlight">\(K\)</span> times:</p>
<ul class="simple">
<li><p>store one chunk (<span class="math notranslate nohighlight">\(1/K\)</span>th of the total data) separately as a test dataset</p></li>
<li><p>combine the remaining chunks (<span class="math notranslate nohighlight">\((K-1)/K\)</span> of the total data set) into one dataset and use this to train the model</p></li>
<li><p>use this model to predict the values of the data in the test dataset</p></li>
<li><p>assess the difference between predicted test data and actual test data using a <span class="math notranslate nohighlight">\(\chi^2\)</span> metric, called the cross-validation score</p></li>
</ul>
<p>When all loops are done, you can average the <span class="math notranslate nohighlight">\(K\)</span> cross-validation scores together into a final score for that model configuration. Lower cross validation scores are better in the sense that the trained model did a better job predicting the test data.</p>
<p><strong>Why does this work?</strong> Cross validation is such a useful tool because it tells us how well a model generalizes to new data, with the idea being that a better model will predict new data more accurately. Some more considered thoughts on cross validation and model fitting are in <a class="reference external" href="https://ui.adsabs.harvard.edu/abs/2021arXiv210107256H/abstract">Hogg and Villar</a>.</p>
</section>
<section id="choosing-the-k-folds">
<h2>Choosing the K-folds<a class="headerlink" href="#choosing-the-k-folds" title="Permalink to this heading">¶</a></h2>
<p>There are many ways to split a dataset into <span class="math notranslate nohighlight">\(K\)</span> chunks, and, depending on your application, some schemes are better than others. For most interferometric datasets, visibility samples are clustered in Fourier space due to the limitations on the number and location of the antennas. One objective of cross validation might be figuring out how sparse <span class="math notranslate nohighlight">\(u\)</span>,<span class="math notranslate nohighlight">\(v\)</span> coverage adversely affects our imaging process—ideally we’d like to tune the algorithm such that we would still recover a similar image even if our <span class="math notranslate nohighlight">\(u\)</span>,<span class="math notranslate nohighlight">\(v\)</span> sampling were different. To explore slicing choices, here is the full <span class="math notranslate nohighlight">\(u\)</span>,<span class="math notranslate nohighlight">\(v\)</span> coverage of our ALMA logo mock dataset (C43-7, 1 hour observation)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(nrows=1)
ax.scatter(uu, vv, s=1.5, rasterized=True, linewidths=0.0, c=&quot;k&quot;)
ax.scatter(
    -uu, -vv, s=1.5, rasterized=True, linewidths=0.0, c=&quot;k&quot;
)  # and Hermitian conjugates
ax.set_xlabel(r&quot;$u$ [k$\lambda$]&quot;)
ax.set_ylabel(r&quot;$v$ [k$\lambda$]&quot;)
ax.set_title(&quot;original dataset&quot;)
ax.invert_xaxis()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a8bf462a9081b2cf6d1473f7b922da44897ee34e3beb01fdeed377ad3921e87c.png" src="../_images/a8bf462a9081b2cf6d1473f7b922da44897ee34e3beb01fdeed377ad3921e87c.png" />
</div>
</div>
<p>As you can see, the <span class="math notranslate nohighlight">\(u\)</span>,<span class="math notranslate nohighlight">\(v\)</span> space is sampled in a very structured way:</p>
<ol class="arabic simple">
<li><p>there are no samples at very low spatial frequencies (the center of the image, <span class="math notranslate nohighlight">\(&lt; 10\)</span> k<span class="math notranslate nohighlight">\(\lambda\)</span>)</p></li>
<li><p>most samples like at intermediate spatial frequencies (100 k<span class="math notranslate nohighlight">\(\lambda\)</span> to 800 k<span class="math notranslate nohighlight">\(\lambda\)</span>)</p></li>
<li><p>there are very few samples at high spatial frequencies (<span class="math notranslate nohighlight">\(&gt;\)</span> 1000 k<span class="math notranslate nohighlight">\(\lambda\)</span>)</p></li>
<li><p>there are large gaps in the <span class="math notranslate nohighlight">\(u\)</span>,<span class="math notranslate nohighlight">\(v\)</span> coverage where there are no visibilities, especially at high spatial frequencies</p></li>
</ol>
<p>If we were to just draw randomly from these visibilities, because there are so many (<span class="math notranslate nohighlight">\(&gt;\)</span> 500,000 just in this single-channel figure), we would end up mostly replicating the same structured pattern in <span class="math notranslate nohighlight">\(u\)</span>,<span class="math notranslate nohighlight">\(v\)</span>. For example, here is what random training set might look like if <span class="math notranslate nohighlight">\(K=10\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>nvis = len(uu)
print(2 * nvis, &quot;visibilities total&quot;)
ind = np.random.choice(np.arange(nvis), size=int(9 * nvis / 10), replace=False)

uk = uu[ind]
vk = vv[ind]

fig, ax = plt.subplots(nrows=1)
ax.scatter(uk, vk, s=1.5, rasterized=True, linewidths=0.0, c=&quot;k&quot;)
ax.scatter(
    -uk, -vk, s=1.5, rasterized=True, linewidths=0.0, c=&quot;k&quot;
)  # and Hermitian conjugates
ax.set_xlabel(r&quot;$u$ [k$\lambda$]&quot;)
ax.set_ylabel(r&quot;$v$ [k$\lambda$]&quot;)
ax.set_title(&quot;randomly drawn 9/10 dataset&quot;)
ax.invert_xaxis()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>650160 visibilities total
</pre></div>
</div>
<img alt="../_images/beaf0daa8ea10d216fb37e3b3ed63084bdbb240572a4b683383e966bb3604f65.png" src="../_images/beaf0daa8ea10d216fb37e3b3ed63084bdbb240572a4b683383e966bb3604f65.png" />
</div>
</div>
<p>As you can see, this training set looks very similar to the full dataset, with the same holes in <span class="math notranslate nohighlight">\(u\)</span>,<span class="math notranslate nohighlight">\(v\)</span> coverage and similar sampling densities.</p>
<p>It turns out that the missing holes in the real dataset are quite important to image fidelity—if we had complete <span class="math notranslate nohighlight">\(u\)</span>,<span class="math notranslate nohighlight">\(v\)</span> coverage, we wouldn’t need to be worrying about CLEAN or RML imaging techniques in the first place! When we make a new interferometric observation, it will have it’s own (different) set of missing holes depending on array configuration, observation duration, and hour angle coverage. We would like our cross validation slices to simulate the <span class="math notranslate nohighlight">\(u\)</span>,<span class="math notranslate nohighlight">\(v\)</span> distribution of possible <em>new datasets</em>, and, at least for ALMA, random sampling doesn’t probe this very well.</p>
<p>Instead, we suggest an approach where we break the UV plane into radial (<span class="math notranslate nohighlight">\(q=\sqrt{u^2 + v^2}\)</span>) and azimuthal (<span class="math notranslate nohighlight">\(\phi = \mathrm{arctan2}(v,u)\)</span>) cells and cross validate by drawing a <span class="math notranslate nohighlight">\(K\)</span>-fold subselection of these cells. This is just one potential suggestion. There are, of course, no limits on how you might split your dataset for cross-validation; it really depends on what works best for your imaging goals.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># create a radial and azimuthal partition
dartboard = datasets.Dartboard(coords=coords)

# create cross validator using this &quot;dartboard&quot;
k = 5
cv = datasets.KFoldCrossValidatorGridded(dset, k, dartboard=dartboard, npseed=42)

# ``cv`` is a Python iterator, it will return a ``(train, test)`` pair of ``GriddedDataset``s for each iteration.
# Because we&#39;ll want to revisit the individual datasets
# several times in this tutorial, we&#39;re storeing them into a list
k_fold_datasets = [(train, test) for (train, test) in cv]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>flayer = images.FourierCube(coords=coords)
flayer.forward(torch.zeros(dset.nchan, coords.npix, coords.npix))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],
         [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],
         [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],
         ...,
         [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],
         [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j],
         [0.+0.j, 0.+0.j, 0.+0.j,  ..., 0.+0.j, 0.+0.j, 0.+0.j]]])
</pre></div>
</div>
</div>
</div>
<p>The following plots visualize how we’ve split up the data. For each <span class="math notranslate nohighlight">\(K\)</span>-fold, we have the “training” visibilities, the dirty image corresponding to those training visibilities, and the “test” visibilities which will be used to evaluate the predictive ability of the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(nrows=k, ncols=3, figsize=(6, 10))

for i, (train, test) in enumerate(k_fold_datasets):

    rtrain = connectors.GriddedResidualConnector(flayer, train)
    rtrain.forward()
    rtest = connectors.GriddedResidualConnector(flayer, test)
    rtest.forward()

    vis_ext = rtrain.coords.vis_ext
    img_ext = rtrain.coords.img_ext

    train_mask = rtrain.ground_mask[0]
    train_chan = rtrain.sky_cube[0]

    test_mask = rtest.ground_mask[0]
    test_chan = rtest.sky_cube[0]

    ax[i, 0].imshow(
        train_mask.detach().numpy(),
        interpolation=&quot;none&quot;,
        origin=&quot;lower&quot;,
        extent=vis_ext,
        cmap=&quot;GnBu&quot;,
    )

    ax[i, 1].imshow(train_chan.detach().numpy(), origin=&quot;lower&quot;, extent=img_ext)

    ax[i, 2].imshow(
        test_mask.detach().numpy(), origin=&quot;lower&quot;, extent=vis_ext, cmap=&quot;GnBu&quot;
    )

    ax[i, 0].set_ylabel(&quot;k-fold {:}&quot;.format(i))

ax[0, 0].set_title(&quot;train mask&quot;)
ax[0, 1].set_title(&quot;train dirty img.&quot;)
ax[0, 2].set_title(&quot;test mask&quot;)

for a in ax.flatten():
    a.xaxis.set_ticklabels([])
    a.yaxis.set_ticklabels([])

fig.subplots_adjust(left=0.15, hspace=0.0, wspace=0.2)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/aae9e9832b50f0c551557420d61a6f35d5464686c44ace8ae76ef1a7f16b5cc5.png" src="../_images/aae9e9832b50f0c551557420d61a6f35d5464686c44ace8ae76ef1a7f16b5cc5.png" />
</div>
</div>
</section>
<section id="the-cross-validation-loop">
<h2>The cross validation loop<a class="headerlink" href="#the-cross-validation-loop" title="Permalink to this heading">¶</a></h2>
<p>Building on the previous optimization tutorial, we’ll wrap the iterative optimization commands into a training function. This will come in handy, because we’ll want to train the model on each of the varied <span class="math notranslate nohighlight">\(K\)</span>-fold training datasets. In this tutorial, we’ll use a loss function of the form</p>
<div class="math notranslate nohighlight">
\[
f_\mathrm{loss} = f_\mathrm{nll} + \lambda_\mathrm{sparsity} f_\mathrm{sparsity} + \lambda_{TV} f_\mathrm{TV}
\]</div>
<p>where the <span class="math notranslate nohighlight">\(\lambda\)</span> prefactors are the strength of the regularization terms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def train(model, dset, config, optimizer, writer=None):
    model.train()  # set to training mode

    for i in range(config[&quot;epochs&quot;]):
        model.zero_grad()

        # get the predicted model
        vis = model.forward()

        # get the sky cube too
        sky_cube = model.icube.sky_cube

        # calculate a loss
        loss = (
            losses.nll_gridded(vis, dset)
            + config[&quot;lambda_sparsity&quot;] * losses.sparsity(sky_cube)
            + config[&quot;lambda_TV&quot;] * losses.TV_image(sky_cube)
        )

        if writer is not None:
            writer.add_scalar(&quot;loss&quot;, loss.item(), i)

        # calculate gradients of parameters
        loss.backward()

        # update the model parameters
        optimizer.step()
</pre></div>
</div>
</div>
</div>
<p>We also create a separate “test” function to evaluate the trained model against a set of witheld “test” visibilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def test(model, dset):
    model.train(False)
    # evaluate test score
    vis = model.forward()
    loss = losses.nll_gridded(vis, dset)
    return loss.item()
</pre></div>
</div>
</div>
</div>
<p>Finally, we put the <span class="math notranslate nohighlight">\(K\)</span>-fold iterator, the training function, and test function together into a cross validation training loop. For each <span class="math notranslate nohighlight">\(K\)</span>-fold, we</p>
<ol class="arabic simple">
<li><p>initialize the model and optimizer from scratch</p></li>
<li><p>fully train the model on the “train” slice</p></li>
<li><p>calculate the predictive power of that model on the “test” slice via a cross-validation metric</p></li>
</ol>
<p>when all <span class="math notranslate nohighlight">\(K\)</span>-folds have been iterated through, we sum the individual cross validations scores into a total cross-validation metric for those hyperparameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def cross_validate(config):
    &quot;&quot;&quot;
    config is a dictionary that should contain ``lr``, ``lambda_sparsity``, ``lambda_TV``, ``epochs``
    &quot;&quot;&quot;
    test_scores = []

    for k_fold, (train_dset, test_dset) in enumerate(k_fold_datasets):

        # create a new model and optimizer for this k_fold
        rml = precomposed.SimpleNet(coords=coords, nchan=train_dset.nchan)
        optimizer = torch.optim.Adam(rml.parameters(), lr=config[&quot;lr&quot;])

        # train for a while
        train(rml, train_dset, config, optimizer)
        # evaluate the test metric
        test_scores.append(test(rml, test_dset))

    # aggregate all test scores and sum to evaluate cross val metric
    test_score = np.sum(np.array(test_scores))

    return test_score
</pre></div>
</div>
</div>
</div>
<p>Finally, we’ll write one more function to train the model using the full dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def train_and_image(pars):
    rml = precomposed.SimpleNet(coords=coords, nchan=dset.nchan)
    optimizer = torch.optim.Adam(rml.parameters(), lr=pars[&quot;lr&quot;])
    writer = SummaryWriter()
    train(rml, dset, pars, optimizer, writer=writer)
    writer.close()

    img_ext = rml.coords.img_ext
    fig, ax = plt.subplots()
    ax.imshow(
        np.squeeze(rml.icube.sky_cube.detach().numpy()), origin=&quot;lower&quot;, extent=img_ext
    )
    return fig, ax
</pre></div>
</div>
</div>
</div>
<p>All of the method presented here can be sped up using GPU acceleration on certain Nvidia GPUs. To learn more about this, please see the <a class="reference internal" href="gpu_setup.html#gpu-reference-label"><span class="std std-ref">GPU Setup Tutorial</span></a>.</p>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this heading">¶</a></h2>
<p>As a starting point, we’ll try cross-validating without any regularization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pars = {&quot;lr&quot;: 0.5, &quot;lambda_sparsity&quot;: 0, &quot;lambda_TV&quot;: 0, &quot;epochs&quot;: 600}
print(&quot;Cross validation score:&quot;, cross_validate(pars))
train_and_image(pars)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross validation score: 36.80444112433067
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;Figure size 480x480 with 1 Axes&gt;, &lt;AxesSubplot: &gt;)
</pre></div>
</div>
<img alt="../_images/f5c993d9601f9dfcec572b92e532719b89f5d4a6b3812556c53dbd1599abc13e.png" src="../_images/f5c993d9601f9dfcec572b92e532719b89f5d4a6b3812556c53dbd1599abc13e.png" />
</div>
</div>
<p>This image looks only slightly better than the dirty image itself. This is because it is nearly equivalent, the only difference is the baked-in regularization provided by our non-negative pixels. Next, let’s try a small level of regularization</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pars = {&quot;lr&quot;: 0.5, &quot;lambda_sparsity&quot;: 1e-5, &quot;lambda_TV&quot;: 1e-5, &quot;epochs&quot;: 600}
print(&quot;Cross validation score:&quot;, cross_validate(pars))
train_and_image(pars)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross validation score: 14.431548643548894
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;Figure size 480x480 with 1 Axes&gt;, &lt;AxesSubplot: &gt;)
</pre></div>
</div>
<img alt="../_images/5af3bfbd2a5e144ffd328b5ec16925ff885dc5801e678e5abd10cb344f48703a.png" src="../_images/5af3bfbd2a5e144ffd328b5ec16925ff885dc5801e678e5abd10cb344f48703a.png" />
</div>
</div>
<p>We see that the cross validation score improved significantly, meaning that these hyperparameter settings produce models that do a better job generalizing to new data. And, we can keep tweaking the hyperparameters to see how low of a cross-validation metric we can find.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pars = {&quot;lr&quot;: 0.5, &quot;lambda_sparsity&quot;: 1e-4, &quot;lambda_TV&quot;: 1e-4, &quot;epochs&quot;: 600}
print(&quot;Cross validation score:&quot;, cross_validate(pars))
train_and_image(pars)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross validation score: 7.660895815561863
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;Figure size 480x480 with 1 Axes&gt;, &lt;AxesSubplot: &gt;)
</pre></div>
</div>
<img alt="../_images/6f4ed4bea1386ac9fff04ee76056d04e74cb51d1c48df03d1212e652b9eef86c.png" src="../_images/6f4ed4bea1386ac9fff04ee76056d04e74cb51d1c48df03d1212e652b9eef86c.png" />
</div>
</div>
<p>More regularizing strength doesn’t always mean better… there will reach a point where the regularizing terms are strong that the model starts ignoring the data (via the <code class="docutils literal notranslate"><span class="pre">nll_gridded</span></code> term). To help you perform a full hyperparameter sweep and identify the “best” settings quickly, we recommend checking out tools like <a class="reference external" href="https://pytorch.org/docs/stable/tensorboard.html">Tensorboard</a> and <a class="reference external" href="https://docs.ray.io/en/master/tune/index.html">Ray Tune</a>.</p>
<p>For the purposes of comparison, here is the image produced by the tclean algorithm using CASA. The full commands are in the <a class="reference external" href="https://github.com/MPoL-dev/mpoldatasets/blob/main/products/ALMA-logo/tclean-iter.py">mpoldatasets</a> package.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fname = download_file(
    &quot;https://zenodo.org/record/4930016/files/logo_cube.tclean.fits&quot;,
    cache=True,
    show_progress=True,
    pkgname=&quot;mpol&quot;,
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hdul = fits.open(fname)
header = hdul[0].header
data = 1e3 * hdul[0].data[4]  # mJy/pixel
# get the coordinate labels
nx = header[&quot;NAXIS1&quot;]
ny = header[&quot;NAXIS2&quot;]
# RA coordinates
CDELT1 = 3600 * header[&quot;CDELT1&quot;]  # arcsec (converted from decimal deg)
# DEC coordinates
CDELT2 = 3600 * header[&quot;CDELT2&quot;]  # arcsec
RA = (np.arange(nx) - nx / 2) * CDELT1  # [arcsec]
DEC = (np.arange(ny) - ny / 2) * CDELT2  # [arcsec]
# extent needs to include extra half-pixels.
# RA, DEC are pixel centers
ext = (
    RA[0] - CDELT1 / 2,
    RA[-1] + CDELT1 / 2,
    DEC[0] - CDELT2 / 2,
    DEC[-1] + CDELT2 / 2,
)  # [arcsec]
norm = matplotlib.colors.Normalize(vmin=0, vmax=np.max(data))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(nrows=1, figsize=(4.5, 3.5))
fig.subplots_adjust(left=0.2, bottom=0.2)
im = ax.imshow(data, extent=ext, origin=&quot;lower&quot;, animated=True, norm=norm)
r = 2.4
ax.set_xlim(r, -r)
ax.set_ylim(-r, r)
ax.set_xlabel(r&quot;$\Delta \alpha \cos \delta$ [${}^{\prime\prime}$]&quot;)
ax.set_ylabel(r&quot;$\Delta \delta$ [${}^{\prime\prime}$]&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;$\\Delta \\delta$ [${}^{\\prime\\prime}$]&#39;)
</pre></div>
</div>
<img alt="../_images/18e18de8a94fc53dcb8a53d7bfab7a983f4a5d2813f350efbba5165157ac6155.png" src="../_images/18e18de8a94fc53dcb8a53d7bfab7a983f4a5d2813f350efbba5165157ac6155.png" />
</div>
</div>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gpu_setup.html" class="btn btn-neutral float-right" title="GPU Acceleration" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="optimization.html" class="btn btn-neutral float-left" title="Optimization Loop" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019-21, Ian Czekala

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>


      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-5472810-8', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>