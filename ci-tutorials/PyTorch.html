

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction to PyTorch: Tensors and Gradient Descent &mdash; MPoL 0.1.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://buttons.github.io/buttons.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
  <link rel="stylesheet" href="../_static/bullets.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/faculty.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans|Roboto:400,700|Roboto+Mono:400,700&display=swap" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gridding and diagnostic images" href="gridder.html" />
    <link rel="prev" title="API" href="../api.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

  
    <a class="heading heading-extra-margin" href="../index.html">
      <div class="logo-box logo-box-large">
        <img class="logo" src="../_static/logo.png"/>
      </div>
      
        <span class="icon icon-home"> MPoL</span>
      
    </a>
  

  
    
    
      <div class="version">0.1.1</div>
    
  

  
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rml_intro.html">Introduction to Regularized Maximum Likelihood Imaging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">MPoL Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../units-and-conventions.html">Units and Conventions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer-documentation.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction to PyTorch: Tensors and Gradient Descent</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction-to-tensors">Introduction to Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#calculating-gradients">Calculating Gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizing-a-function-with-gradient-descent">Optimizing a Function with Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gridder.html">Gridding and diagnostic images</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">Optimization Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="crossvalidation.html">Cross validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu_setup.html">GPU Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializedirtyimage.html">Initializing with the Dirty Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../large-tutorials/HD143006_part_1.html">HD143006 Tutorial Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../large-tutorials/HD143006_part_2.html">HD143006 Tutorial Part 2</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MPoL</a>
        
      </nav>


      <div class="wy-nav-content">

  

  
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
  <li class="breadcrumb"><a href="../index.html">MPoL</a> &raquo;</li>
    
  <li class="breadcrumb">Introduction to PyTorch: Tensors and Gradient Descent</li>

    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/ci-tutorials/PyTorch.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%matplotlib inline
%run notebook_setup
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/runner/work/MPoL/MPoL/docs/ci-tutorials/notebook_setup.py:7: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).
  get_ipython().magic(&#39;config InlineBackend.figure_format = &quot;retina&quot;&#39;)
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="introduction-to-pytorch-tensors-and-gradient-descent">
<h1>Introduction to PyTorch: Tensors and Gradient Descent<a class="headerlink" href="#introduction-to-pytorch-tensors-and-gradient-descent" title="Permalink to this heading">¶</a></h1>
<p>This tutorial provides an introduction to PyTorch tensors, automatic differentiation, and optimization with gradient descent.</p>
<section id="introduction-to-tensors">
<h2>Introduction to Tensors<a class="headerlink" href="#introduction-to-tensors" title="Permalink to this heading">¶</a></h2>
<p>Tensors are matrices, similar to numpy arrays, with the added benefit that they can be used to calculate gradients (more on that later). MPoL is built on PyTorch, and uses a form of gradient descent optimization to find the “best” image given a dataset and choice of regularizers.</p>
<p>We’ll start this tutorial by importing the torch and numpy packages. Make sure you have <a class="reference external" href="https://pytorch.org/get-started/locally/">PyTorch installed</a> before proceeding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
import numpy as np
import torch
</pre></div>
</div>
</div>
</div>
<p>There are several <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">ways to initialize a tensor</a>. A common method to create a tensor is from a numpy array:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>an_array = np.array([[1, 2], [3, 4]])
a_tensor = torch.tensor(an_array)  # creates tensor of same size as an_array

print(a_tensor)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 2],
        [3, 4]])
</pre></div>
</div>
</div>
</div>
<p>Tensors are similar to numpy arrays—many of the same <a class="reference external" href="https://pytorch.org/docs/stable/torch.html">operations</a> that we would perform on numpy arrays can easily be performed on PyTorch tensors. For example, we can compare how to calculate a matrix product using numpy and PyTorch</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>another_array = np.array([[5, 6, 7], [8, 9, 0]])  # create 2x3 array
another_tensor = torch.tensor(
    another_array
)  # create another tensor of same size as above array

# numpy array multiplication
prod_array = np.matmul(an_array, another_array)

# torch tensor multiplication
prod_tensor = torch.matmul(a_tensor, another_tensor)

print(f&quot;Numpy array multiplication result: {prod_array}&quot;)
print(f&quot;Torch tensor multiplication result: {prod_tensor}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Numpy array multiplication result: [[21 24  7]
 [47 54 21]]
Torch tensor multiplication result: tensor([[21, 24,  7],
        [47, 54, 21]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="calculating-gradients">
<h2>Calculating Gradients<a class="headerlink" href="#calculating-gradients" title="Permalink to this heading">¶</a></h2>
<p>PyTorch provides a key functionality—the ability to calculate the gradients on tensors. Let’s start by creating a tensor with a single value. Here we are setting <code class="docutils literal notranslate"><span class="pre">requires_grad</span> <span class="pre">=</span> <span class="pre">True</span></code>, we’ll see why this is important in a moment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = torch.tensor(3.0, requires_grad=True)
x
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(3., requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>Let’s define some variable <span class="math notranslate nohighlight">\(y\)</span> in terms of <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y = x ** 2
</pre></div>
</div>
</div>
</div>
<p>We see that the value of <span class="math notranslate nohighlight">\(y\)</span> is as we expect—nothing too strange here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(f&quot;x: {x}&quot;)
print(f&quot;y: {y}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x: 3.0
y: 9.0
</pre></div>
</div>
</div>
</div>
<p>But what if we wanted to calculate the gradient of <span class="math notranslate nohighlight">\(y\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>? Using calculus, we find that the answer is <span class="math notranslate nohighlight">\(\frac{dy}{dx} = 2x\)</span>. The derivative evaluated at <span class="math notranslate nohighlight">\(x = 3\)</span> is <span class="math notranslate nohighlight">\(6\)</span>.</p>
<p>The magic is that can use PyTorch to get the same answer—no analytic derivative needed!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y.backward()  # populates gradient (.grad) attributes of y with respect to all of its independent variables
x.grad  # returns the grad attribute (the gradient) of y with respect to x
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(6.)
</pre></div>
</div>
</div>
</div>
<p>PyTorch uses the concept of automatic differentiation to calculate the derivative. Instead of computing the derivative as we would by hand, the program is using a computational graph and mechanistic application of the chain rule. For example, a computational graph with several operations on <span class="math notranslate nohighlight">\(x\)</span> resulting in a final output <span class="math notranslate nohighlight">\(y\)</span> will use the chain rule to compute the differential associated with each operation and multiply these differentials together to get the derivative of <span class="math notranslate nohighlight">\(y\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>.</p>
</section>
<section id="optimizing-a-function-with-gradient-descent">
<h2>Optimizing a Function with Gradient Descent<a class="headerlink" href="#optimizing-a-function-with-gradient-descent" title="Permalink to this heading">¶</a></h2>
<p>If we were on the side of a hill in the dark and we wanted to get down to the bottom of a valley, how would we do it?</p>
<p>We wouldn’t be able to see all the way to the bottom of the valley, but we could feel which way is down based on the incline of where we are standing. We would take steps in the downward direction and we’d know when to stop when the ground felt flat.</p>
<p>Before we leap, though, we need to consider how large our steps should be. If we take very small steps, it will take us a longer time than if we take larger steps. However, if we take large leaps, we might completely miss the flat part of the valley, and jump straight across to the other side of the valley.</p>
<p>We can look at the gradient descent from a more mathematical lense by looking at the graph <span class="math notranslate nohighlight">\(y = x^2\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def y(x):
    return torch.square(x)
</pre></div>
</div>
</div>
</div>
<p>We will choose some arbitrary place to start on the left side of the hill and use PyTorch to calculate the tangent.</p>
<p>Note that Matplotlib requires numpy arrays instead of PyTorch tensors, so in the following code you might see the occasional <code class="docutils literal notranslate"><span class="pre">detach().numpy()</span></code> or <code class="docutils literal notranslate"><span class="pre">.item()</span></code> calls, which are used to convert PyTorch tensors to numpy arrays and scalar values, respectively. When it comes time to use MPoL for RML imaging, or any large production run, we’ll try to keep the calculations native to PyTorch tensors as long as possible, to avoid the overhead of converting types.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = torch.linspace(-5, 5, 100)
plt.plot(x, y(x))

x_start = torch.tensor(
    -4.0, requires_grad=True
)  # tensor with x coordinate of starting point
y_start = y(x_start)  # tensor with y coordinate of starting point

plt.scatter(x_start.item(), y_start.item())  # plot starting point

# we can calculate the derivative of y = x ** 2 evaluated at x_start
y_start.backward()  # populate x_start.grad
slope_start = x_start.grad

# and use this to evaluate the tangent line
tangent_line = slope_start * (x - x_start) + y_start

plt.plot(x, tangent_line.detach().numpy())
plt.xlabel(r&quot;$x$&quot;)
plt.ylabel(r&quot;$y$&quot;)
plt.xlim(xmin=-5, xmax=5)
plt.ylim(ymin=0, ymax=25)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/903b73ac1974c73e6bb75138595d44ebde563542c7c638c75139c3ac4c678dda.png" src="../_images/903b73ac1974c73e6bb75138595d44ebde563542c7c638c75139c3ac4c678dda.png" />
</div>
</div>
<p>We see we need to go to the right to go down toward the minimum. For a multivariate function, the gradient will point in the direction of the steepest downward slope. When we take steps, we find the x coordinate of our new location by this equation:</p>
<p><span class="math notranslate nohighlight">\(x_\mathrm{new} = x_\mathrm{current} - \nabla y(x_\mathrm{current}) * (\mathrm{step\,size})\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_\mathrm{current}\)</span> is our current x value</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla y(x_\mathrm{current})\)</span> is the gradient at our current point</p></li>
<li><p><span class="math notranslate nohighlight">\((\mathrm{step\,size})\)</span> is a value we choose that scales our steps</p></li>
</ul>
<p>We will choose <code class="docutils literal notranslate"><span class="pre">step_size</span> <span class="pre">=</span> <span class="pre">0.1</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = torch.linspace(-5, 5, 100)
plt.plot(x, y(x), zorder=0)  # plot y = x ** 2

step_size = 0.1

# Tensors containing current coordinates at the starting point we chose:
x_current = x_start
y_current = y(x_current)

# To keep track of our coordinates at each step, we will create 2 lists, initialized with the values at our chosen starting point
# These lists will be used to plot points with Matplotlib.pyplot so we use .item() to only retain the value in the tensor
x_coords = [x_current.item()]
y_coords = [y_current.item()]

# Slope at current point
y_current.backward()  # populate x_current.grad
slope_current = (
    x_current.grad
)  # tensor containing derivative of y = x ** 2 evaluated at current point

# Using equation for x_new to get x coordinate of second point, store it in a tensor
# We cannot use torch.tensor(...) to make a new tensor from previous tensors without altering the
# computational graph. We use .item() to only use float values to create our new tensor
x_new = torch.tensor(
    x_current.item() - (slope_current.item()) * step_size, requires_grad=True
)

# Plug in x_new into y = x ** 2 to get y_new of second point
y_new = y(x_new)


# Store second point coordinates in our lists
x_coords.append(x_new.item())
y_coords.append(y_new.item())


plt.scatter(x_coords, y_coords)  # plot points showing steps
# replot the last point in a new color
plt.scatter(x_coords[-1], y_coords[-1], c=&quot;C1&quot;, zorder=1)
plt.text(-2, 5, &quot;step 1&quot;, va=&quot;center&quot;)

plt.xlim(xmin=-5, xmax=5)
plt.ylim(ymin=-1, ymax=25)
plt.xlabel(r&quot;$x$&quot;)
plt.ylabel(r&quot;$y$&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/60082b91c572792ea9612bedb620596a03a17137c2a8adb6bc182484a21e8215.png" src="../_images/60082b91c572792ea9612bedb620596a03a17137c2a8adb6bc182484a21e8215.png" />
</div>
</div>
<p>The gradient at our new point (shown in orange) is still not close to zero, meaning we haven’t reached the minimum. We continue this process of checking if the gradient is nearly zero, and taking a step in the direction of steepest descent until we reach the bottom of the valley. We’ll say we’ve reached the bottom of the valley when the absolute value of the gradient is <span class="math notranslate nohighlight">\(&lt;0.1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x = torch.linspace(-5, 5, 100)
plt.plot(x, y(x), zorder=0)  # plot y = x ** 2

# We are now at our second point so we need to update our tensors containing our current coordinates
x_current = x_new
y_current = y_new


# We automate this process with the following while loop
y_current.backward()  # populate x_current.grad
while abs(x_current.grad) &gt;= 0.1:  # Check to see if we&#39;re at minimum
    # Get tensors containing new coordinates
    x_new = torch.tensor(
        x_current.item() - x_current.grad.item() * step_size, requires_grad=True
    )
    y_new = y(x_new)

    # Add new coordinates to lists
    x_coords.append(x_new.item())
    y_coords.append(y_new.item())

    # Update current position
    x_current = x_new
    y_current = y_new

    # Update current slope
    y_current.backward()  # populate x_current.grad


plt.scatter(x_coords, y_coords)  # plot points showing steps
plt.scatter(x_coords[-1], y_coords[-1], c=&quot;C1&quot;)  # highlight last point

plt.xlim(xmin=-5, xmax=5)
plt.ylim(ymin=-1, ymax=25)
plt.xlabel(r&quot;$x$&quot;)
plt.ylabel(r&quot;$y$&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7d94bcdae442cd202e2c4b90b550021ec902f20bb6b89eea1a40ba6c2cf10e95.png" src="../_images/7d94bcdae442cd202e2c4b90b550021ec902f20bb6b89eea1a40ba6c2cf10e95.png" />
</div>
</div>
<p>This works, but it takes a long time since we have several small steps.</p>
<p>Can we speed up the process by taking large steps? Most likely, yes. But there is a danger in taking step sizes that are too large. For example, let’s repeat this exercise with a step size of <span class="math notranslate nohighlight">\(1.5\)</span>. Our first step now looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>x_large_step = torch.linspace(-20, 20, 1000)
plt.plot(x_large_step, y(x_large_step), zorder=0)  # plot y = x ** 2

# Current values at starting point we chose:
x_large_step_current = torch.tensor(-4.0, requires_grad=True)
y_large_step_current = y(x_large_step_current)

# Slope at current point
y_large_step_current.backward()  # populate x_large_step_current.grad
slope_large_step_current = (
    x_large_step_current.grad
)  # tensor containing derivative of y = x ** 2 evaluated at current point

# To keep track of our coordinates at each step, we will create 2 lists, initialized with the coordinates at our chosen starting point
# These lists will be used to plot points with Matplotlib.pyplot so we use .item() to only retain the value in the tensor
x_large_coords = [x_large_step_current.item()]
y_large_coords = [y_large_step_current.item()]

# New step_size
large_step_size = 1.5

# Get coordinates of our second point using x_new equation and y = x ** 2
x_large_step_new = torch.tensor(
    x_large_step_current.item() - slope_large_step_current.item() * large_step_size,
    requires_grad=True,
)
y_large_step_new = y(x_large_step_new)

# Store second point coordinates in our lists
x_large_coords.append(x_large_step_new.item())
y_large_coords.append(y_large_step_new.item())


plt.scatter(x_large_coords, y_large_coords)  # plot points showing steps
plt.scatter(x_large_coords[-1], y_large_coords[-1], c=&quot;C1&quot;)


plt.xlim(xmin=-20, xmax=20)
plt.ylim(ymin=-1, ymax=260)
plt.xlabel(r&quot;$x$&quot;)
plt.ylabel(r&quot;$y$&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/136ca26b71e8e43458941b15d98d5b526f40a1a964e198ed823e471a221d405a.png" src="../_images/136ca26b71e8e43458941b15d98d5b526f40a1a964e198ed823e471a221d405a.png" />
</div>
</div>
<p><em>Note the change in scale.</em> With only one step, we already see that we stepped <em>right over</em> the minimum to somewhere far up the other side of the valley (orange point)! This is not good. If we kept iterating with the same learning rate, we’d find that the optimization process diverges and the step sizes start blowing up. This is why it is important to pick the proper step size by setting the learning rate appropriately. Steps that are too small take a long time while steps that are too large render the optimization process invalid. In this case, a reasonable choice appears to be <code class="docutils literal notranslate"><span class="pre">step</span> <span class="pre">size</span> <span class="pre">=</span> <span class="pre">0.6</span></code>, which would have reached pretty close to the minimum after only 3 steps.</p>
<p>To sum up, optimizing a function with gradient descent consists of</p>
<ol class="arabic simple">
<li><p>Calculate the gradient at your current point</p></li>
<li><p>Determine if the gradient is within the stopping criterion (in this case, the gradient is about equal to zero or <span class="math notranslate nohighlight">\(&lt;0.1\)</span>), if so stop</p></li>
<li><p>Otherwise, take a step in the direction of the gradient and go to #1</p></li>
</ol>
<p>Autodifferentiation frameworks like PyTorch allow us to easily calculate the gradient of complex functions, including a large set of prior/regularizer functions that we would want to use for Regularized Maximum Likelihood (RML) imaging. This makes it relatively easy to quickly and efficiently solve for the “optimal” image given a set of data and regularizer terms.</p>
</section>
<section id="additional-resources">
<h2>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/autograd.html">PyTorch documentation on autograd</a></p></li>
<li><p><a class="reference external" href="https://anguswilliams91.github.io/statistics/computing/jax/">Angus Williams’ blog post on autodifferentiaton, JAX, and Laplace’s method</a></p></li>
<li><p><a class="reference external" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">Paperspace blog post on understanding graphs and automatic differentiation</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/IHZwWFHWa-w">3Blue1Brown video on gradient descent</a></p></li>
</ul>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gridder.html" class="btn btn-neutral float-right" title="Gridding and diagnostic images" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../api.html" class="btn btn-neutral float-left" title="API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019-21, Ian Czekala

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>


      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-5472810-8', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>