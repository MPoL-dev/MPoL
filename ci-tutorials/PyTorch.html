

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction to PyTorch: Tensors and Gradient Descent &mdash; MPoL 0.1.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
        <script src="https://buttons.github.io/buttons.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/bullets.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/faculty.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans|Roboto:400,700|Roboto+Mono:400,700&display=swap" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gridding and diagnostic images" href="gridder.html" />
    <link rel="prev" title="API" href="../api.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

  
    <a class="heading heading-extra-margin" href="../index.html">
      <div class="logo-box logo-box-large">
        <img class="logo" src="../_static/logo.png"/>
      </div>
      
        <span class="icon icon-home"> MPoL</span>
      
    </a>
  

  
    
    
      <div class="version">0.1.1</div>
    
  

  
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rml_intro.html">Introduction to Regularized Maximum Likelihood Imaging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">MPoL Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../units-and-conventions.html">Units and Conventions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer-documentation.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction to PyTorch: Tensors and Gradient Descent</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Introduction-to-Tensors">Introduction to Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Calculating-Gradients">Calculating Gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Optimizing-a-Function-with-Gradient-Descent">Optimizing a Function with Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Additional-Resources">Additional Resources</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gridder.html">Gridding and diagnostic images</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">Optimization Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="crossvalidation.html">Cross validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu_setup.html">GPU Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializedirtyimage.html">Initializing with the Dirty Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../large-tutorials/HD143006_part_1.html">HD143006 Tutorial Part 1</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MPoL</a>
        
      </nav>


      <div class="wy-nav-content">

  

  
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
  <li class="breadcrumb"><a href="../index.html">MPoL</a> &raquo;</li>
    
  <li class="breadcrumb">Introduction to PyTorch: Tensors and Gradient Descent</li>

    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/ci-tutorials/PyTorch.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<section id="Introduction-to-PyTorch:-Tensors-and-Gradient-Descent">
<h1>Introduction to PyTorch: Tensors and Gradient Descent<a class="headerlink" href="#Introduction-to-PyTorch:-Tensors-and-Gradient-Descent" title="Permalink to this headline">¶</a></h1>
<p>This tutorial provides an introduction to PyTorch tensors, automatic differentiation, and optimization with gradient descent.</p>
<section id="Introduction-to-Tensors">
<h2>Introduction to Tensors<a class="headerlink" href="#Introduction-to-Tensors" title="Permalink to this headline">¶</a></h2>
<p>Tensors are matrices, similar to numpy arrays, with the added benefit that they can be used to calculate gradients (more on that later). MPoL is built on PyTorch, and uses a form of gradient descent optimization to find the “best” image given a dataset and choice of regularizers.</p>
<p>We’ll start this tutorial by importing the torch and numpy packages. Make sure you have <a class="reference external" href="https://pytorch.org/get-started/locally/">PyTorch installed</a> before proceeding.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
<p>There are several <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">ways to initialize a tensor</a>. A common method to create a tensor is from a numpy array:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">an_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">a_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">an_array</span><span class="p">)</span>  <span class="c1"># creates tensor of same size as an_array</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[1, 2],
        [3, 4]])
</pre></div></div>
</div>
<p>Tensors are similar to numpy arrays—many of the same <a class="reference external" href="https://pytorch.org/docs/stable/torch.html">operations</a> that we would perform on numpy arrays can easily be performed on PyTorch tensors. For example, we can compare how to calculate a matrix product using numpy and PyTorch</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">another_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># create 2x3 array</span>
<span class="n">another_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="n">another_array</span>
<span class="p">)</span>  <span class="c1"># create another tensor of same size as above array</span>

<span class="c1"># numpy array multiplication</span>
<span class="n">prod_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">an_array</span><span class="p">,</span> <span class="n">another_array</span><span class="p">)</span>

<span class="c1"># torch tensor multiplication</span>
<span class="n">prod_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_tensor</span><span class="p">,</span> <span class="n">another_tensor</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Numpy array multiplication result: </span><span class="si">{</span><span class="n">prod_array</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Torch tensor multiplication result: </span><span class="si">{</span><span class="n">prod_tensor</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Numpy array multiplication result: [[21 24  7]
 [47 54 21]]
Torch tensor multiplication result: tensor([[21, 24,  7],
        [47, 54, 21]])
</pre></div></div>
</div>
</section>
<section id="Calculating-Gradients">
<h2>Calculating Gradients<a class="headerlink" href="#Calculating-Gradients" title="Permalink to this headline">¶</a></h2>
<p>PyTorch provides a key functionality—the ability to calculate the gradients on tensors. Let’s start by creating a tensor with a single value. Here we are setting <code class="docutils literal notranslate"><span class="pre">requires_grad</span> <span class="pre">=</span> <span class="pre">True</span></code>, we’ll see why this is important in a moment.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor(3., requires_grad=True)
</pre></div></div>
</div>
<p>Let’s define some variable <span class="math notranslate nohighlight">\(y\)</span> in terms of <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
<p>We see that the value of <span class="math notranslate nohighlight">\(y\)</span> is as we expect—nothing too strange here.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x: 3.0
y: 9.0
</pre></div></div>
</div>
<p>But what if we wanted to calculate the gradient of <span class="math notranslate nohighlight">\(y\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>? Using calculus, we find that the answer is <span class="math notranslate nohighlight">\(\frac{dy}{dx} = 2x\)</span>. The derivative evaluated at <span class="math notranslate nohighlight">\(x = 3\)</span> is <span class="math notranslate nohighlight">\(6\)</span>.</p>
<p>The magic is that can use PyTorch to get the same answer—no analytic derivative needed!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populates gradient (.grad) attributes of y with respect to all of its independent variables</span>
<span class="n">x</span><span class="o">.</span><span class="n">grad</span>  <span class="c1"># returns the grad attribute (the gradient) of y with respect to x</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor(6.)
</pre></div></div>
</div>
<p>PyTorch uses the concept of automatic differentiation to calculate the derivative. Instead of computing the derivative as we would by hand, the program is using a computational graph and mechanistic application of the chain rule. For example, a computational graph with several operations on <span class="math notranslate nohighlight">\(x\)</span> resulting in a final output <span class="math notranslate nohighlight">\(y\)</span> will use the chain rule to compute the differential associated with each operation and multiply these differentials together to get the derivative of <span class="math notranslate nohighlight">\(y\)</span>
with respect to <span class="math notranslate nohighlight">\(x\)</span>.</p>
</section>
<section id="Optimizing-a-Function-with-Gradient-Descent">
<h2>Optimizing a Function with Gradient Descent<a class="headerlink" href="#Optimizing-a-Function-with-Gradient-Descent" title="Permalink to this headline">¶</a></h2>
<p>If we were on the side of a hill in the dark and we wanted to get down to the bottom of a valley, how would we do it?</p>
<p>We wouldn’t be able to see all the way to the bottom of the valley, but we could feel which way is down based on the incline of where we are standing. We would take steps in the downward direction and we’d know when to stop when the ground felt flat.</p>
<p>Before we leap, though, we need to consider how large our steps should be. If we take very small steps, it will take us a longer time than if we take larger steps. However, if we take large leaps, we might completely miss the flat part of the valley, and jump straight across to the other side of the valley.</p>
<p>We can look at the gradient descent from a more mathematical lense by looking at the graph <span class="math notranslate nohighlight">\(y = x^2\)</span>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We will choose some arbitrary place to start on the left side of the hill and use PyTorch to calculate the tangent.</p>
<p>Note that Matplotlib requires numpy arrays instead of PyTorch tensors, so in the following code you might see the occasional <code class="docutils literal notranslate"><span class="pre">detach().numpy()</span></code> or <code class="docutils literal notranslate"><span class="pre">.item()</span></code> calls, which are used to convert PyTorch tensors to numpy arrays and scalar values, respectively. When it comes time to use MPoL for RML imaging, or any large production run, we’ll try to keep the calculations native to PyTorch tensors as long as possible, to avoid the overhead of converting types.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">x_start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>  <span class="c1"># tensor with x coordinate of starting point</span>
<span class="n">y_start</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>  <span class="c1"># tensor with y coordinate of starting point</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_start</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">y_start</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># plot starting point</span>

<span class="c1"># we can calculate the derivative of y = x ** 2 evaluated at x_start</span>
<span class="n">y_start</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_start.grad</span>
<span class="n">slope_start</span> <span class="o">=</span> <span class="n">x_start</span><span class="o">.</span><span class="n">grad</span>

<span class="c1"># and use this to evaluate the tangent line</span>
<span class="n">tangent_line</span> <span class="o">=</span> <span class="n">slope_start</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_start</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_start</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tangent_line</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/ci-tutorials_PyTorch_22_0.png" class="no-scaled-link" src="../_images/ci-tutorials_PyTorch_22_0.png" style="width: 459px; height: 459px;" />
</div>
</div>
<p>We see we need to go to the right to go down toward the minimum. For a multivariate function, the gradient will point in the direction of the steepest downward slope. When we take steps, we find the x coordinate of our new location by this equation:</p>
<p><span class="math notranslate nohighlight">\(x_{new} = x_{current} - \nabla y(x_{current}) * (step \: size)\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_{current}\)</span> is our current x value</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla y(x_{current})\)</span> is the gradient at our current point</p></li>
<li><p><span class="math notranslate nohighlight">\((step \: size)\)</span> is a value we choose that scales our steps</p></li>
</ul>
<p>We will choose <code class="docutils literal notranslate"><span class="pre">step_size</span> <span class="pre">=</span> <span class="pre">0.1</span></code>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># plot y = x ** 2</span>

<span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Tensors containing current coordinates at the starting point we chose:</span>
<span class="n">x_current</span> <span class="o">=</span> <span class="n">x_start</span>
<span class="n">y_current</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_current</span><span class="p">)</span>

<span class="c1"># To keep track of our coordinates at each step, we will create 2 lists, initialized with the values at our chosen starting point</span>
<span class="c1"># These lists will be used to plot points with Matplotlib.pyplot so we use .item() to only retain the value in the tensor</span>
<span class="n">x_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="n">y_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">y_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>

<span class="c1"># Slope at current point</span>
<span class="n">y_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_current.grad</span>
<span class="n">slope_current</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x_current</span><span class="o">.</span><span class="n">grad</span>
<span class="p">)</span>  <span class="c1"># tensor containing derivative of y = x ** 2 evaluated at current point</span>

<span class="c1"># Using equation for x_new to get x coordinate of second point, store it in a tensor</span>
<span class="c1"># We cannot use torch.tensor(...) to make a new tensor from previous tensors without altering the</span>
<span class="c1"># computational graph. We use .item() to only use float values to create our new tensor</span>
<span class="n">x_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="n">x_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="n">slope_current</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Plug in x_new into y = x ** 2 to get y_new of second point</span>
<span class="n">y_new</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>


<span class="c1"># Store second point coordinates in our lists</span>
<span class="n">x_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">y_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>


<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">,</span> <span class="n">y_coords</span><span class="p">)</span>  <span class="c1"># plot points showing steps</span>
<span class="c1"># replot the last point in a new color</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;step 1&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/ci-tutorials_PyTorch_24_0.png" class="no-scaled-link" src="../_images/ci-tutorials_PyTorch_24_0.png" style="width: 459px; height: 459px;" />
</div>
</div>
<p>The gradient at our new point (shown in orange) is still not close to zero, meaning we haven’t reached the minimum. We continue this process of checking if the gradient is nearly zero, and taking a step in the direction of steepest descent until we reach the bottom of the valley. We’ll say we’ve reached the bottom of the valley when the absolute value of the gradient is <span class="math notranslate nohighlight">\(&lt;0.1\)</span>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># plot y = x ** 2</span>

<span class="c1"># We are now at our second point so we need to update our tensors containing our current coordinates</span>
<span class="n">x_current</span> <span class="o">=</span> <span class="n">x_new</span>
<span class="n">y_current</span> <span class="o">=</span> <span class="n">y_new</span>


<span class="c1"># We automate this process with the following while loop</span>
<span class="n">y_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_current.grad</span>
<span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x_current</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.1</span><span class="p">:</span>  <span class="c1"># Check to see if we&#39;re at minimum</span>
    <span class="c1"># Get tensors containing new coordinates</span>
    <span class="n">x_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">x_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">x_current</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">y_new</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>

    <span class="c1"># Add new coordinates to lists</span>
    <span class="n">x_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">y_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># Update current position</span>
    <span class="n">x_current</span> <span class="o">=</span> <span class="n">x_new</span>
    <span class="n">y_current</span> <span class="o">=</span> <span class="n">y_new</span>

    <span class="c1"># Update current slope</span>
    <span class="n">y_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_current.grad</span>


<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">,</span> <span class="n">y_coords</span><span class="p">)</span>  <span class="c1"># plot points showing steps</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>  <span class="c1"># highlight last point</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/ci-tutorials_PyTorch_26_0.png" class="no-scaled-link" src="../_images/ci-tutorials_PyTorch_26_0.png" style="width: 459px; height: 459px;" />
</div>
</div>
<p>This works, but it takes a long time since we have several small steps.</p>
<p>Can we speed up the process by taking large steps? Most likely, yes. But there is a danger in taking step sizes that are too large. For example, let’s repeat this exercise with a step size of <span class="math notranslate nohighlight">\(1.5\)</span>. Our first step now looks like:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x_large_step</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_large_step</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x_large_step</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># plot y = x ** 2</span>

<span class="c1"># Current values at starting point we chose:</span>
<span class="n">x_large_step_current</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_large_step_current</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_large_step_current</span><span class="p">)</span>

<span class="c1"># Slope at current point</span>
<span class="n">y_large_step_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_large_step_current.grad</span>
<span class="n">slope_large_step_current</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x_large_step_current</span><span class="o">.</span><span class="n">grad</span>
<span class="p">)</span>  <span class="c1"># tensor containing derivative of y = x ** 2 evaluated at current point</span>

<span class="c1"># To keep track of our coordinates at each step, we will create 2 lists, initialized with the coordinates at our chosen starting point</span>
<span class="c1"># These lists will be used to plot points with Matplotlib.pyplot so we use .item() to only retain the value in the tensor</span>
<span class="n">x_large_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="n">y_large_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">y_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>

<span class="c1"># New step_size</span>
<span class="n">large_step_size</span> <span class="o">=</span> <span class="mf">1.5</span>

<span class="c1"># Get coordinates of our second point using x_new equation and y = x ** 2</span>
<span class="n">x_large_step_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="n">x_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">slope_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">large_step_size</span><span class="p">,</span>
    <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">y_large_step_new</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_large_step_new</span><span class="p">)</span>

<span class="c1"># Store second point coordinates in our lists</span>
<span class="n">x_large_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_large_step_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">y_large_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_large_step_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>


<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_large_coords</span><span class="p">,</span> <span class="n">y_large_coords</span><span class="p">)</span>  <span class="c1"># plot points showing steps</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_large_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_large_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">20</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">260</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/ci-tutorials_PyTorch_28_0.png" class="no-scaled-link" src="../_images/ci-tutorials_PyTorch_28_0.png" style="width: 485px; height: 451px;" />
</div>
</div>
<p><em>Note the change in scale.</em> With only one step, we already see that we stepped <em>right over</em> the minimum to somewhere far up the other side of the valley (orange point)! This is not good. If we kept iterating with the same learning rate, we’d find that the optimization process diverges and the step sizes start blowing up. This is why it is important to pick the proper step size by setting the learning rate appropriately. Steps that are too small take a long time while steps that are too large
render the optimization process invalid. In this case, a reasonable choice appears to be <code class="docutils literal notranslate"><span class="pre">step</span> <span class="pre">size</span> <span class="pre">=</span> <span class="pre">0.6</span></code>, which would have reached pretty close to the minimum after only 3 steps.</p>
<p>To sum up, optimizing a function with gradient descent consists of</p>
<ol class="arabic simple">
<li><p>Calculate the gradient at your current point</p></li>
<li><p>Determine if the gradient is within the stopping criterion (in this case, the gradient is about equal to zero or <span class="math notranslate nohighlight">\(&lt;0.1\)</span>), if so stop</p></li>
<li><p>Otherwise, take a step in the direction of the gradient and go to #1</p></li>
</ol>
<p>Autodifferentiation frameworks like PyTorch allow us to easily calculate the gradient of complex functions, including a large set of prior/regularizer functions that we would want to use for Regularized Maximum Likelihood (RML) imaging. This makes it relatively easy to quickly and efficiently solve for the “optimal” image given a set of data and regularizer terms.</p>
</section>
<section id="Additional-Resources">
<h2>Additional Resources<a class="headerlink" href="#Additional-Resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/autograd.html">PyTorch documentation on autograd</a></p></li>
<li><p><a class="reference external" href="https://anguswilliams91.github.io/statistics/computing/jax/">Angus Williams’ blog post on autodifferentiaton, JAX, and Laplace’s method</a></p></li>
<li><p><a class="reference external" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">Paperspace blog post on understanding graphs and automatic differentiation</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/IHZwWFHWa-w">3Blue1Brown video on gradient descent</a></p></li>
</ul>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gridder.html" class="btn btn-neutral float-right" title="Gridding and diagnostic images" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../api.html" class="btn btn-neutral float-left" title="API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019-21, Ian Czekala

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>


      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-5472810-8', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>