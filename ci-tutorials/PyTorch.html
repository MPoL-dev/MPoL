
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Introduction to PyTorch: Tensors and Gradient Descent &#8212; MPoL 0.1.13 documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="https://buttons.github.io/buttons.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gridding and diagnostic images" href="gridder.html" />
    <link rel="prev" title="API" href="../api.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">MPoL 0.1.13 documentation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  User Guide
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rml_intro.html">
   Introduction to Regularized Maximum Likelihood Imaging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../installation.html">
   MPoL Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../units-and-conventions.html">
   Units and Conventions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../developer-documentation.html">
   Developer Documentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../api.html">
   API
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction to PyTorch: Tensors and Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gridder.html">
   Gridding and diagnostic images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optimization.html">
   Intro to RML with MPoL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="loose-visibilities.html">
   Likelihood functions and model visibilities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="crossvalidation.html">
   Cross validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gpu_setup.html">
   GPU Acceleration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="initializedirtyimage.html">
   Initializing with the Dirty Image
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../large-tutorials/HD143006_part_1.html">
   HD143006 Tutorial Part 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../large-tutorials/HD143006_part_2.html">
   HD143006 Tutorial Part 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fakedata.html">
   Making a Mock Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../large-tutorials/pyro.html">
   Parametric Inference with Pyro
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../changelog.html">
   Changelog
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/MPoL-dev/MPoL"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/ci-tutorials/PyTorch.md.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-tensors">
   Introduction to Tensors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#calculating-gradients">
   Calculating Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-a-function-with-gradient-descent">
   Optimizing a Function with Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-resources">
   Additional Resources
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction to PyTorch: Tensors and Gradient Descent</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-tensors">
   Introduction to Tensors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#calculating-gradients">
   Calculating Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizing-a-function-with-gradient-descent">
   Optimizing a Function with Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-resources">
   Additional Resources
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">run</span> notebook_setup
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="introduction-to-pytorch-tensors-and-gradient-descent">
<h1>Introduction to PyTorch: Tensors and Gradient Descent<a class="headerlink" href="#introduction-to-pytorch-tensors-and-gradient-descent" title="Permalink to this headline">#</a></h1>
<p>This tutorial provides a gentle introduction to PyTorch tensors, automatic differentiation, and optimization with gradient descent outside of any specifics about radio interferometry or the MPoL package itself.</p>
<section id="introduction-to-tensors">
<h2>Introduction to Tensors<a class="headerlink" href="#introduction-to-tensors" title="Permalink to this headline">#</a></h2>
<p>Tensors are multi-dimensional arrays, similar to numpy arrays, with the added benefit that they can be used to calculate gradients (more on that later). MPoL is built on the <a class="reference external" href="https://pytorch.org/">PyTorch</a> machine learning library, and uses a form of gradient descent optimization to find the “best” image given some dataset and loss function, which may include regularizers.</p>
<p>We’ll start this tutorial by importing the torch and numpy packages. Make sure you have <a class="reference external" href="https://pytorch.org/get-started/locally/">PyTorch installed</a> before proceeding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<p>There are several <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">ways to initialize a tensor</a>. A common method to create a tensor is from a numpy array:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">an_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">a_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">an_array</span><span class="p">)</span>  <span class="c1"># creates tensor of same size as an_array</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 2],
        [3, 4]])
</pre></div>
</div>
</div>
</div>
<p>Tensors are similar to numpy arrays—many of the same <a class="reference external" href="https://pytorch.org/docs/stable/torch.html">operations</a> that we would perform on numpy arrays can easily be performed on PyTorch tensors. For example, we can compare how to calculate a matrix product using numpy and PyTorch</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">another_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># create 2x3 array</span>
<span class="n">another_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="n">another_array</span>
<span class="p">)</span>  <span class="c1"># create another tensor of same size as above array</span>

<span class="c1"># numpy array multiplication</span>
<span class="n">prod_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">an_array</span><span class="p">,</span> <span class="n">another_array</span><span class="p">)</span>

<span class="c1"># torch tensor multiplication</span>
<span class="n">prod_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_tensor</span><span class="p">,</span> <span class="n">another_tensor</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Numpy array multiplication result: </span><span class="si">{</span><span class="n">prod_array</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Torch tensor multiplication result: </span><span class="si">{</span><span class="n">prod_tensor</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Numpy array multiplication result: [[21 24  7]
 [47 54 21]]
Torch tensor multiplication result: tensor([[21, 24,  7],
        [47, 54, 21]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="calculating-gradients">
<h2>Calculating Gradients<a class="headerlink" href="#calculating-gradients" title="Permalink to this headline">#</a></h2>
<p>PyTorch allows us to calculate the gradients on tensors, which is a key functionality underlying MPoL. Let’s start by creating a tensor with a single value. Here we are setting <code class="docutils literal notranslate"><span class="pre">requires_grad</span> <span class="pre">=</span> <span class="pre">True</span></code>; we’ll see why this is important in a moment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(3., requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>Let’s define some variable <span class="math notranslate nohighlight">\(y\)</span> in terms of <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<p>We see that the value of <span class="math notranslate nohighlight">\(y\)</span> is as we expect—nothing too strange here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x: 3.0
y: 9.0
</pre></div>
</div>
</div>
</div>
<p>But what if we wanted to calculate the gradient of <span class="math notranslate nohighlight">\(y\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>? Using calculus, we find that the answer is <span class="math notranslate nohighlight">\(\frac{dy}{dx} = 2x\)</span>. The derivative evaluated at <span class="math notranslate nohighlight">\(x = 3\)</span> is <span class="math notranslate nohighlight">\(6\)</span>.</p>
<p>We can use PyTorch to get the same answer—no analytic derivative needed!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populates gradient (.grad) attributes of y with respect to all of its independent variables</span>
<span class="n">x</span><span class="o">.</span><span class="n">grad</span>  <span class="c1"># returns the grad attribute (the gradient) of y with respect to x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(6.)
</pre></div>
</div>
</div>
</div>
<p>PyTorch uses the concept of <a class="reference external" href="https://arxiv.org/abs/1502.05767">automatic differentiation</a> to calculate the derivative. Instead of computing the derivative as we would by hand, the program uses a computational graph and the mechanistic application of the chain rule. For example, a computational graph with several operations on <span class="math notranslate nohighlight">\(x\)</span> resulting in a final output <span class="math notranslate nohighlight">\(y\)</span> will use the chain rule to compute the differential associated with each operation and multiply these differentials together to get the derivative of <span class="math notranslate nohighlight">\(y\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>.</p>
</section>
<section id="optimizing-a-function-with-gradient-descent">
<h2>Optimizing a Function with Gradient Descent<a class="headerlink" href="#optimizing-a-function-with-gradient-descent" title="Permalink to this headline">#</a></h2>
<p>If we were on the side of a hill in the dark and we wanted to get down to the bottom of a valley, how might we do it?</p>
<p>We can’t see all the way to the bottom of the valley, but we can feel which way is down based on the incline of where we are standing. We might take steps in the downward direction and we’d know when to stop when the ground finally felt flat. We would also need to consider how large our steps should be. If we take very small steps, it will take us a longer time than if we take larger steps. However, if we take large leaps, we might completely miss the flat part of the valley, and jump straight across to the other side of the valley.</p>
<p>Now let’s take a more quantitative look at the gradient descent using the function <span class="math notranslate nohighlight">\(y = x^2\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will choose some arbitrary place to start on the left side of the hill and use PyTorch to calculate the tangent.</p>
<p>Note that the plotting library Matplotlib requires numpy arrays instead of PyTorch tensors, so in the following code you might see the occasional <code class="docutils literal notranslate"><span class="pre">detach().numpy()</span></code> or <code class="docutils literal notranslate"><span class="pre">.item()</span></code> calls, which are used to convert PyTorch tensors to numpy arrays and scalar values, respectively, for plotting. When it comes time to use MPoL for RML imaging, or any large production run, we’ll try to keep the calculations native to PyTorch tensors as long as possible, to avoid the overhead of converting types.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">x_start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>  <span class="c1"># tensor with x coordinate of starting point</span>
<span class="n">y_start</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>  <span class="c1"># tensor with y coordinate of starting point</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_start</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">y_start</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># plot starting point</span>

<span class="c1"># we can calculate the derivative of y = x ** 2 evaluated at x_start</span>
<span class="n">y_start</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_start.grad</span>
<span class="n">slope_start</span> <span class="o">=</span> <span class="n">x_start</span><span class="o">.</span><span class="n">grad</span>

<span class="c1"># and use this to evaluate the tangent line</span>
<span class="n">tangent_line</span> <span class="o">=</span> <span class="n">slope_start</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_start</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_start</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tangent_line</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/aa4014aa819c4fd1240e4a412fa9c5f975b660c6935e84d2412d2cf2876047a6.png" src="../_images/aa4014aa819c4fd1240e4a412fa9c5f975b660c6935e84d2412d2cf2876047a6.png" />
</div>
</div>
<p>We see we need to go to the right to go down toward the minimum. For a multivariate function, the gradient will be a vector pointing in the direction of the steepest downward slope. When we take steps, we find the x coordinate of our new location by:</p>
<p><span class="math notranslate nohighlight">\(x_\mathrm{new} = x_\mathrm{current} - \nabla y(x_\mathrm{current}) * (\mathrm{step\,size})\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_\mathrm{current}\)</span> is our current x value</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla y(x_\mathrm{current})\)</span> is the gradient at our current point</p></li>
<li><p><span class="math notranslate nohighlight">\((\mathrm{step\,size})\)</span> is a value we choose that scales our steps</p></li>
</ul>
<p>We will choose <code class="docutils literal notranslate"><span class="pre">step_size</span> <span class="pre">=</span> <span class="pre">0.1</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># plot y = x ** 2</span>

<span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Tensors containing current coordinates at the starting point we chose:</span>
<span class="n">x_current</span> <span class="o">=</span> <span class="n">x_start</span>
<span class="n">y_current</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_current</span><span class="p">)</span>

<span class="c1"># To keep track of our coordinates at each step, we will create 2 lists, initialized with the values at our chosen starting point</span>
<span class="c1"># These lists will be used to plot points with Matplotlib.pyplot so we use .item() to only retain the value in the tensor</span>
<span class="n">x_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="n">y_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">y_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>

<span class="c1"># Slope at current point</span>
<span class="n">y_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_current.grad</span>
<span class="n">slope_current</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x_current</span><span class="o">.</span><span class="n">grad</span>
<span class="p">)</span>  <span class="c1"># tensor containing derivative of y = x ** 2 evaluated at current point</span>

<span class="c1"># Using equation for x_new to get x coordinate of second point, store it in a tensor</span>
<span class="c1"># We cannot use torch.tensor(...) to make a new tensor from previous tensors without altering the</span>
<span class="c1"># computational graph. We use .item() to only use float values to create our new tensor</span>
<span class="n">x_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="n">x_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="n">slope_current</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Plug in x_new into y = x ** 2 to get y_new of second point</span>
<span class="n">y_new</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>


<span class="c1"># Store second point coordinates in our lists</span>
<span class="n">x_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">y_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>


<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">,</span> <span class="n">y_coords</span><span class="p">)</span>  <span class="c1"># plot points showing steps</span>
<span class="c1"># replot the last point in a new color</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;step 1&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ade4bc80dd522959106b566d6fde94298c7ac454f018589c5d93860443d26859.png" src="../_images/ade4bc80dd522959106b566d6fde94298c7ac454f018589c5d93860443d26859.png" />
</div>
</div>
<p>The gradient at our new point (shown in orange) is still not close to zero, meaning we haven’t reached the minimum. We’ll continue this process of checking if the gradient is nearly zero, and take a step in the direction of steepest descent until we reach the bottom of the valley. We’ll say we’ve reached the bottom of the valley when the absolute value of the gradient is <span class="math notranslate nohighlight">\(&lt;0.1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># plot y = x ** 2</span>

<span class="c1"># We are now at our second point so we need to update our tensors containing our current coordinates</span>
<span class="n">x_current</span> <span class="o">=</span> <span class="n">x_new</span>
<span class="n">y_current</span> <span class="o">=</span> <span class="n">y_new</span>


<span class="c1"># We automate this process with the following while loop</span>
<span class="n">y_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_current.grad</span>
<span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x_current</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.1</span><span class="p">:</span>  <span class="c1"># Check to see if we&#39;re at minimum</span>
    <span class="c1"># Get tensors containing new coordinates</span>
    <span class="n">x_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">x_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">x_current</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">y_new</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>

    <span class="c1"># Add new coordinates to lists</span>
    <span class="n">x_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">y_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># Update current position</span>
    <span class="n">x_current</span> <span class="o">=</span> <span class="n">x_new</span>
    <span class="n">y_current</span> <span class="o">=</span> <span class="n">y_new</span>

    <span class="c1"># Update current slope</span>
    <span class="n">y_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_current.grad</span>


<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">,</span> <span class="n">y_coords</span><span class="p">)</span>  <span class="c1"># plot points showing steps</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>  <span class="c1"># highlight last point</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b555dc26be48899b117e71e690cb7df070138fa29eee2689d72878b436f76e01.png" src="../_images/b555dc26be48899b117e71e690cb7df070138fa29eee2689d72878b436f76e01.png" />
</div>
</div>
<p>This works, but it takes a long time since we have several small steps.</p>
<p>Can we speed up the process by taking large steps? Most likely, yes. But there is a danger in taking step sizes that are too large. For example, let’s repeat this exercise with a step size of <span class="math notranslate nohighlight">\(1.5\)</span>. Our first step now looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_large_step</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_large_step</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x_large_step</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># plot y = x ** 2</span>

<span class="c1"># Current values at starting point we chose:</span>
<span class="n">x_large_step_current</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_large_step_current</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_large_step_current</span><span class="p">)</span>

<span class="c1"># Slope at current point</span>
<span class="n">y_large_step_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_large_step_current.grad</span>
<span class="n">slope_large_step_current</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x_large_step_current</span><span class="o">.</span><span class="n">grad</span>
<span class="p">)</span>  <span class="c1"># tensor containing derivative of y = x ** 2 evaluated at current point</span>

<span class="c1"># To keep track of our coordinates at each step, we will create 2 lists, initialized with the coordinates at our chosen starting point</span>
<span class="c1"># These lists will be used to plot points with Matplotlib.pyplot so we use .item() to only retain the value in the tensor</span>
<span class="n">x_large_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="n">y_large_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">y_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>

<span class="c1"># New step_size</span>
<span class="n">large_step_size</span> <span class="o">=</span> <span class="mf">1.5</span>

<span class="c1"># Get coordinates of our second point using x_new equation and y = x ** 2</span>
<span class="n">x_large_step_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="n">x_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">slope_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">large_step_size</span><span class="p">,</span>
    <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">y_large_step_new</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_large_step_new</span><span class="p">)</span>

<span class="c1"># Store second point coordinates in our lists</span>
<span class="n">x_large_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_large_step_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">y_large_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_large_step_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>


<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_large_coords</span><span class="p">,</span> <span class="n">y_large_coords</span><span class="p">)</span>  <span class="c1"># plot points showing steps</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_large_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_large_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="s2">&quot;step 1&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">20</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">260</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/03507cfedf12a44d611a8d154f2826c860b51a2438ffbd10054023085b3a891c.png" src="../_images/03507cfedf12a44d611a8d154f2826c860b51a2438ffbd10054023085b3a891c.png" />
</div>
</div>
<p><em>Note the change in scale!</em> With only one step, we already see that we stepped <em>right over</em> the minimum to somewhere far up the other side of the valley (orange point)! This is not good. If we kept iterating with the same learning rate, we’d find that the optimization process diverges and the step sizes start blowing up. This is why it is important to pick the proper step size by setting the learning rate appropriately. Steps that are too small take a long time while steps that are too large render the optimization process invalid. In this case, a reasonable choice appears to be <code class="docutils literal notranslate"><span class="pre">step</span> <span class="pre">size</span> <span class="pre">=</span> <span class="pre">0.6</span></code>, which would have reached pretty close to the minimum after only 3 steps.</p>
<p>To sum up, optimizing a function with gradient descent consists of</p>
<ol class="arabic simple">
<li><p>Calculate the gradient at your current point</p></li>
<li><p>Determine if the gradient is within the stopping criterion (in this case, the gradient is about equal to zero or <span class="math notranslate nohighlight">\(&lt;0.1\)</span>), if so stop</p></li>
<li><p>Otherwise, take a step in the direction of the gradient and go to #1</p></li>
</ol>
<p>Autodifferentiation frameworks like PyTorch allow us to easily calculate the gradient of complex functions, including a large set of prior/regularizer functions that we would want to use for Regularized Maximum Likelihood (RML) imaging. This makes it relatively easy to quickly and efficiently solve for the “optimal” image given a set of data and regularizer terms.</p>
</section>
<section id="additional-resources">
<h2>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/autograd.html">PyTorch documentation on autograd</a></p></li>
<li><p><a class="reference external" href="https://anguswilliams91.github.io/statistics/computing/jax/">Angus Williams’ blog post on autodifferentiaton, JAX, and Laplace’s method</a></p></li>
<li><p><a class="reference external" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">Paperspace blog post on understanding graphs and automatic differentiation</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/IHZwWFHWa-w">3Blue1Brown video on gradient descent</a></p></li>
</ul>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../api.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">API</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="gridder.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gridding and diagnostic images</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ian Czekala<br/>
  
      &copy; Copyright 2019-22, Ian Czekala.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>