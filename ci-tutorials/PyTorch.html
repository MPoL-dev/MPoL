

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Introduction to PyTorch: Tensors and Gradient Descent &#8212; MPoL 0.2.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="https://buttons.github.io/buttons.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ci-tutorials/PyTorch';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gridding and diagnostic images" href="gridder.html" />
    <link rel="prev" title="API" href="../api.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="MPoL 0.2.0 documentation - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="MPoL 0.2.0 documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../rml_intro.html">Introduction to Regularized Maximum Likelihood Imaging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">MPoL Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../units-and-conventions.html">Units and Conventions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer-documentation.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Introduction to PyTorch: Tensors and Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="gridder.html">Gridding and diagnostic images</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">Intro to RML with MPoL</a></li>
<li class="toctree-l1"><a class="reference internal" href="loose-visibilities.html">Likelihood functions and model visibilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="crossvalidation.html">Cross validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu_setup.html">GPU Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="initializedirtyimage.html">Initializing with the Dirty Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../large-tutorials/HD143006_part_1.html">HD143006 Tutorial Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../large-tutorials/HD143006_part_2.html">HD143006 Tutorial Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="fakedata.html">Making a Mock Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../large-tutorials/pyro.html">Parametric Inference with Pyro</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/MPoL-dev/MPoL" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/ci-tutorials/PyTorch.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction to PyTorch: Tensors and Gradient Descent</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-tensors">Introduction to Tensors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-gradients">Calculating Gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-a-function-with-gradient-descent">Optimizing a Function with Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">run</span> notebook_setup
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="introduction-to-pytorch-tensors-and-gradient-descent">
<h1>Introduction to PyTorch: Tensors and Gradient Descent<a class="headerlink" href="#introduction-to-pytorch-tensors-and-gradient-descent" title="Permalink to this heading">#</a></h1>
<p>This tutorial provides a gentle introduction to PyTorch tensors, automatic differentiation, and optimization with gradient descent outside of any specifics about radio interferometry or the MPoL package itself.</p>
<section id="introduction-to-tensors">
<h2>Introduction to Tensors<a class="headerlink" href="#introduction-to-tensors" title="Permalink to this heading">#</a></h2>
<p>Tensors are multi-dimensional arrays, similar to numpy arrays, with the added benefit that they can be used to calculate gradients (more on that later). MPoL is built on the <a class="reference external" href="https://pytorch.org/">PyTorch</a> machine learning library, and uses a form of gradient descent optimization to find the “best” image given some dataset and loss function, which may include regularizers.</p>
<p>We’ll start this tutorial by importing the torch and numpy packages. Make sure you have <a class="reference external" href="https://pytorch.org/get-started/locally/">PyTorch installed</a> before proceeding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<p>There are several <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">ways to initialize a tensor</a>. A common method to create a tensor is from a numpy array:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">an_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">a_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">an_array</span><span class="p">)</span>  <span class="c1"># creates tensor of same size as an_array</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 2],
        [3, 4]])
</pre></div>
</div>
</div>
</div>
<p>Tensors are similar to numpy arrays—many of the same <a class="reference external" href="https://pytorch.org/docs/stable/torch.html">operations</a> that we would perform on numpy arrays can easily be performed on PyTorch tensors. For example, we can compare how to calculate a matrix product using numpy and PyTorch</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">another_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># create 2x3 array</span>
<span class="n">another_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="n">another_array</span>
<span class="p">)</span>  <span class="c1"># create another tensor of same size as above array</span>

<span class="c1"># numpy array multiplication</span>
<span class="n">prod_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">an_array</span><span class="p">,</span> <span class="n">another_array</span><span class="p">)</span>

<span class="c1"># torch tensor multiplication</span>
<span class="n">prod_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_tensor</span><span class="p">,</span> <span class="n">another_tensor</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Numpy array multiplication result: </span><span class="si">{</span><span class="n">prod_array</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Torch tensor multiplication result: </span><span class="si">{</span><span class="n">prod_tensor</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Numpy array multiplication result: [[21 24  7]
 [47 54 21]]
Torch tensor multiplication result: tensor([[21, 24,  7],
        [47, 54, 21]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="calculating-gradients">
<h2>Calculating Gradients<a class="headerlink" href="#calculating-gradients" title="Permalink to this heading">#</a></h2>
<p>PyTorch allows us to calculate the gradients on tensors, which is a key functionality underlying MPoL. Let’s start by creating a tensor with a single value. Here we are setting <code class="docutils literal notranslate"><span class="pre">requires_grad</span> <span class="pre">=</span> <span class="pre">True</span></code>; we’ll see why this is important in a moment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(3., requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>Let’s define some variable <span class="math notranslate nohighlight">\(y\)</span> in terms of <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<p>We see that the value of <span class="math notranslate nohighlight">\(y\)</span> is as we expect—nothing too strange here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x: 3.0
y: 9.0
</pre></div>
</div>
</div>
</div>
<p>But what if we wanted to calculate the gradient of <span class="math notranslate nohighlight">\(y\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>? Using calculus, we find that the answer is <span class="math notranslate nohighlight">\(\frac{dy}{dx} = 2x\)</span>. The derivative evaluated at <span class="math notranslate nohighlight">\(x = 3\)</span> is <span class="math notranslate nohighlight">\(6\)</span>.</p>
<p>We can use PyTorch to get the same answer—no analytic derivative needed!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populates gradient (.grad) attributes of y with respect to all of its independent variables</span>
<span class="n">x</span><span class="o">.</span><span class="n">grad</span>  <span class="c1"># returns the grad attribute (the gradient) of y with respect to x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(6.)
</pre></div>
</div>
</div>
</div>
<p>PyTorch uses the concept of <a class="reference external" href="https://arxiv.org/abs/1502.05767">automatic differentiation</a> to calculate the derivative. Instead of computing the derivative as we would by hand, the program uses a computational graph and the mechanistic application of the chain rule. For example, a computational graph with several operations on <span class="math notranslate nohighlight">\(x\)</span> resulting in a final output <span class="math notranslate nohighlight">\(y\)</span> will use the chain rule to compute the differential associated with each operation and multiply these differentials together to get the derivative of <span class="math notranslate nohighlight">\(y\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>.</p>
</section>
<section id="optimizing-a-function-with-gradient-descent">
<h2>Optimizing a Function with Gradient Descent<a class="headerlink" href="#optimizing-a-function-with-gradient-descent" title="Permalink to this heading">#</a></h2>
<p>If we were on the side of a hill in the dark and we wanted to get down to the bottom of a valley, how might we do it?</p>
<p>We can’t see all the way to the bottom of the valley, but we can feel which way is down based on the incline of where we are standing. We might take steps in the downward direction and we’d know when to stop when the ground finally felt flat. We would also need to consider how large our steps should be. If we take very small steps, it will take us a longer time than if we take larger steps. However, if we take large leaps, we might completely miss the flat part of the valley, and jump straight across to the other side of the valley.</p>
<p>Now let’s take a more quantitative look at the gradient descent using the function <span class="math notranslate nohighlight">\(y = x^2\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will choose some arbitrary place to start on the left side of the hill and use PyTorch to calculate the tangent.</p>
<p>Note that the plotting library Matplotlib requires numpy arrays instead of PyTorch tensors, so in the following code you might see the occasional <code class="docutils literal notranslate"><span class="pre">detach().numpy()</span></code> or <code class="docutils literal notranslate"><span class="pre">.item()</span></code> calls, which are used to convert PyTorch tensors to numpy arrays and scalar values, respectively, for plotting. When it comes time to use MPoL for RML imaging, or any large production run, we’ll try to keep the calculations native to PyTorch tensors as long as possible, to avoid the overhead of converting types.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">x_start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>  <span class="c1"># tensor with x coordinate of starting point</span>
<span class="n">y_start</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>  <span class="c1"># tensor with y coordinate of starting point</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_start</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">y_start</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># plot starting point</span>

<span class="c1"># we can calculate the derivative of y = x ** 2 evaluated at x_start</span>
<span class="n">y_start</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_start.grad</span>
<span class="n">slope_start</span> <span class="o">=</span> <span class="n">x_start</span><span class="o">.</span><span class="n">grad</span>

<span class="c1"># and use this to evaluate the tangent line</span>
<span class="n">tangent_line</span> <span class="o">=</span> <span class="n">slope_start</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_start</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_start</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tangent_line</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dfdf0cf35b8294203427be2940b2db02f73d6348a17767e03dae3f5023ac90fa.png" src="../_images/dfdf0cf35b8294203427be2940b2db02f73d6348a17767e03dae3f5023ac90fa.png" />
</div>
</div>
<p>We see we need to go to the right to go down toward the minimum. For a multivariate function, the gradient will be a vector pointing in the direction of the steepest downward slope. When we take steps, we find the x coordinate of our new location by:</p>
<p><span class="math notranslate nohighlight">\(x_\mathrm{new} = x_\mathrm{current} - \nabla y(x_\mathrm{current}) * (\mathrm{step\,size})\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_\mathrm{current}\)</span> is our current x value</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla y(x_\mathrm{current})\)</span> is the gradient at our current point</p></li>
<li><p><span class="math notranslate nohighlight">\((\mathrm{step\,size})\)</span> is a value we choose that scales our steps</p></li>
</ul>
<p>We will choose <code class="docutils literal notranslate"><span class="pre">step_size</span> <span class="pre">=</span> <span class="pre">0.1</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># plot y = x ** 2</span>

<span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Tensors containing current coordinates at the starting point we chose:</span>
<span class="n">x_current</span> <span class="o">=</span> <span class="n">x_start</span>
<span class="n">y_current</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_current</span><span class="p">)</span>

<span class="c1"># To keep track of our coordinates at each step, we will create 2 lists, initialized with the values at our chosen starting point</span>
<span class="c1"># These lists will be used to plot points with Matplotlib.pyplot so we use .item() to only retain the value in the tensor</span>
<span class="n">x_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="n">y_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">y_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>

<span class="c1"># Slope at current point</span>
<span class="n">y_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_current.grad</span>
<span class="n">slope_current</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x_current</span><span class="o">.</span><span class="n">grad</span>
<span class="p">)</span>  <span class="c1"># tensor containing derivative of y = x ** 2 evaluated at current point</span>

<span class="c1"># Using equation for x_new to get x coordinate of second point, store it in a tensor</span>
<span class="c1"># We cannot use torch.tensor(...) to make a new tensor from previous tensors without altering the</span>
<span class="c1"># computational graph. We use .item() to only use float values to create our new tensor</span>
<span class="n">x_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="n">x_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="n">slope_current</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Plug in x_new into y = x ** 2 to get y_new of second point</span>
<span class="n">y_new</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>


<span class="c1"># Store second point coordinates in our lists</span>
<span class="n">x_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">y_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>


<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">,</span> <span class="n">y_coords</span><span class="p">)</span>  <span class="c1"># plot points showing steps</span>
<span class="c1"># replot the last point in a new color</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;step 1&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e3f7b005855acd893ba88e5878ba004ef2538d97c0836896e0347095220aabcd.png" src="../_images/e3f7b005855acd893ba88e5878ba004ef2538d97c0836896e0347095220aabcd.png" />
</div>
</div>
<p>The gradient at our new point (shown in orange) is still not close to zero, meaning we haven’t reached the minimum. We’ll continue this process of checking if the gradient is nearly zero, and take a step in the direction of steepest descent until we reach the bottom of the valley. We’ll say we’ve reached the bottom of the valley when the absolute value of the gradient is <span class="math notranslate nohighlight">\(&lt;0.1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># plot y = x ** 2</span>

<span class="c1"># We are now at our second point so we need to update our tensors containing our current coordinates</span>
<span class="n">x_current</span> <span class="o">=</span> <span class="n">x_new</span>
<span class="n">y_current</span> <span class="o">=</span> <span class="n">y_new</span>


<span class="c1"># We automate this process with the following while loop</span>
<span class="n">y_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_current.grad</span>
<span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x_current</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.1</span><span class="p">:</span>  <span class="c1"># Check to see if we&#39;re at minimum</span>
    <span class="c1"># Get tensors containing new coordinates</span>
    <span class="n">x_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">x_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">x_current</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">y_new</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>

    <span class="c1"># Add new coordinates to lists</span>
    <span class="n">x_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">y_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># Update current position</span>
    <span class="n">x_current</span> <span class="o">=</span> <span class="n">x_new</span>
    <span class="n">y_current</span> <span class="o">=</span> <span class="n">y_new</span>

    <span class="c1"># Update current slope</span>
    <span class="n">y_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_current.grad</span>


<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">,</span> <span class="n">y_coords</span><span class="p">)</span>  <span class="c1"># plot points showing steps</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>  <span class="c1"># highlight last point</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/6930efd6c02871bf9782795d8f70b08f0aec315d6e1571c80d02170d27a0c81d.png" src="../_images/6930efd6c02871bf9782795d8f70b08f0aec315d6e1571c80d02170d27a0c81d.png" />
</div>
</div>
<p>This works, but it takes a long time since we have several small steps.</p>
<p>Can we speed up the process by taking large steps? Most likely, yes. But there is a danger in taking step sizes that are too large. For example, let’s repeat this exercise with a step size of <span class="math notranslate nohighlight">\(1.5\)</span>. Our first step now looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_large_step</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_large_step</span><span class="p">,</span> <span class="n">y</span><span class="p">(</span><span class="n">x_large_step</span><span class="p">),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># plot y = x ** 2</span>

<span class="c1"># Current values at starting point we chose:</span>
<span class="n">x_large_step_current</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_large_step_current</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_large_step_current</span><span class="p">)</span>

<span class="c1"># Slope at current point</span>
<span class="n">y_large_step_current</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># populate x_large_step_current.grad</span>
<span class="n">slope_large_step_current</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">x_large_step_current</span><span class="o">.</span><span class="n">grad</span>
<span class="p">)</span>  <span class="c1"># tensor containing derivative of y = x ** 2 evaluated at current point</span>

<span class="c1"># To keep track of our coordinates at each step, we will create 2 lists, initialized with the coordinates at our chosen starting point</span>
<span class="c1"># These lists will be used to plot points with Matplotlib.pyplot so we use .item() to only retain the value in the tensor</span>
<span class="n">x_large_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="n">y_large_coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">y_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>

<span class="c1"># New step_size</span>
<span class="n">large_step_size</span> <span class="o">=</span> <span class="mf">1.5</span>

<span class="c1"># Get coordinates of our second point using x_new equation and y = x ** 2</span>
<span class="n">x_large_step_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="n">x_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">slope_large_step_current</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">large_step_size</span><span class="p">,</span>
    <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">y_large_step_new</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">x_large_step_new</span><span class="p">)</span>

<span class="c1"># Store second point coordinates in our lists</span>
<span class="n">x_large_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_large_step_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">y_large_coords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_large_step_new</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>


<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_large_coords</span><span class="p">,</span> <span class="n">y_large_coords</span><span class="p">)</span>  <span class="c1"># plot points showing steps</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_large_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_large_coords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="s2">&quot;step 1&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xmin</span><span class="o">=-</span><span class="mi">20</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">260</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d582c6e26c212383ba79f829d3e11eb6de5b9e18bc55cc5902719f8240a64f41.png" src="../_images/d582c6e26c212383ba79f829d3e11eb6de5b9e18bc55cc5902719f8240a64f41.png" />
</div>
</div>
<p><em>Note the change in scale!</em> With only one step, we already see that we stepped <em>right over</em> the minimum to somewhere far up the other side of the valley (orange point)! This is not good. If we kept iterating with the same learning rate, we’d find that the optimization process diverges and the step sizes start blowing up. This is why it is important to pick the proper step size by setting the learning rate appropriately. Steps that are too small take a long time while steps that are too large render the optimization process invalid. In this case, a reasonable choice appears to be <code class="docutils literal notranslate"><span class="pre">step</span> <span class="pre">size</span> <span class="pre">=</span> <span class="pre">0.6</span></code>, which would have reached pretty close to the minimum after only 3 steps.</p>
<p>To sum up, optimizing a function with gradient descent consists of</p>
<ol class="arabic simple">
<li><p>Calculate the gradient at your current point</p></li>
<li><p>Determine if the gradient is within the stopping criterion (in this case, the gradient is about equal to zero or <span class="math notranslate nohighlight">\(&lt;0.1\)</span>), if so stop</p></li>
<li><p>Otherwise, take a step in the direction of the gradient and go to #1</p></li>
</ol>
<p>Autodifferentiation frameworks like PyTorch allow us to easily calculate the gradient of complex functions, including a large set of prior/regularizer functions that we would want to use for Regularized Maximum Likelihood (RML) imaging. This makes it relatively easy to quickly and efficiently solve for the “optimal” image given a set of data and regularizer terms.</p>
</section>
<section id="additional-resources">
<h2>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/autograd.html">PyTorch documentation on autograd</a></p></li>
<li><p><a class="reference external" href="https://anguswilliams91.github.io/statistics/computing/jax/">Angus Williams’ blog post on autodifferentiaton, JAX, and Laplace’s method</a></p></li>
<li><p><a class="reference external" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">Paperspace blog post on understanding graphs and automatic differentiation</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/IHZwWFHWa-w">3Blue1Brown video on gradient descent</a></p></li>
</ul>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../api.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">API</p>
      </div>
    </a>
    <a class="right-next"
       href="gridder.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gridding and diagnostic images</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-tensors">Introduction to Tensors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-gradients">Calculating Gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-a-function-with-gradient-descent">Optimizing a Function with Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ian Czekala
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2019-22, Ian Czekala.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>