

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>HD143006 Tutorial Part 2 &mdash; MPoL 0.1.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://buttons.github.io/buttons.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
  <link rel="stylesheet" href="../_static/bullets.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/faculty.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans|Roboto:400,700|Roboto+Mono:400,700&display=swap" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Changelog" href="../changelog.html" />
    <link rel="prev" title="HD143006 Tutorial Part 1" href="HD143006_part_1.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

  
    <a class="heading heading-extra-margin" href="../index.html">
      <div class="logo-box logo-box-large">
        <img class="logo" src="../_static/logo.png"/>
      </div>
      
        <span class="icon icon-home"> MPoL</span>
      
    </a>
  

  
    
    
      <div class="version">0.1.1</div>
    
  

  
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rml_intro.html">Introduction to Regularized Maximum Likelihood Imaging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">MPoL Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../units-and-conventions.html">Units and Conventions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer-documentation.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../ci-tutorials/PyTorch.html">Introduction to PyTorch: Tensors and Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ci-tutorials/gridder.html">Gridding and diagnostic images</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ci-tutorials/optimization.html">Optimization Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ci-tutorials/crossvalidation.html">Cross validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ci-tutorials/gpu_setup.html">GPU Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ci-tutorials/initializedirtyimage.html">Initializing with the Dirty Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="HD143006_part_1.html">HD143006 Tutorial Part 1</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">HD143006 Tutorial Part 2</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#loading-data">Loading Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#getting-the-dirty-image-and-creating-the-model">Getting the Dirty Image and Creating the Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#initializing-model-with-the-dirty-image">Initializing Model with the Dirty Image</a></li>
<li class="toctree-l2"><a class="reference internal" href="#visualization-utilities">Visualization utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-loop">Training loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cross-validation">Cross Validation</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MPoL</a>
        
      </nav>


      <div class="wy-nav-content">

  

  
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
  <li class="breadcrumb"><a href="../index.html">MPoL</a> &raquo;</li>
    
  <li class="breadcrumb">HD143006 Tutorial Part 2</li>

    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/large-tutorials/HD143006_part_2.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%matplotlib inline
%run notebook_setup
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/ian/Documents/Research/Disks/RML/MPoL/docs/large-tutorials/notebook_setup.py:7: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).
  get_ipython().magic(&#39;config InlineBackend.figure_format = &quot;retina&quot;&#39;)
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="hd143006-tutorial-part-2">
<h1>HD143006 Tutorial Part 2<a class="headerlink" href="#hd143006-tutorial-part-2" title="Permalink to this heading">¶</a></h1>
<p>This tutorial is part 2 of the HD 143006 tutorial series (part 1 can be found <a class="reference internal" href="HD143006_part_1.html"><span class="doc std std-doc">here</span></a>).</p>
<p>We’ll be covering much of the same content in the tutorials on <a class="reference internal" href="../ci-tutorials/optimization.html"><span class="doc std std-doc">optimization</span></a>, <a class="reference internal" href="../ci-tutorials/initializedirtyimage.html"><span class="doc std std-doc">initalizing with the dirty image</span></a>, and <a class="reference internal" href="../ci-tutorials/crossvalidation.html"><span class="doc std std-doc">cross validation</span></a> as part of an integrated workflow using real data. For more information on a particular step, we recommend referencing the individual tutorials.</p>
<p>This tutorial will cover model initialization, optimization, and cross validation, as well as touch on how to use <a class="reference external" href="https://www.tensorflow.org/tensorboard">TensorBoard</a> to analyze the results.</p>
<section id="loading-data">
<h2>Loading Data<a class="headerlink" href="#loading-data" title="Permalink to this heading">¶</a></h2>
<p>Let’s load the data as we did in the previous HD143006 tutorial (<a class="reference internal" href="HD143006_part_1.html"><span class="doc std std-doc">part 1</span></a>) and create an MPoL Gridder object.</p>
<p><em>You can download the extracted visibilities (<code class="docutils literal notranslate"><span class="pre">HD143006_continuum.npz</span></code>) directly to your working directory, or use the Astropy <code class="docutils literal notranslate"><span class="pre">download_file</span></code> utility to download it during run time.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
import numpy as np
from astropy.io import fits
from astropy.utils.data import download_file
import torch
from torch.utils import tensorboard

# downloading extracted visibilities file
fname = download_file(
    &quot;https://zenodo.org/record/4904794/files/HD143006_continuum.npz&quot;,
    cache=True,
    pkgname=&quot;mpol&quot;,
)

# load extracted visibilities from npz file
d = np.load(fname)
uu = d[&quot;uu&quot;]
vv = d[&quot;vv&quot;]
weight = d[&quot;weight&quot;]
data = d[&quot;data&quot;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from mpol import gridding, coordinates

# we&#39;ll assume the same cell_size as before
cell_size = 0.003  # arcseconds

# creating Gridder object
coords = coordinates.GridCoords(cell_size=cell_size, npix=512)
gridder = gridding.Gridder(
    coords=coords,
    uu=uu,
    vv=vv,
    weight=weight,
    data_re=data.real,
    data_im=data.imag,
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="getting-the-dirty-image-and-creating-the-model">
<h2>Getting the Dirty Image and Creating the Model<a class="headerlink" href="#getting-the-dirty-image-and-creating-the-model" title="Permalink to this heading">¶</a></h2>
<p>First, we will use the Gridder to solve for a dirty image, so that we can</p>
<ol class="arabic simple">
<li><p>confirm that we’ve loaded the visibilities correctly and that the dirty image looks roughly as we’d expect (as in <a class="reference internal" href="HD143006_part_1.html"><span class="doc std std-doc">part 1</span></a>)</p></li>
<li><p>use the dirty image to initialize the RML model image state (as in the <a class="reference internal" href="../ci-tutorials/initializedirtyimage.html"><span class="doc std std-doc">initalizing with the dirty image</span></a> tutorial)</p></li>
</ol>
<p>We will use Briggs weighting with <code class="docutils literal notranslate"><span class="pre">robust=0.0</span></code>, since this similar to what the DSHARP team used.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>img, beam = gridder.get_dirty_image(weighting=&quot;briggs&quot;, robust=0.0, unit=&quot;Jy/arcsec^2&quot;)
</pre></div>
</div>
</div>
</div>
<p>Now is also a good time to export the gridded visibilities to a PyTorch dataset, to be used later during the RML imaging loop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dataset = (
    gridder.to_pytorch_dataset()
)  # export the visibilities from gridder to a PyTorch dataset
</pre></div>
</div>
</div>
</div>
<p>Now let’s import a <a class="reference internal" href="../api.html#mpol.precomposed.SimpleNet" title="mpol.precomposed.SimpleNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">mpol.precomposed.SimpleNet</span></code></a> model for RML imaging.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from mpol.precomposed import SimpleNet

model = SimpleNet(coords=coords, nchan=gridder.nchan)
</pre></div>
</div>
</div>
</div>
</section>
<section id="initializing-model-with-the-dirty-image">
<h2>Initializing Model with the Dirty Image<a class="headerlink" href="#initializing-model-with-the-dirty-image" title="Permalink to this heading">¶</a></h2>
<p>Before we begin the optimization process, we need to choose a set of starting values for our image model. So long as we run the optimization process to convergence, this starting point could be anything. But, it’s also true that “better” guesses will lead us to convergence faster. A fairly decent starting guess is the dirty image itself, since it is already a maximum likelihood fit to the data. The problem, though, is that the dirty image contains pixels with negative flux, while by construction our <a class="reference internal" href="../api.html#mpol.precomposed.SimpleNet" title="mpol.precomposed.SimpleNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">mpol.precomposed.SimpleNet</span></code></a> model enforces image positivity. One solution is to train the RML model such that the mean squared error between its image plane component and the dirty image are minimized. This “mini” optimization loop is just to find a starting point for the actual RML optimization loop, which will be described later in this tutorial.</p>
<p>Following the <a class="reference internal" href="../ci-tutorials/initializedirtyimage.html"><span class="doc std std-doc">initalizing with the dirty image</span></a> tutorial, we use PyTorch’s <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html">mean squared error</a> function to calculate the loss between the RML model image and the dirty image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># convert the dirty image into a tensor
dirty_image = torch.tensor(img.copy())

# initialize an optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.5)
loss_fn = torch.nn.MSELoss()  # creating the MSEloss function from Pytorch
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for iteration in range(500):

    optimizer.zero_grad()

    model.forward()  # get the predicted model
    sky_cube = model.icube.sky_cube

    loss = loss_fn(sky_cube, dirty_image)  # calculate the loss

    loss.backward()  # calculate gradients of parameters
    optimizer.step()  # update the parameters
</pre></div>
</div>
</div>
</div>
<p>Let’s visualize the actual dirty image and our “pseudo-dirty image” that results from the optimization process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(ncols=2, figsize=(6.5, 4))

ax[0].imshow(
    np.squeeze(dirty_image.detach().cpu().numpy()),
    origin=&quot;lower&quot;,
    interpolation=&quot;none&quot;,
    extent=model.icube.coords.img_ext,
)

ax[1].imshow(
    np.squeeze(model.icube.sky_cube.detach().cpu().numpy()),
    origin=&quot;lower&quot;,
    interpolation=&quot;none&quot;,
    extent=model.icube.coords.img_ext,
)

r = 0.75
for a in ax:
    a.set_xlim(left=0.75, right=-0.75)
    a.set_ylim(bottom=-0.75, top=0.75)
    a.axis(&quot;off&quot;)

ax[0].set_title(&quot;Dirty Image&quot;)
_ = ax[1].set_title(&quot;Pseudo-Dirty Image&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4bfd4ec2255e041bb4d28b91436ba94fc5f46117531c09294270c73cafd027cc.png" src="../_images/4bfd4ec2255e041bb4d28b91436ba94fc5f46117531c09294270c73cafd027cc.png" />
</div>
</div>
<p>We can confirm that the pseudo-dirty image contains no negative flux values</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(
    &quot;Minimum flux value {:.4f} Jy/arcsec^2&quot;.format(
        np.min(model.icube.sky_cube.detach().cpu().numpy())
    )
)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Minimum flux value 0.0014 Jy/arcsec^2
</pre></div>
</div>
</div>
</div>
<p>Later in this tutorial, we’ll want to run many RML optimization loops with different hyperparameter configurations. To make this process easier, we’ll save the model state to disk, making it easy for us restart from the pseudo-dirty image each time. More information on saving and loading models (and the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>) can be found in the <a class="reference external" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch documentation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>torch.save(model.state_dict(), &quot;model.pt&quot;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualization-utilities">
<h2>Visualization utilities<a class="headerlink" href="#visualization-utilities" title="Permalink to this heading">¶</a></h2>
<p>In this section we’ll set up some visualization tools, including <a class="reference external" href="https://pytorch.org/docs/stable/tensorboard.html">TensorBoard</a>.</p>
<p>Note that to display the TensorBoard dashboards referenced in this tutorial, you will need to run a TensorBoard instance on your own device. The code necessary to do this is displayed in this tutorial as <code class="docutils literal notranslate"><span class="pre">#%tensorboard</span> <span class="pre">--logdir</span> <span class="pre">&lt;directory&gt;</span></code>. Uncomment and execute this IPython line magic command in Jupyter Notebook to open the dashboard.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from torch.utils.tensorboard import SummaryWriter
import os

logs_base_dir = &quot;./logs/&quot;
writer = SummaryWriter(logs_base_dir)
os.makedirs(logs_base_dir, exist_ok=True)
%load_ext tensorboard
</pre></div>
</div>
</div>
</div>
<p>Here we’ll define a plotting routine that will visualize the image plane model cube, the image plane residuals, the amplitude of the Fourier plane model, and the amplitude of the Fourier plane residuals.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def log_figure(model, residuals):
    &quot;&quot;&quot;
    Args:
        model: the SimpleNet model instanceS
        residuals: the mpol ResidualConnector instance

    Returns:
        matplotlib figure with plots corresponding to image
        plane model cube, the image plane residuals, the amplitude
        of the Fourier plane model, and the amplitude of the Fourier
        plane residuals.
    &quot;&quot;&quot;

    # populate residual connector
    residuals()

    fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(10, 10))
    im = ax[0, 0].imshow(
        np.squeeze(model.icube.sky_cube.detach().cpu().numpy()),
        origin=&quot;lower&quot;,
        interpolation=&quot;none&quot;,
        extent=model.icube.coords.img_ext,
    )
    plt.colorbar(im, ax=ax[0, 0])

    im = ax[0, 1].imshow(
        np.squeeze(residuals.sky_cube.detach().cpu().numpy()),
        origin=&quot;lower&quot;,
        interpolation=&quot;none&quot;,
        extent=residuals.coords.img_ext,
    )
    plt.colorbar(im, ax=ax[0, 1])

    im = ax[1, 0].imshow(
        np.squeeze(torch.log(model.fcube.ground_amp.detach()).cpu().numpy()),
        origin=&quot;lower&quot;,
        interpolation=&quot;none&quot;,
        extent=residuals.coords.vis_ext,
    )
    plt.colorbar(im, ax=ax[1, 0])

    im = ax[1, 1].imshow(
        np.squeeze(torch.log(residuals.ground_amp.detach()).cpu().numpy()),
        origin=&quot;lower&quot;,
        interpolation=&quot;none&quot;,
        extent=residuals.coords.vis_ext,
    )
    plt.colorbar(im, ax=ax[1, 1])

    return fig
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-loop">
<h2>Training loop<a class="headerlink" href="#training-loop" title="Permalink to this heading">¶</a></h2>
<p>Now lets encapsulate the training loop into a function which we can easily re-run with different configuration values.</p>
<p>To learn more information about the components of this training loop, please see the <a class="reference internal" href="../api.html#module-mpol.losses" title="mpol.losses"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Losses</span> <span class="pre">API</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from mpol import losses, connectors


def train(model, dataset, optimizer, config, writer=None, logevery=50):
    model.train()
    residuals = connectors.GriddedResidualConnector(model.fcube, dataset)
    for iteration in range(config[&quot;epochs&quot;]):
        optimizer.zero_grad()
        vis = model.forward()  # calculate the predicted model
        sky_cube = model.icube.sky_cube

        loss = (
            losses.nll_gridded(vis, dataset)
            + config[&quot;lambda_sparsity&quot;] * losses.sparsity(sky_cube)
            + config[&quot;lambda_TV&quot;] * losses.TV_image(sky_cube)
            + config[&quot;entropy&quot;] * losses.entropy(sky_cube, config[&quot;prior_intensity&quot;])
            + config[&quot;TSV&quot;] * losses.TSV(sky_cube)
        )

        if (
            iteration % logevery == 0
        ) and writer is not None:  # logging the loss and image for visualization and analysis
            writer.add_scalar(&quot;loss&quot;, loss.item(), iteration)
            writer.add_figure(&quot;image&quot;, log_figure(model, residuals), iteration)

        loss.backward()  # calculate gradient of the parameters
        optimizer.step()  # update the model parameters

    return loss.item()
</pre></div>
</div>
</div>
</div>
<p>Now lets initialize the model to the pseudo-dirty image, set our hyperparameters in a <code class="docutils literal notranslate"><span class="pre">config</span></code> dictionary, and create our optimizer.</p>
<p>The hyperparameters (also referred to as scalar prefactors in the <a class="reference internal" href="../rml_intro.html#rml-intro-label"><span class="std std-ref">Introduction to Regularized Maximum Likelihood Imaging</span></a>. Most of these hyperparameters, such as <code class="docutils literal notranslate"><span class="pre">lambda_TV</span></code> and <code class="docutils literal notranslate"><span class="pre">entropy</span></code> are used in the loss functions and can be read about <a class="reference internal" href="../api.html#module-mpol.losses" title="mpol.losses"><code class="xref py py-mod docutils literal notranslate"><span class="pre">here</span></code></a>. We chose these specific values from a past hyperparameter tuning trial, since they result in a decent image but still have a suboptimal cross-validation score, leaving something for us to do in the cross-validation loops at the end of this tutorial.</p>
<p>Hyperparameter values are not a “one size fits all” metric, so if you are working with a different dataset you will most likely find successful images with a different set of hyperparameters. To find your own hyperparameters, we recommend looking into <a class="reference external" href="https://docs.ray.io/en/master/tune/index.html">Ray Tune</a>, <a class="reference external" href="https://pytorch.org/docs/stable/tensorboard.html">TensorBoard</a>, or your favorite hyperparameter tuning library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model.load_state_dict(
    torch.load(&quot;model.pt&quot;)
)  # load our initialized model from the previous section
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;All keys matched successfully&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>config = (
    {  # config includes the hyperparameters used in the function and in the optimizer
        &quot;lr&quot;: 0.3,
        &quot;lambda_sparsity&quot;: 7.0e-05,
        &quot;lambda_TV&quot;: 0.00,
        &quot;entropy&quot;: 1e-03,
        &quot;prior_intensity&quot;: 1.5e-07,
        &quot;TSV&quot;: 0.00,
        &quot;epochs&quot;: 1000,
    }
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>optimizer = torch.optim.Adam(
    model.parameters(), lr=config[&quot;lr&quot;]
)  # create our optimizer, using the learning rate from config
</pre></div>
</div>
</div>
</div>
<p>We are now ready to run the training loop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train(model, dataset, optimizer, config, writer=writer)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.2544955152404116
</pre></div>
</div>
</div>
</div>
<p>Below we can see the loss function, images, and residuals for every saved iteration including our final result. To view the loss function, navigate to the scalars tab. To view the four images, be sure your window is wide enough to navigate to the images tab within TensorBoard. The images, in order from left-right top-bottom are: image cube representation, imaged residuals, visibility amplitudes of model on a log scale, residual amplitudes on a log scale. You can use the slider to view different iterations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># %tensorboard --logdir {logs_base_dir}
## uncomment the above line when running to view TensorBoard
</pre></div>
</div>
</div>
</div>
</section>
<section id="cross-validation">
<h2>Cross Validation<a class="headerlink" href="#cross-validation" title="Permalink to this heading">¶</a></h2>
<p>Now we will move onto cross validation, which is a technique used to assess model validity. The general idea is that we store some fraction of the dataset as a test dataset and using the remaining data to train the model. Once the model is trained, it is used to predict the missing values of the data in the test dataset. These predicted values are compared to the values from the test dataset, producing a cross validation score.</p>
<p>The advantage of <span class="math notranslate nohighlight">\(k\)</span>-fold cross validation is that it allows one dataset to be used to train the model multiple times since it can take different chunks out for the test dataset. For more information see the <a class="reference internal" href="../ci-tutorials/crossvalidation.html"><span class="doc std std-doc">Cross Validation tutorial</span></a>.</p>
<p>Just like in the previous section we will be viewing our results in TensorBoard, with the addition of the cross validation score log.</p>
<p>Cross validation requires a <code class="docutils literal notranslate"><span class="pre">test</span></code> function (to determine the cross validation score) and a <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code> function (to utilize cross validation with the previous <code class="docutils literal notranslate"><span class="pre">train</span></code> function). We implement these below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def test(model, dataset):
    model.eval()
    vis = model.forward()
    loss = losses.nll_gridded(
        vis, dataset
    )  # calculates the loss function that goes to make up the cross validation score
    return loss.item()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def cross_validate(model, config, k_fold_datasets, MODEL_PATH, writer=None):
    test_scores = []

    for k_fold, (train_dset, test_dset) in enumerate(k_fold_datasets):

        # reset model
        model.load_state_dict(torch.load(MODEL_PATH))

        # create a new optimizer for this k_fold
        optimizer = torch.optim.Adam(model.parameters(), lr=config[&quot;lr&quot;])

        # train for a while
        train(model, train_dset, optimizer, config, writer=writer)
        # evaluate the test metric
        test_scores.append(test(model, test_dset))

    # aggregate all test scores and sum to evaluate cross val metric
    test_score = np.sum(np.array(test_scores))

    # adds cross validation score
    if writer is not None:
        writer.add_scalar(&quot;Cross Validation&quot;, test_score)

    return test_score
</pre></div>
</div>
</div>
</div>
<p>Now that we have our functions defined, we need to divide our dataset into training and test datasets. There are many ways of going about this; here we are splitting the dataset into radial and azimuthal chunks in a dartboard-like pattern. MPoL’s <code class="docutils literal notranslate"><span class="pre">Dartboard</span></code> presents an easy built-in way to get the polar coordinate grid of a dataset. To to read more, please see <a class="reference internal" href="../ci-tutorials/crossvalidation.html#choosing-the-k-folds"><span class="std std-doc">Choosing the K-folds</span></a> in the <a class="reference internal" href="../ci-tutorials/crossvalidation.html"><span class="doc std std-doc">Cross Validation tutorial</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from mpol import datasets

# create a radial and azimuthal partition for the dataset
dartboard = datasets.Dartboard(coords=coords)

# create cross validator using this &quot;dartboard&quot;
k = 5
cv = datasets.KFoldCrossValidatorGridded(dataset, k, dartboard=dartboard, npseed=42)

# ``cv`` is a Python iterator, it will return a ``(train, test)`` pair of ``GriddedDataset``s for each iteration.
# Because we&#39;ll want to revisit the individual datasets
# several times in this tutorial, we&#39;re storing them into a list

k_fold_datasets = [(train, test) for (train, test) in cv]
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">MODEL_PATH</span></code> is defined below so we can reset the model between cross validation loops by reloading the <code class="docutils literal notranslate"><span class="pre">model.pt</span></code> we saved, which contained the state of the model initialized to the pseudo-dirty image. We will run the cross validation loops for a few different configurations, starting with the hyperparameters found in <code class="docutils literal notranslate"><span class="pre">config</span></code>, defined above in this tutorial. This configuration has been included in the following cell for convenience.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>MODEL_PATH = &quot;model.pt&quot;

new_config = (
    {  # config includes the hyperparameters used in the function and in the optimizer
        &quot;lr&quot;: 0.3,
        &quot;lambda_sparsity&quot;: 7.0e-05,
        &quot;lambda_TV&quot;: 0.00,
        &quot;entropy&quot;: 1e-03,
        &quot;prior_intensity&quot;: 1.5e-07,
        &quot;TSV&quot;: 0.00,
        &quot;epochs&quot;: 1000,
    }
)
</pre></div>
</div>
</div>
</div>
<p>We are now ready to run our cross validation loop. We’ll run this a few times while changing hyperparameters in the config to lower the cross validation score then compare all three with TensorBoard.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># new directory to write the progress of our first cross val. loop to
cv_log_dir1 = logs_base_dir + &quot;cv/cv1/&quot;
cv_writer1 = SummaryWriter(cv_log_dir1)
os.makedirs(cv_log_dir1, exist_ok=True)

cv_score1 = cross_validate(
    model, new_config, k_fold_datasets, MODEL_PATH, writer=cv_writer1
)
print(f&quot;Cross Validation Score: {cv_score1}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross Validation Score: 194.15814189984272
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># new directory to write the progress of our second cross val. loop to
cv_log_dir2 = logs_base_dir + &quot;cv/cv2/&quot;
cv_writer2 = SummaryWriter(cv_log_dir2)
os.makedirs(cv_log_dir2, exist_ok=True)

new_config = (
    {  # config includes the hyperparameters used in the function and in the optimizer
        &quot;lr&quot;: 0.3,
        &quot;lambda_sparsity&quot;: 1.0e-4,
        &quot;lambda_TV&quot;: 1.0e-4,
        &quot;entropy&quot;: 1e-02,
        &quot;prior_intensity&quot;: 2.0e-09,
        &quot;TSV&quot;: 1.0e-6,
        &quot;epochs&quot;: 850,
    }
)
cv_score2 = cross_validate(
    model, new_config, k_fold_datasets, MODEL_PATH, writer=cv_writer2
)
print(f&quot;Cross Validation Score: {cv_score2}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross Validation Score: 37.00766535308739
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># new directory to write the progress of our third cross val. loop to
cv_log_dir3 = logs_base_dir + &quot;cv/cv3/&quot;
cv_writer3 = SummaryWriter(cv_log_dir3)
os.makedirs(cv_log_dir3, exist_ok=True)

new_config = (
    {  # config includes the hyperparameters used in the function and in the optimizer
        &quot;lr&quot;: 0.3,
        &#39;lambda_sparsity&#39;: 1.8e-4,
        &#39;lambda_TV&#39;: 2.3e-5,
        &#39;entropy&#39;: 7.4e-06,
        &#39;prior_intensity&#39;: 5.0e-07,
        &#39;TSV&#39;: 1.0e-02,
        &quot;epochs&quot;: 1000,
    }
)

cv_score3 = cross_validate(
    model, new_config, k_fold_datasets, MODEL_PATH, writer=cv_writer3
)
print(f&quot;Cross Validation Score: {cv_score3}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross Validation Score: 20.545847829541017
</pre></div>
</div>
</div>
</div>
<p>Here is the final result for our model with the lowest cross validation score:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots()
im = ax.imshow(
    np.squeeze(model.icube.sky_cube.detach().cpu().numpy()),
    origin=&quot;lower&quot;,
    interpolation=&quot;none&quot;,
    extent=model.icube.coords.img_ext,
)
plt.colorbar(im)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.colorbar.Colorbar at 0x7fc7bb738850&gt;
</pre></div>
</div>
<img alt="../_images/b84135c4e975a28bd44573d9131ce09be6d408c8bb60ad7d35bd77d6015c94a7.png" src="../_images/b84135c4e975a28bd44573d9131ce09be6d408c8bb60ad7d35bd77d6015c94a7.png" />
</div>
</div>
<p>And you can visualize all of the results in TensorBoard.</p>
<p>It may seem strange that the lowest total converged loss values do not correspond with the lowest cross validation scores. This is just a consequence of the fact that we are working with loss functions that correspond to the <em>logarithm</em> of the likelihood function and prior functions. This means that the normalization prefactors aren’t required for each optimization loop (so we don’t calculate them), which also has the consequence that we can’t directly compare loss values across different hyperparameter settings (this is the role of cross-validation).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cv_log_dir = logs_base_dir + &quot;cv/&quot;
# %tensorboard --logdir {cv_log_dir}
## uncomment the above line when running to view TensorBoard
</pre></div>
</div>
</div>
</div>
<p>Hopefully this tutorial has provided an introduction to RML imaging with an actual ALMA dataset.</p>
<p>We started by initializing the RML model to a pseudo-dirty image, which allowed our model converge to the optimal image in fewer iterations.</p>
<p>We also used cross validation to help us understand how well the model fits the dataset. Using TensorBoard, we were able to visualize how changing hyperparameters can result in a lower cross validation score, and therefore a better image, if done correctly. The process of changing the hyperparameters can be automated using a hyperparameter tuning library which we will explore in Part 3 of this tutorial series.</p>
<p>Of the three hyperparameter configurations that we cross-validated above, the third has the lowest cross validation score, and so we might reasonably conclude that this image most closely matches reality because it generates well to new data.</p>
<p>If you would like to compare these results yourself, please run TensorBoard locally. In the next part of the HD143006 tutorial we will be expanding on how to analyze the results of the training, optimization loops, hyperparameter tuning, and exploring the full pipeline of data analysis which can be adapted to any real world data.</p>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../changelog.html" class="btn btn-neutral float-right" title="Changelog" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="HD143006_part_1.html" class="btn btn-neutral float-left" title="HD143006 Tutorial Part 1" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019-21, Ian Czekala

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>


      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-5472810-8', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>